{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU llama-cpp-python\n",
    "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python==0.1.78 --force-reinstall --upgrade --no-cache-dir --verbose\n",
    "\n",
    "# For download the models\n",
    "!pip install huggingface_hub llama-cpp-python==0.1.78"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from llama_cpp import Llama\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "\n",
    "class LlamaModel():\n",
    "\n",
    "    def __init__(self):\n",
    "        # GPU llama-cpp-python\n",
    "        # Set environment variables\n",
    "        os.environ['CMAKE_ARGS'] = \"-DLLAMA_CUBLAS=on\"\n",
    "        os.environ['FORCE_CMAKE'] = \"1\"\n",
    "\n",
    "\n",
    "        model_name_or_path = \"TheBloke/Llama-2-13B-chat-GGML\"\n",
    "        model_basename = \"llama-2-13b-chat.ggmlv3.q5_1.bin\" # the model is in bin format\n",
    "\n",
    "        # Download the model\n",
    "        self.model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)\n",
    "    \n",
    "        self.lcpp_llm = Llama(\n",
    "            model_path=self.model_path,\n",
    "            n_threads=2, # CPU cores\n",
    "            n_batch=512, # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
    "            n_gpu_layers=43, # Change this value based on your model and your GPU VRAM pool.\n",
    "            n_ctx=4096, # Context window\n",
    "            logits_all=True\n",
    "        )\n",
    "\n",
    "        self.default_prompt_template = '''SYSTEM: You are a helpful, respectful and honest assistant. Always answer as helpfully.\n",
    "\n",
    "        USER: {}\n",
    "\n",
    "        ASSISTANT:\n",
    "        '''\n",
    "    \n",
    "    def get_response(\n",
    "            self,\n",
    "            prompt,\n",
    "            prompt_template=None,\n",
    "            max_tokens=256,\n",
    "            temperature=0.5,\n",
    "            top_p=0.95,\n",
    "            top_k=50,\n",
    "            logprobs=10,\n",
    "            stop=['USER:']):\n",
    "        if prompt_template is None:\n",
    "            prompt_template = self.default_prompt_template\n",
    "\n",
    "        context = prompt_template.format(prompt)\n",
    "        \n",
    "        response = self.lcpp_llm(\n",
    "            prompt=context,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            repeat_penalty=1.2,\n",
    "            top_k=top_k,\n",
    "            logprobs=logprobs,\n",
    "            stop = stop, # Dynamic stopping when such token is detected.\n",
    "            echo=True # return the prompt\n",
    "        )\n",
    "\n",
    "        logprobs_df = pd.DataFrame(response[\"choices\"][0][\"logprobs\"][\"top_logprobs\"])\n",
    "\n",
    "        return response[\"choices\"][0][\"text\"], logprobs_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_model = LlamaModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimental code to get probability of text\n",
    "import re\n",
    "\n",
    "t2 = \"\"\"[P1]:  yeahM003_S13:  [eh] Well hello, thanks very much for coming here today. Eh we're going to play a quiz.P028:  hm[MOD]:  #1 I'm going\"\"\"\n",
    "\n",
    "for val in re.finditer(\"\\[P1\\]\", t2):\n",
    "    start_idx = val.start()\n",
    "    if start_idx!=0:\n",
    "        context = t2[:start_idx]\n",
    "        text, probs = llama_model.get_response(\n",
    "            prompt=context,\n",
    "            max_tokens=4,\n",
    "            temperature=0.5,\n",
    "            top_p=0.95,\n",
    "            top_k=50,\n",
    "            logprobs=10,\n",
    "            stop=['USER:'],\n",
    "            echo=True\n",
    "        )\n",
    "        print(text)\n",
    "        print(probs)\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
