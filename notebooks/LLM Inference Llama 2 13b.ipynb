{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihaZyg2aYQpH"
      },
      "source": [
        "# llama-cpp-python\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MDuvLwarQO8"
      },
      "source": [
        "The Python package provides simple bindings for the llama.cpp library, offering access to the C API via ctypes interface, a high-level Python API for text completion, OpenAI-like API, and LangChain compatibility. It supports multiple BLAS backends for faster processing and includes both high-level and low-level APIs, along with web server functionality.\n",
        "\n",
        "`llama.cpp`'s objective is to run the LLaMA model with 4-bit integer quantization on MacBook. It is a plain C/C++ implementation optimized for Apple silicon and x86 architectures, supporting various integer quantization and BLAS libraries. Originally a web chat example, it now serves as a development playground for ggml library features.\n",
        "\n",
        "`GGML`, a C library for machine learning, facilitates the distribution of large language models (LLMs). It utilizes quantization to enable efficient LLM execution on consumer hardware. GGML files contain binary-encoded data, including version number, hyperparameters, vocabulary, and weights. The vocabulary comprises tokens for language generation, while the weights determine the LLM's size. Quantization reduces precision to optimize resource usage."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The GGML format has been replaced by GGUF, effective as of August 21st, 2023. Starting from this date, llama.cpp will no longer provide compatibility with GGML models. This notebook uses `llama-cpp-python==0.1.78, which is compatible with GGML Models`."
      ],
      "metadata": {
        "id": "V2WYqinVm8DB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBnzlAqYZM6_"
      },
      "source": [
        "| Code Credits | Link |\n",
        "| ----------- | ---- |\n",
        "| 🎉 llama-cpp-python | [![GitHub Repository](https://img.shields.io/github/stars/abetlen/llama-cpp-python?style=social)](https://github.com/abetlen/llama-cpp-python) |\n",
        "| 🎉 llama.cpp | [![GitHub Repository](https://img.shields.io/github/stars/ggerganov/llama.cpp?style=social)](https://github.com/ggerganov/llama.cpp) |\n",
        "| 🎉 GGML | [![GitHub Repository](https://img.shields.io/github/stars/ggerganov/ggml?style=social)](https://github.com/ggerganov/ggml) |\n",
        "| 🚀 Online inference | [![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/ysharma/Explore_llamav2_with_TGI) |\n",
        "| 🚀 Online inference | [![Replicate](https://replicate.com/google-research/frame-interpolation/badge)](https://replicate.com/replicate/llama70b-v2-chat)\n",
        " |\n",
        "| 🔥 Discover More Colab Notebooks | [![GitHub Repository](https://img.shields.io/badge/GitHub-Repository-black?style=flat-square&logo=github)](https://github.com/R3gm/InsightSolver-Colab/) |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rkl4zg7nLhJ"
      },
      "source": [
        "\n",
        "The library works the same with a CPU, but the inference can take about three times longer compared to using it on a GPU.\n",
        "\n",
        "If you want to use only the CPU, you can replace the content of the cell below with the following lines.\n",
        "```\n",
        "# CPU llama-cpp-python\n",
        "!pip install llama-cpp-python==0.1.78\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rkBmY3vQvRSw",
        "outputId": "0f553e56-662d-41f3-c51c-38f1253a02dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using pip 23.1.2 from /usr/local/lib/python3.10/dist-packages/pip (python 3.10)\n",
            "Collecting llama-cpp-python==0.1.78\n",
            "  Downloading llama_cpp_python-0.1.78.tar.gz (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Running command pip subprocess to install build dependencies\n",
            "  Using pip 23.1.2 from /usr/local/lib/python3.10/dist-packages/pip (python 3.10)\n",
            "  Collecting setuptools>=42\n",
            "    Downloading setuptools-69.0.2-py3-none-any.whl (819 kB)\n",
            "       ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 819.5/819.5 kB 12.5 MB/s eta 0:00:00\n",
            "  Collecting scikit-build>=0.13\n",
            "    Downloading scikit_build-0.17.6-py3-none-any.whl (84 kB)\n",
            "       ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 84.3/84.3 kB 12.2 MB/s eta 0:00:00\n",
            "  Collecting cmake>=3.18\n",
            "    Downloading cmake-3.27.7-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (26.0 MB)\n",
            "       ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 26.0/26.0 MB 72.0 MB/s eta 0:00:00\n",
            "  Collecting ninja\n",
            "    Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
            "       ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 307.2/307.2 kB 27.7 MB/s eta 0:00:00\n",
            "  Collecting distro (from scikit-build>=0.13)\n",
            "    Downloading distro-1.8.0-py3-none-any.whl (20 kB)\n",
            "  Collecting packaging (from scikit-build>=0.13)\n",
            "    Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "       ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.0/53.0 kB 6.9 MB/s eta 0:00:00\n",
            "  Collecting tomli (from scikit-build>=0.13)\n",
            "    Downloading tomli-2.0.1-py3-none-any.whl (12 kB)\n",
            "  Collecting wheel>=0.32.0 (from scikit-build>=0.13)\n",
            "    Downloading wheel-0.41.3-py3-none-any.whl (65 kB)\n",
            "       ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 65.8/65.8 kB 8.3 MB/s eta 0:00:00\n",
            "  Installing collected packages: ninja, cmake, wheel, tomli, setuptools, packaging, distro, scikit-build\n",
            "    Creating /tmp/pip-build-env-9fevp89u/overlay/local/bin\n",
            "    changing mode of /tmp/pip-build-env-9fevp89u/overlay/local/bin/ninja to 755\n",
            "    changing mode of /tmp/pip-build-env-9fevp89u/overlay/local/bin/cmake to 755\n",
            "    changing mode of /tmp/pip-build-env-9fevp89u/overlay/local/bin/cpack to 755\n",
            "    changing mode of /tmp/pip-build-env-9fevp89u/overlay/local/bin/ctest to 755\n",
            "    changing mode of /tmp/pip-build-env-9fevp89u/overlay/local/bin/wheel to 755\n",
            "    changing mode of /tmp/pip-build-env-9fevp89u/overlay/local/bin/distro to 755\n",
            "  ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "  ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
            "  lida 0.0.10 requires fastapi, which is not installed.\n",
            "  lida 0.0.10 requires kaleido, which is not installed.\n",
            "  lida 0.0.10 requires python-multipart, which is not installed.\n",
            "  lida 0.0.10 requires uvicorn, which is not installed.\n",
            "  Successfully installed cmake-3.27.7 distro-1.8.0 ninja-1.11.1.1 packaging-23.2 scikit-build-0.17.6 setuptools-69.0.2 tomli-2.0.1 wheel-0.41.3\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Running command Getting requirements to build wheel\n",
            "  running egg_info\n",
            "  writing llama_cpp_python.egg-info/PKG-INFO\n",
            "  writing dependency_links to llama_cpp_python.egg-info/dependency_links.txt\n",
            "  writing requirements to llama_cpp_python.egg-info/requires.txt\n",
            "  writing top-level names to llama_cpp_python.egg-info/top_level.txt\n",
            "  reading manifest file 'llama_cpp_python.egg-info/SOURCES.txt'\n",
            "  adding license file 'LICENSE.md'\n",
            "  writing manifest file 'llama_cpp_python.egg-info/SOURCES.txt'\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Running command Preparing metadata (pyproject.toml)\n",
            "  running dist_info\n",
            "  creating /tmp/pip-modern-metadata-b0mjm7e1/llama_cpp_python.egg-info\n",
            "  writing /tmp/pip-modern-metadata-b0mjm7e1/llama_cpp_python.egg-info/PKG-INFO\n",
            "  writing dependency_links to /tmp/pip-modern-metadata-b0mjm7e1/llama_cpp_python.egg-info/dependency_links.txt\n",
            "  writing requirements to /tmp/pip-modern-metadata-b0mjm7e1/llama_cpp_python.egg-info/requires.txt\n",
            "  writing top-level names to /tmp/pip-modern-metadata-b0mjm7e1/llama_cpp_python.egg-info/top_level.txt\n",
            "  writing manifest file '/tmp/pip-modern-metadata-b0mjm7e1/llama_cpp_python.egg-info/SOURCES.txt'\n",
            "  reading manifest file '/tmp/pip-modern-metadata-b0mjm7e1/llama_cpp_python.egg-info/SOURCES.txt'\n",
            "  adding license file 'LICENSE.md'\n",
            "  writing manifest file '/tmp/pip-modern-metadata-b0mjm7e1/llama_cpp_python.egg-info/SOURCES.txt'\n",
            "  creating '/tmp/pip-modern-metadata-b0mjm7e1/llama_cpp_python-0.1.78.dist-info'\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting typing-extensions>=4.5.0 (from llama-cpp-python==0.1.78)\n",
            "  Downloading typing_extensions-4.8.0-py3-none-any.whl (31 kB)\n",
            "Collecting numpy>=1.20.0 (from llama-cpp-python==0.1.78)\n",
            "  Downloading numpy-1.26.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m299.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting diskcache>=5.6.1 (from llama-cpp-python==0.1.78)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m249.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n",
            "  Running command Building wheel for llama-cpp-python (pyproject.toml)\n",
            "\n",
            "\n",
            "  --------------------------------------------------------------------------------\n",
            "  -- Trying 'Ninja' generator\n",
            "  --------------------------------\n",
            "  ---------------------------\n",
            "  ----------------------\n",
            "  -----------------\n",
            "  ------------\n",
            "  -------\n",
            "  --\n",
            "  CMake Deprecation Warning at CMakeLists.txt:1 (cmake_minimum_required):\n",
            "    Compatibility with CMake < 3.5 will be removed from a future version of\n",
            "    CMake.\n",
            "\n",
            "    Update the VERSION argument <min> value or use a ...<max> suffix to tell\n",
            "    CMake that the project does not need compatibility with older versions.\n",
            "\n",
            "  Not searching for unused variables given on the command line.\n",
            "\n",
            "  -- The C compiler identification is GNU 11.4.0\n",
            "  -- Detecting C compiler ABI info\n",
            "  -- Detecting C compiler ABI info - done\n",
            "  -- Check for working C compiler: /usr/bin/cc - skipped\n",
            "  -- Detecting C compile features\n",
            "  -- Detecting C compile features - done\n",
            "  -- The CXX compiler identification is GNU 11.4.0\n",
            "  -- Detecting CXX compiler ABI info\n",
            "  -- Detecting CXX compiler ABI info - done\n",
            "  -- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "  -- Detecting CXX compile features\n",
            "  -- Detecting CXX compile features - done\n",
            "  -- Configuring done (0.7s)\n",
            "  -- Generating done (0.0s)\n",
            "  -- Build files have been written to: /tmp/pip-install-aorwkvk5/llama-cpp-python_4763dae819994999ba8a77d30e866043/_cmake_test_compile/build\n",
            "  --\n",
            "  -------\n",
            "  ------------\n",
            "  -----------------\n",
            "  ----------------------\n",
            "  ---------------------------\n",
            "  --------------------------------\n",
            "  -- Trying 'Ninja' generator - success\n",
            "  --------------------------------------------------------------------------------\n",
            "\n",
            "  Configuring Project\n",
            "    Working directory:\n",
            "      /tmp/pip-install-aorwkvk5/llama-cpp-python_4763dae819994999ba8a77d30e866043/_skbuild/linux-x86_64-3.10/cmake-build\n",
            "    Command:\n",
            "      /tmp/pip-build-env-9fevp89u/overlay/local/lib/python3.10/dist-packages/cmake/data/bin/cmake /tmp/pip-install-aorwkvk5/llama-cpp-python_4763dae819994999ba8a77d30e866043 -G Ninja -DCMAKE_MAKE_PROGRAM:FILEPATH=/tmp/pip-build-env-9fevp89u/overlay/local/lib/python3.10/dist-packages/ninja/data/bin/ninja --no-warn-unused-cli -DCMAKE_INSTALL_PREFIX:PATH=/tmp/pip-install-aorwkvk5/llama-cpp-python_4763dae819994999ba8a77d30e866043/_skbuild/linux-x86_64-3.10/cmake-install -DPYTHON_VERSION_STRING:STRING=3.10.12 -DSKBUILD:INTERNAL=TRUE -DCMAKE_MODULE_PATH:PATH=/tmp/pip-build-env-9fevp89u/overlay/local/lib/python3.10/dist-packages/skbuild/resources/cmake -DPYTHON_EXECUTABLE:PATH=/usr/bin/python3 -DPYTHON_INCLUDE_DIR:PATH=/usr/include/python3.10 -DPYTHON_LIBRARY:PATH=/usr/lib/x86_64-linux-gnu/libpython3.10.so -DPython_EXECUTABLE:PATH=/usr/bin/python3 -DPython_ROOT_DIR:PATH=/usr -DPython_FIND_REGISTRY:STRING=NEVER -DPython_INCLUDE_DIR:PATH=/usr/include/python3.10 -DPython3_EXECUTABLE:PATH=/usr/bin/python3 -DPython3_ROOT_DIR:PATH=/usr -DPython3_FIND_REGISTRY:STRING=NEVER -DPython3_INCLUDE_DIR:PATH=/usr/include/python3.10 -DCMAKE_MAKE_PROGRAM:FILEPATH=/tmp/pip-build-env-9fevp89u/overlay/local/lib/python3.10/dist-packages/ninja/data/bin/ninja -DLLAMA_CUBLAS=on -DCMAKE_BUILD_TYPE:STRING=Release -DLLAMA_CUBLAS=on\n",
            "\n",
            "  Not searching for unused variables given on the command line.\n",
            "  -- The C compiler identification is GNU 11.4.0\n",
            "  -- The CXX compiler identification is GNU 11.4.0\n",
            "  -- Detecting C compiler ABI info\n",
            "  -- Detecting C compiler ABI info - done\n",
            "  -- Check for working C compiler: /usr/bin/cc - skipped\n",
            "  -- Detecting C compile features\n",
            "  -- Detecting C compile features - done\n",
            "  -- Detecting CXX compiler ABI info\n",
            "  -- Detecting CXX compiler ABI info - done\n",
            "  -- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "  -- Detecting CXX compile features\n",
            "  -- Detecting CXX compile features - done\n",
            "  -- Found Git: /usr/bin/git (found version \"2.34.1\")\n",
            "  fatal: not a git repository (or any of the parent directories): .git\n",
            "  fatal: not a git repository (or any of the parent directories): .git\n",
            "  CMake Warning at vendor/llama.cpp/CMakeLists.txt:117 (message):\n",
            "    Git repository not found; to enable automatic generation of build info,\n",
            "    make sure Git is installed and the project is a Git repository.\n",
            "\n",
            "\n",
            "  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
            "  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
            "  -- Found Threads: TRUE\n",
            "  -- Found CUDAToolkit: /usr/local/cuda/include (found version \"11.8.89\")\n",
            "  -- cuBLAS found\n",
            "  -- The CUDA compiler identification is NVIDIA 11.8.89\n",
            "  -- Detecting CUDA compiler ABI info\n",
            "  -- Detecting CUDA compiler ABI info - done\n",
            "  -- Check for working CUDA compiler: /usr/local/cuda/bin/nvcc - skipped\n",
            "  -- Detecting CUDA compile features\n",
            "  -- Detecting CUDA compile features - done\n",
            "  -- Using CUDA architectures: 52;61;70\n",
            "  -- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
            "  -- x86 detected\n",
            "  -- Configuring done (3.3s)\n",
            "  -- Generating done (0.0s)\n",
            "  -- Build files have been written to: /tmp/pip-install-aorwkvk5/llama-cpp-python_4763dae819994999ba8a77d30e866043/_skbuild/linux-x86_64-3.10/cmake-build\n",
            "  [1/9] Building C object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o\n",
            "  [2/9] Building C object vendor/llama.cpp/CMakeFiles/ggml.dir/k_quants.c.o\n",
            "  [3/9] Building C object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o\n",
            "  [4/9] Building CXX object vendor/llama.cpp/CMakeFiles/llama.dir/llama.cpp.o\n",
            "  [5/9] Building CUDA object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda.cu.o\n",
            "  [6/9] Linking CUDA shared library vendor/llama.cpp/libggml_shared.so\n",
            "  [7/9] Linking CXX shared library vendor/llama.cpp/libllama.so\n",
            "  [8/9] Linking CUDA static library vendor/llama.cpp/libggml_static.a\n",
            "  [8/9] Install the project...\n",
            "  -- Install configuration: \"Release\"\n",
            "  -- Installing: /tmp/pip-install-aorwkvk5/llama-cpp-python_4763dae819994999ba8a77d30e866043/_skbuild/linux-x86_64-3.10/cmake-install/lib/libggml_shared.so\n",
            "  -- Installing: /tmp/pip-install-aorwkvk5/llama-cpp-python_4763dae819994999ba8a77d30e866043/_skbuild/linux-x86_64-3.10/cmake-install/lib/libllama.so\n",
            "  -- Set runtime path of \"/tmp/pip-install-aorwkvk5/llama-cpp-python_4763dae819994999ba8a77d30e866043/_skbuild/linux-x86_64-3.10/cmake-install/lib/libllama.so\" to \"\"\n",
            "  -- Installing: /tmp/pip-install-aorwkvk5/llama-cpp-python_4763dae819994999ba8a77d30e866043/_skbuild/linux-x86_64-3.10/cmake-install/bin/convert.py\n",
            "  -- Installing: /tmp/pip-install-aorwkvk5/llama-cpp-python_4763dae819994999ba8a77d30e866043/_skbuild/linux-x86_64-3.10/cmake-install/bin/convert-lora-to-ggml.py\n",
            "  -- Installing: /tmp/pip-install-aorwkvk5/llama-cpp-python_4763dae819994999ba8a77d30e866043/_skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/libllama.so\n",
            "  -- Set runtime path of \"/tmp/pip-install-aorwkvk5/llama-cpp-python_4763dae819994999ba8a77d30e866043/_skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/libllama.so\" to \"\"\n",
            "\n",
            "  copying llama_cpp/__init__.py -> _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/__init__.py\n",
            "  copying llama_cpp/llama_grammar.py -> _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama_grammar.py\n",
            "  copying llama_cpp/llama.py -> _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama.py\n",
            "  copying llama_cpp/llama_cpp.py -> _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama_cpp.py\n",
            "  copying llama_cpp/utils.py -> _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/utils.py\n",
            "  copying llama_cpp/llama_types.py -> _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama_types.py\n",
            "  creating directory _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/server\n",
            "  copying llama_cpp/server/__main__.py -> _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/server/__main__.py\n",
            "  copying llama_cpp/server/app.py -> _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/server/app.py\n",
            "  copying llama_cpp/server/__init__.py -> _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/server/__init__.py\n",
            "  copying /tmp/pip-install-aorwkvk5/llama-cpp-python_4763dae819994999ba8a77d30e866043/llama_cpp/py.typed -> _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/py.typed\n",
            "\n",
            "  running bdist_wheel\n",
            "  running build\n",
            "  running build_py\n",
            "  creating _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310\n",
            "  creating _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/__init__.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama_grammar.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama_cpp.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/utils.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama_types.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n",
            "  creating _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/server\n",
            "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/server/__main__.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/server\n",
            "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/server/app.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/server\n",
            "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/server/__init__.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/server\n",
            "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/py.typed -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/libllama.so -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/__init__.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama_grammar.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama_cpp.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/utils.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama_types.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/server/__main__.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/server\n",
            "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/server/app.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/server\n",
            "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/server/__init__.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/server\n",
            "  copied 9 files\n",
            "  running build_ext\n",
            "  installing to _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel\n",
            "  running install\n",
            "  running install_lib\n",
            "  creating _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64\n",
            "  creating _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel\n",
            "  creating _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/libllama.so -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/__init__.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/py.typed -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n",
            "  creating _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp/server\n",
            "  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/server/__main__.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp/server\n",
            "  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/server/app.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp/server\n",
            "  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/server/__init__.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp/server\n",
            "  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/llama_grammar.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/llama.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/llama_cpp.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/utils.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/llama_types.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n",
            "  copied 11 files\n",
            "  running install_data\n",
            "  creating _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data\n",
            "  creating _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data/data\n",
            "  creating _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data/data/lib\n",
            "  copying _skbuild/linux-x86_64-3.10/cmake-install/lib/libllama.so -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data/data/lib\n",
            "  copying _skbuild/linux-x86_64-3.10/cmake-install/lib/libggml_shared.so -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data/data/lib\n",
            "  creating _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data/data/bin\n",
            "  copying _skbuild/linux-x86_64-3.10/cmake-install/bin/convert.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data/data/bin\n",
            "  copying _skbuild/linux-x86_64-3.10/cmake-install/bin/convert-lora-to-ggml.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data/data/bin\n",
            "  running install_egg_info\n",
            "  running egg_info\n",
            "  writing llama_cpp_python.egg-info/PKG-INFO\n",
            "  writing dependency_links to llama_cpp_python.egg-info/dependency_links.txt\n",
            "  writing requirements to llama_cpp_python.egg-info/requires.txt\n",
            "  writing top-level names to llama_cpp_python.egg-info/top_level.txt\n",
            "  reading manifest file 'llama_cpp_python.egg-info/SOURCES.txt'\n",
            "  adding license file 'LICENSE.md'\n",
            "  writing manifest file 'llama_cpp_python.egg-info/SOURCES.txt'\n",
            "  Copying llama_cpp_python.egg-info to _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78-py3.10.egg-info\n",
            "  running install_scripts\n",
            "  copied 0 files\n",
            "  creating _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.dist-info/WHEEL\n",
            "  creating '/tmp/pip-wheel-ukwvygst/.tmp-wszofbii/llama_cpp_python-0.1.78-cp310-cp310-linux_x86_64.whl' and adding '_skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel' to it\n",
            "  adding 'llama_cpp/__init__.py'\n",
            "  adding 'llama_cpp/libllama.so'\n",
            "  adding 'llama_cpp/llama.py'\n",
            "  adding 'llama_cpp/llama_cpp.py'\n",
            "  adding 'llama_cpp/llama_grammar.py'\n",
            "  adding 'llama_cpp/llama_types.py'\n",
            "  adding 'llama_cpp/py.typed'\n",
            "  adding 'llama_cpp/utils.py'\n",
            "  adding 'llama_cpp/server/__init__.py'\n",
            "  adding 'llama_cpp/server/__main__.py'\n",
            "  adding 'llama_cpp/server/app.py'\n",
            "  adding 'llama_cpp_python-0.1.78.data/data/bin/convert-lora-to-ggml.py'\n",
            "  adding 'llama_cpp_python-0.1.78.data/data/bin/convert.py'\n",
            "  adding 'llama_cpp_python-0.1.78.data/data/lib/libggml_shared.so'\n",
            "  adding 'llama_cpp_python-0.1.78.data/data/lib/libllama.so'\n",
            "  adding 'llama_cpp_python-0.1.78.dist-info/LICENSE.md'\n",
            "  adding 'llama_cpp_python-0.1.78.dist-info/METADATA'\n",
            "  adding 'llama_cpp_python-0.1.78.dist-info/WHEEL'\n",
            "  adding 'llama_cpp_python-0.1.78.dist-info/top_level.txt'\n",
            "  adding 'llama_cpp_python-0.1.78.dist-info/RECORD'\n",
            "  removing _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.1.78-cp310-cp310-linux_x86_64.whl size=5822280 sha256=fe126d94a5a6bb55d3ef22aff22b06da4129d1b83fc9346f9ef982affdc6c6ca\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-iieumfpy/wheels/61/f9/20/9ca660a9d3f2a47e44217059409478865948b5c8a1cba70030\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: typing-extensions, numpy, diskcache, llama-cpp-python\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/__pycache__/typing_extensions.cpython-310.pyc\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/typing_extensions-4.5.0.dist-info/\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/typing_extensions.py\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.23.5\n",
            "    Uninstalling numpy-1.23.5:\n",
            "      Removing file or directory /usr/local/bin/f2py\n",
            "      Removing file or directory /usr/local/bin/f2py3\n",
            "      Removing file or directory /usr/local/bin/f2py3.10\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/numpy-1.23.5.dist-info/\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/numpy.libs/\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/numpy/\n",
            "      Successfully uninstalled numpy-1.23.5\n",
            "  changing mode of /usr/local/bin/f2py to 755\n",
            "  Attempting uninstall: diskcache\n",
            "    Found existing installation: diskcache 5.6.3\n",
            "    Uninstalling diskcache-5.6.3:\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/diskcache-5.6.3.dist-info/\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/diskcache/\n",
            "      Successfully uninstalled diskcache-5.6.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\n",
            "cupy-cuda11x 11.0.0 requires numpy<1.26,>=1.20, but you have numpy 1.26.2 which is incompatible.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed diskcache-5.6.3 llama-cpp-python-0.1.78 numpy-1.26.2 typing-extensions-4.8.0\n"
          ]
        }
      ],
      "source": [
        "# GPU llama-cpp-python\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python==0.1.78 --force-reinstall --upgrade --no-cache-dir --verbose"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_nBHTYSoIWV",
        "outputId": "d6fbdf95-dfaa-4264-9efb-0889da9bafaa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.19.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.8.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2023.7.22)\n"
          ]
        }
      ],
      "source": [
        "# For download the models\n",
        "!pip install huggingface_hub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R11KqY7lW0yv"
      },
      "source": [
        "# Select the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzJkCoRRbICP"
      },
      "source": [
        "\n",
        "First, we need to specify the model to use. In Colab with T4 GPU, we can run models of up to 20B of parameters with all optimizations, but this may degrade the quality of the model's inference. The library can run GGML models on a CPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHAz1yGZb4lq"
      },
      "source": [
        "In this case, we will use a [Llama 2 13B-chat](https://huggingface.co/meta-llama/Llama-2-13b-chat) The Llama 2 is a collection of pretrained and fine-tuned generative text models, ranging from 7 billion to 70 billion parameters, designed for dialogue use cases. It outperforms open-source chat models on most benchmarks and is on par with popular closed-source models in human evaluations for helpfulness and safety.\n",
        "\n",
        "![asd](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc24dac6d-6b5e-4b5f-938c-05951c938a9e_1085x543.png)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdmNwEqjRcPY"
      },
      "source": [
        "# Model quantization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tx-5Fgx80UXj"
      },
      "source": [
        "We can quantize the model using this library, but for practical purposes, it is better to use pre-quantized models. The resulting model is only compatible with libraries that support GGML.\n",
        "\n",
        "\n",
        "```\n",
        "# obtain the original LLaMA model weights and place them in ./models\n",
        "ls ./models\n",
        "65B 30B 13B 7B tokenizer_checklist.chk tokenizer.model\n",
        "\n",
        "# install Python dependencies\n",
        "python3 -m pip install -r requirements.txt\n",
        "\n",
        "# convert the 7B model to ggml FP16 format\n",
        "python3 convert.py models/7B/\n",
        "\n",
        "# quantize the model to 4-bits (using q4_0 method)\n",
        "./quantize ./models/7B/ggml-model-f16.bin ./models/7B/ggml-model-q4_0.bin q4_0\n",
        "\n",
        "# run the inference\n",
        "./main -m ./models/7B/ggml-model-q4_0.bin -n 128\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSeedwAFSay9"
      },
      "source": [
        "#  Quantized Models from the Hugging Face Community"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QwfjGFYRM4g"
      },
      "source": [
        "The Hugging Face community provides quantized models, which allow us to efficiently and effectively utilize the model on the T4 GPU. It is important to consult reliable sources before using any model.\n",
        "\n",
        "There are several variations available, but the ones that interest us are based on the GGLM library.\n",
        "\n",
        "We can see the different variations that Llama-2-13B-GGML has [here](https://huggingface.co/models?search=llama%202%20ggml).\n",
        "\n",
        "\n",
        "\n",
        "In this case, we will use the model called [Llama-2-13B-chat-GGML](https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TbhbyD9wIdy"
      },
      "source": [
        "![22.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAToAAALICAIAAABgkquEAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAADsMAAA7DAcdvqGQAAHSmSURBVHhe7b17XFTHuvepmHuMgDdQQUUFFCJKwASNgpeAF4gIijbh5o0QiSSKbHaI7kSIF2IIURQNihIwwtHIRuXdBHICmy2EKFv0AHFgkzl9eOfF15nx837eOfOZmX/3PFW17r266W4a6NU8+Xw/lepnVdWqVat+66nqxvWMe9V5BoIgmgDliiCaAeWKIJoB5YogmgHliiCaAeWKIJphFOT6PzwWIIijopjttsU2clX0GEGQQVGIyBwslqvilAiC2BCF3BSYK1dFowiCDCsKATLMkquiIQRBRgCFDIGRlquiZTPxXPNB+JplCqMBvn7RH6wM8TWwm8WbKSd2RSiNapTeffrsicAvpQYFEOvI+vM/yJDePa+wawnFbB8iisYBpVxfmeT+wisuE+a+On7LC07JL0x485XnJ7soWjHk0Gdfiry/HtpZk1dTmpcibXkw1m/mWjiqS/ogPHK9u7t41LzWQncV13x5INTAbg4JJffuFqwWPjJNtp8VCwjYo1zTb/+72KV/1KUbFDCO5HKU12L0Ss/+IrFTbKGxIcqVVf/3P2co7IMivUyD6ufb+UOygeVHgJ8hGXU9tAzXeVZrsLmhEJEhivKATK4vTZ36vPOrTu8+P/7W+PF948cPOI3/DyensglGWhEEVvTllZoLxUVDk2vKH6/WFH7FNXi0pKb0aqkumDs67HKNuNJ07/to0WJCrjzm3ZIRwFA8ZiuWUwiH9Fr4+SeDL6ByRmt0omBU5Cp/JCkuRKpVBj+wwgj03M6SfrRIrsCyFeH5py+npB5kGBGaiCjXV9zcJsS+OGHp8+OLncZ3EqGO/69ORLH/1WmwVlR0Yp1c/7hN+LhMd7rmwmdx7ONwy3Xl6butlxMkFu3IldOVkce8SQQX8WfmnCXXwg6xuUjgJi53FnZUOIWysJWMgly5VYlChNw4cP3hr4sTNuseP3Ssrqh5K+QKKmWZ4BXvDCY0iVyf/+DVCRufH7/eafxVJ6cHE8b/L07j/+H0/L+/OOnMpMFaMSbXD3ySvgTHW3r1xpc5ce7C0SlLQw4UFV4l9sKT2Uu4Ra9Crp7hJ6GF99hHuVznLdldeJw0W3PhdOHmVfN4u6wbPh9eKDp9bKUXPeQTs+vkNShfeuXakQ/XO3PlBd468fNvZalSyxDkKvNLykf13fNCy4rZAIh3nZbkK5ok/XwpTJc/i42YPek5BZKS3Kw1Mb1YYfm0Fk6h+Dgo4lyX1RJ7LhSQPwKMjA9/IQrMeHwYjhWc4t//fLuU3HfFcxCAE/2j7s/nySjRHnKPuZ5/QAq12P3lS0Jds+Uq+FiF0MCuKC/KdULIy04uL41fMt4tfPo7F9+Zfs9t6oNpGysj5wXNU7QiVOFRl+uFKze+zPvAz2+ZT1Jh0dUbH0V60kO+K/NulBYfDVni6+yxbvPJGxfyUqYSu0Su7v4+kUe/vHotdQ2rIpWrp9+B70qvfpcaGerpFxqSVQr5XSGsmKQbIYe/FOwe7x26UvNlVpyXx7ypIRlHrtz44zb511GLi+of3tgltVgtV5U1JK9YWp7eWmK/+4swydhZZHORIZlGZsN1wCwnAyplc9qoXKWXI1lgS/XGMLurnELU6vLPL36IKMKFGB8f6+XK2jQyVqxZyVVLYXI9K4zPP+rO0jG0Wq7MxyqEZkquThtedHJ+ZfyGCeNeG8f+Gz9+PMsoWhGq8BjxricFf7hOLBAMQirV+fEl/TKPXy0KnwJ5IlfiAHkKwTnz3zaJcvX44MjVmqO7/bnqzv6bvoITfUAFz3fDK+WPRJNcGR+Q9+lsL678DM/3SyUdIyw69nP79UNSi9VyTT/f3iO5/WxaS28hmUPsHtOK3F0nR+t6/iFOeqYf6YRTKEQoKYeb07KZqpjNanPILLkaXJcc6VgpNCnRA9cZvjBrn5MEL1eu8/KPg42PSe0Z64+0Cjd0FHZHaFdZ36RjKN44UlG4xaxL0nutOtQKrJfr82kvOQW8+PykieM9nhv32vhxL4yb8Py4ZZMmzp75kqIVoQqPEbmKy1exwNTdF0qvfPdH7jsq4EIhuEHylZJsMezstR4cb2nxYT+iZElra44WceU5iBpLji4Rz5ICaelXmZ5cAV/Qs/g1GPDVNWjWh6/+qrP3pzW/XftQ+MiwUq6A4VSW3kKap42Ld507C6cZCZbJVZhSii5Jp5rhUYpRuQpwjXBdFeYoO2pQ3ahc1WQmwNVSNCsUNj0+oyVXeoiUZ92T3mtT48kzbeaCud4BVsn1ZZfnX5g0ecrMCS+84jTlRad5z73g9fwHU17OdJ2iaEWowmOBXIlzu1KUmPRBuMh7fh5QRrF3BUeacYSsh0lebC3yywuGcr3yZQjJk7OAWy7KO/bHKzVHOA9MvrIqOpktOd0H4dHrqDemzDrxw+O6T2fxHzmslKvsnimmNS1P82pypUcVU9PItFZBmMrmV5HCVVdcS4ZRMSjkyg2FkXWjFJPXZVKug4+PrIfmoTyj7L5zKwvJHJDccVGuzllnz5NusC5J77U5cmVYI9eJzu5v+U9/fY7z67MnrVs6dcmCKb6erqtnu+Qvna1oRajCY4Fcqd4ubGbfAMkwkKtf9nGy/yR5sTWyeBa2wYDnys9ulJ5mvpTKlTpk921FF64UhdOzhECBrzLEL7oUfHirveaEm8JopVwVt1/+kZaneRW5MgEI80/x0TSssDlqMYaBXLmeix1gF2tErlx1czrAtcMPrGRfYCgeqSbNGB8r5GowdLLusQaVJ5XcONm5RlqugPdctyO61w5FT86Idtkf6xy3ZsqhmClFu10VrUirUCyRq3NoYjG4uwxuX+q+zC+YuUGJXKf4uq967yOivWy2apW0xr6pOhbiQxTrHnH0y6s3DkWzr46kZyF72qK894gXDc6GMkd2h7IvhJ391vtIHhZp13+7c+wt4SMPd6vkyO6iAnafuDsqGPnvk8hRk3LlZjxPzy+wB6YZ2YxUQ7a9lGDGXFGclAdmYVY6/Z5TCd+m4jIZgsxMwmlSSs8vpVQtpuRqxvgoWx586AD10eOeJurjY75cFRi/IyDLQ5+eMPwhZxC5ApOnuEctd1nhPzF146trl0zavnKibrVt5TrjVa+Y1NM3YNV6oQTSG19mxVAhEbkK3zOVXrlx/DPxqyYvWEJfrTm0jTrVKcs35dFfZQjXjry/jmtWcRbqh9kXTqBq+sPPjSJIr3yXGiH89pNR9vDuicUsL8VKuUonDZ0uXGFy1KRc+TyFHOLaGSW50gLylgUVAYZyNUsbPNLqkmZNyRUYfHzkIjG3S2ItuHB2v7g7AognBQ/MSoo3zmZyBUCZgEJosLNVFFPKVRVFK4qjVuKx1NNv6VT6TZIZeE71WSb9s0QLqwOkBU8f+U84qTfaf/52pdSCIKPHoEIbPbnaAdGXH9af3qgwIshoMajQrJGrKooqWmBd1uUbn5r1r3AQxMYo5KOKogpgM7lah+JECKI5FFPahihOBJglV0DREIIgw41Cg4C5cgUUbSEIMkwopCdggVwFFE0jCGITFEIzxBq5GkNxbgRBVFEIx3xsKVcrUFwGgmgOxZQeVkZZrgiCmA/KFUE0A8oVQTQDyhVBNAPKFUE0A8oVQTSDilwnuni4TPeePMPP1X3RVI8AxExc3BfCoDlP957oMksxpIaM5UF2+IEavgtUynXSlLlTZvq7uvm4TF/gMn0+YhluC2DoYABhGBUDKwUH2fEHanguUCbXSVPnTZ6xSFECsQIYRhhM6dgK4CBLcfiBsu0FinIFpwxCVxxGrAYG03AthINsiMMPlA0vUJQrLKCpU1aWQKwDBhN2L8Lw4iAbw+EHyoYXKMoVNru4X7UlbgtgSIXhxUE2isMPlO0uUJSrq/tCxTFkiMCQCsNr5iBHhnh8ssUdgIzi0PATPM9XYRkhrBgoYPbM+dt9vRVG+8SKC5w0f9GkpYGE+eL+VpTrVI8AwSpl5tzQrTvaN8c0CcBHvyV7FcXMIuiD7JOn80SO6YIMygxOQEBSDqmem7HaS3HI1oSfv9NYHqswmg0MqTC8pgcZiArx+PnI5K7yGf/smv2fLR6QuZ09ddXS2YpiAhH7pCN5Om/fdkUBy4i8VP/o0Z0fy9MV9hHBooFigFZb1/hlLvZV2FXhx+rU3pjweQZHhzotd1S13j6lNMqx6AJBny/veffFP+gYLx3YCh+ZaAeXq7ffjuWhst4YWsyFG5fqpsf3vzNrXE7dvF8lV0tk9g+P2hur86H62fqmH8+vkB21Cbsv3q8/wvJeiem5GVafwvybBLJsPDEdhKqg6ejUAN+5isIMNgXP/fi4/cdyMsmGJtfVZ1tbyz5QGEcMi2YzwLR66S3ioD4P9AWkRw05cvv3pmswXOcv3r7f2mLw/LV4WsqxqVwnLQp4MXvHxJVvvZy4iXlXlgEjHAK5zoTKE11mGqvPxDnHexM41fjEniXBB5kdLEIZCzl18zGvh+nzPWKOXfyhvrLsWJTX/NV7TqXHkG54xOQc2fO+7nB106OmcydP743k6358p72lPIKvKyBtBD5G7DsU4bs775poAZ8cdbiq8vadi4cTPcjH7Xv3bV+xr7zy2umo6cER+85/d7u+siQH3HXEvvI7j8hty04KI8UOfxBATzFvxyl6ilOxbMUY9MHepMjVH5dX3q7O3xNJT6GEu0kuZITZOBsb5MrMaf985PG3Cr8vDr3JhPrH9JD//td5YPx+/zRFYSmxZY9ay3azfEDSIV1YIlz1uY/fCUgivb157bTx3gbH5tIByd09L+iDfCZ7qnnVK406WX2zJGchy8NI/lCVHRNASwpDOt8lLCP/Gpz0fHKYoj8rPML5Q+EqIwDDAjNQGCUYMWMDxagL47QqcNCkmwW53sxl+ZjCxkcXszOOCE+3yENH9sTQvGxaKgdBNmHm85dDh5fI9bStZgI40lfXriTe9UgCyUevJZl9W17ZHg4fmXc1VV/wpVKnum791Xc2fC9I10Ik47KjvKmlOj0mJupkfesPOR6wJGup0nltL2y8f25HmO7rpna5XGF21p9lgytB0Qi9Pa0/VqXHJObdfszKx5bcb7qWExUOlkeVH8OVgt9+dOeHU3tjYjx2XKq8dkoXnph+7VFTyfaIfdxTlsqVd+9wikdN+UkxUR9XN5EekpsExW6e/GD1vqqmR3dU15D0JrE7xFCfhXPmeLV8Oe2ff3P/f1o8/+97c5lc++q8//lgFhhbj09VlJcilWtsWWtrS33hvg8itsNUOw+XpgOf+cMxoiXD3n58p/XH8zAg6V+f1q2SyFXtSuvv37959pAuMlJoJyr3TuujR/W3T+tiTt981FoYDqqGsSIVySkaL8EjVexPGBHJd/tiViedzj+sMqG52cyNlanZCMAC+Je1sp8rwdmCZa+f0S9aRbl6HfoOerv5WOV97pal/wDzgRWTT0vFIMgmjHD0fF5SAJHr40c2mQlkGbxvC8mAU4UFMPOu+7ZACkbIjGNPtUG9qzQDWgVPCx8hFYpZgjgue689vnN2++rwmNXhOd+1kEslumpsrS/7gE2yVvliOP2Hx2zco862tj963P7oTrZaI0duP7q4g1bJre8kCxW4SU2FkVAgZjXcgB9ySB8eVe8VWw4Ohv1wWSstLH3KcnKFU9R/DeoFSwB378UlECye+dPJYbNQGGFIVQcZlrv3zroxlZYeD7x+egnwL6eXMAsoWVFeilyuj/hOAgEBMYeOnK3nBtCwt2SdcudIUhhzjEI76lfaeJ4tMSTtiKPEigV83dp6LYfehe2FPxIBS/oDcoV7BEse2ogBMCzSUTIxG4EfQ/1WzFV+p7rd1/v620b/5IA8vhvrb/7YCo+YO2fJ1CIqPQynyBF0q5iWykGQTBjJUYrtZgJZ9B5JMAFdDJP6g8uVAVoFpzpzbmjyrgFr18OyO03G8TbjUjIsMw7faX8MT2LaGRgIuVyjSu63XzvEf+Q2mYaNGMgVzgiugC9zNoNY+JY9ksrr7zd9d/IQPErpuKvIlb9nBF3ZIwvkSseW3iEyzqqDDHL9P0qd//njlH/+ZfL9Cp/qNaurw1f/R60XfATjs4uTjG1fAYVcuU6GQbfBH+bows/fMSZXtt778X57S9XeILEdk1cqzSvlCi20tzTxd6HqCJWr0JSLV0zy13ea7oMXUtlj09ksjpKJ2QgoXCsDHCzIWGEUkF4Uxx5yrz1giSFOJ+UVMTs3CJIJo2zNdjPBTO9KWjE2QNNmBW5893bYuguQZ1oFy+aYJpvsXck3HNcOsQc8AdYqLfVHktiSeL5L+KX6+1U64ShAvsBsyuP2P5xclY2QAVXIlS3GpBcojj547DsnySGPfdXtpHBOJZyC2/FyxSSnILfkuz1m3yT+DrFUdZB/ypn8H5Vu/9/dGf/8+6x/NkxlTpWo9++z/t+/ztD/izsUUFQRUJXrCuF7o7DzMIDqcvUNZt+Rgq+AWkI7Jq/UlFyJu26UffMnkWvAPF964UJ/5MCwSEfJxGwEQJaGv9/ASti0d5UJjLD9XGNT5Q/3v9snWFSnJT8IkgkjORrgQTcLtpoJANmvrn375cSN4Evle9cIund1ofVNDhAjePlnttAqII6Li9cHIKTWxqabP8Le8vO0svv1X5O9DVkSk/Xw7nMtj5t+bDpHdptc9RWH7zQ9etza2ErSHy+REZQ1krNQRa7gQkH5j+p/rL/Tcv+7j1dIRz8gF1aM9+/82Fr/YyuVa9iR29B+feVJmL58MXqKJqh+/3FTCb9QN+MmsbEVUtVB/uGT6S3nZ/7Pv3oQlf6b5z//dRoBMl2z/6+/esAhKKCoIqAqV7r1enTndhNckbHFcNTXTa3EGUKB+iMS72rySqV5Q18Umf0D+Gq4C61wX+jele/PYtik0DYhLeF6K4XMZskoQWpiNoJWu8L9wJ0KFlgb92/w37DA6G+wanKdH3CyqfN+NazFeItyWsoGQTJhhPkGl1N/NtGGMwEg3wz/QafyzfAfdOSbYfrXjFB/lstgv9vCkniB3/Yha1UFj6DIFUHGbk9wcBi3v5IQEBAWEyzfCJlshAG1IgM4tynHN1zeGpRU+YEOTqE46SC4L2JjS+8QGWewKMtMn58b7956dkbXTaLP/7PG7cFJV+B//zPZzYLxLydnQAFFlcHxCgseZDRImRUqY0uw+EoFfMNXLw9WGinzlitvmYj7QukoDTobv17mC/rc4+cDEoUU8oP+lmMIyLX1GmyLlHYB04Ng2RCZNxMY3O+u2TsYLx3YRhbG7HdXqMxaoX8VpawpBfarsI+FVGFH1KF/eibcIQYZZDeVPz2bM8crKsQj/73pwt9FQAY+ghEOCcUcE/Y3epJR4maj2kAJgFAvvbUQFsYgXRN+VZ3Fu7O/rq6HZQX9wWnYsWQmCDh7+DDvChnBSL0rtOI6y8XN29VNG3/SpQnoH3B7s7Fl9wlSFzcfh/nLdVvB/gKeGyU+pbNx2AbKKyY5N0c36OrDRthwJnDeFXjN1XPKDPy3XTZjyqzXYUiF4RUHeebripJjHIcfKBte4LiJrh6vQn1X0sSkqfNc3fHZbwPYP0rmx9aD3CF+nB3jX13bCu5fb/MzUDpijjFQtp0JIFdSk7VCm/Aiq2r8R17WwV75Met1GEY2ntKxFfJwFJ6sZC1kcvfiyIgDBVNZfZQgo+GBGp6ZQLwr1HyNplx+8mxYak+e6TcZX61mCa4zFtHvD3xgnSMbT7U8lIHtGZSH56uiHYfH1X0RzC6X6T4w00yPEslrcDYO30ygckUQRAuMoyoHiZMPIHRJHu2OZze/JNrt0S56V/YZ846dt64W5u0kP+61yZ4khymmmNp9SrwrzcGemFpJyufR7mh280ui3R7tUu/qifkxkLeuFubtIk/kSqBWzDt43rpamLebPMh1NlUwpphiau8p866zMcUUU/tPiXeVfmY6NiyHdsewG6aj1RO0W2FnckUQRAOMe20K+d8kgxTtjmof+TOi3VZ26l2Z1RC0MxzPPvJnZKCdYa193KQpc+B/mGKKqf2nsBieQ2CfMe/YeetqYd5u8obedY6BBe2OZDdMR6snaLfYTrwryUlSBtodz25oQbu27OBd59IcTadK8mh3PLthOlo9QbtVdrIYhpwkZUgtaHcku2HKQLsG7ONAxyTH1CxP0e6o9pE/I9ptYieLYfpZnjLQ7nh2QwvatWMH7+ol+Yx5x85bVwvz9pKni2HOyoN5R81bVwvzdpMf50y1iymmmNp/ShfDIqBg6UcBtDO0bjdktHqCdoZldupdp1LtTvVynibJo93x7IaW0eoJ2q2yE+8qsXJILWh3JLthykC7JuzjnKfNIzlMMcXU7lPqXafNA+EKVsw7at66Wpi3nzx6V0wx1UxK5EqYSlPMO3beulqYt5s8lSvNTWLHLMnP9Qma6x1kusxQ814hHvN81MvMWDIrIGiKiboW5qd4r/TwtLKuVvLW1XLU/MqAWSuXkCjmJ+Nd7376EsubKD/qed67Woibp9/ioNVvhIQDkIGPigJS3FambNi60U20LFm8dX/YyiVCARMsyiw/lRmuMHKEfHbq6ul1CuMQWPdFzSc7lMahA0MEKXm0SZAWEAjf903eSZHspFDnj++0P/795mFy9Mjt3ztvn5KWHy52VLbCSXMN7ENEci32wHthbv+8PA4AofYVvfzr6cn9Xz0HilUUk+G/ed2+vKzDn21YE6I8NCKMc5k+n+ZIamZ+3qJlb4REwCwMWk5Slp+36E1j5UFypVcvxy/h7Us+PX615lRmhLHy0jyVKympUobK9R3jdS3NU7kOUsaKfOnVGkjf2bRjHQ/kFWVYngjy8e/tjx4z6s8mOIcdu3i7MjuMP3r7lPnnNcybW1KUq/EyVuQl1zKkdmyU/9++eq6l2A34zwtOAz97/lo2g6m35dOXVMt76k6fvVL+SebBDTv/kFV845Md4KKUZYY7D94V/id8Hjw/1yc4PCo+eMV6qlJG+NtrN2fmnIC8al2Q3PEzl49nrGIfvTMunypmIqRlZixdvDljw+ZNbjMkdT1Xvrk1Y92akMWcd2XG4MWb968LX+M2g37kvCurItT1dVuZsGFnwuJ5vpx93hrSVPgqsmwGy4ylHv5LnWeEvLF152IfWmXexrd3Zrwd4s97V9YOrWuLPJWrit0wTwR5vypGavcKXR4eudiL5HnvSu2+u/adPJ2Xeyjcl6s7K+gDasmkFgZrJ2BxUg5x1x9/sNiLWrzCloeHe00Livn4dN7JnBi+BedpweH7TuWd/CRmTxWVq2gPCo9ZHrSY5uFEkWHh73jRlsP2QPlv9iWFzeIObQpbHuzsuxt6sicSNjJhO8gpTu2JhNUEOy9ci6xXQl3naUFB5Chr81RSeAA1khHgGwnm+8MYUn7fhmmg0v/rl9mtRVNbS9x/r/UcOPdCC82AYg3LO8/L+NPV8t0rYVJR+wx+dk2bP2XJ1nU7d765hKkX8PdYAns0/7nhOzdsjiCzjky2jLdXLuXqzoP9nS+ddTvf8CftTAlJ2LB161xP7lxTfFZ5cHkfN/8QNtun+MBObT7zrvOZgs3Je/kGxyXui9WlrlwXTb1rxI7kD8+WVMKkhLxqXZDriczP/nTmU29iidhTfC4tm5Ori+/eTy5WnsgvyMq/fPbi6XU+tK7P3pwrNafyv8rKLymEDPOutGRO5sH4w5cLCw56Q93lgncVzhW87ovKC8Xnsj4/d+LiuSj/+VN1p+nHgk/O3Dh7bCcsyEmt4nM5Z8r/9HneO0vnO6+ERiqPk3NdvnC1JkfH9Vn12i3Knzpz2W8peUIx7xqj2wvjtj1pH6SQV63L5Borte8A5Ty6SHw+J1ewz0oqb3r8eyfxwJDeSQcxh31zByz3m+60QFpJW2BtLie1HhNfTfz27VOLoWXaZn3jY2gBjJ0t5bT8piO3aRlmpHJlvXKZHpbf+Htn43lSd3pA9g+POxsvhXHluZbrz26HM8aWPepsrL9JetV6LiYy70do59GdH+9DenEHd156LbJzQa+Wk96euvn4cX3jI3Jd5OrqjwRBSUUjrD+Qqoye+fkA39n/85wTqLTrB48n51548rPn72df7rvjCZl/FL0Mcp3tOVdZd8e50oKDbso2fcGdsAmWe/HG8YyN1L4352p5bv63fyLGmsLPv8g6823O5+dOXb2RtZUo1i+z/OyZktyCc2TOXy1Jyy6B+f/JmcrS4k8X0fbBbdB5CPmI3cXle5aT/DvHak4VfEsfJPTcZqbgXePonIM0MjYl71QxTEfGG8upwzSoRRe0W6Pyy3eHzHde+8WpzxOoz4xwnu4LPTv+EfW6033fyK688HkCuMd10LPMTZK6XMk/pcLzlZTckF+ZFiHxrsK5NhdcKP5sMTyNBAs8BVl+/sE/XT0dBnlSiz4miR2GA1Y1wawMnJcshln5IadpHx0GxUKeyNVkSSGl0hIWw3eywS5McXaUeNcwMoMbzy+Ho16Hvnv0e/3XYc659Z2PWwvDoZ0AL1/ql/g2Z4VvD6POObwEZnxTHuRJm4/vnIyZNZ0ZacU91eBR67+OhJKzaJ56V66dxV+3krpEPzmV7IykzKPv9pFzLYejj6r3TJ8fA3J9/PhmbiStBfIDGceQPvgGw7nEayF1oQPkXMtPNrU/flz5MVe+9VqGF5Q/DLtcaiQapo1Ar6ARyXUNJe364oVfz0z5Z9dsWAMP/KsnOFVwsy3n3f5+2rntygzYx34SM0VRC+Zh6Rd7Fe2QuXSlIIzNNx+YYLDjg/zeT65WfriJut+IL85ePPn2bK4FMr3Z3rDgoAdpYWlUQU3uPtgGz3eeCbUux4P/kM1DIleiGqrhs4d3gHddACXMT718l1Evkc5SQatErsS7qtRiknPbea4wO/qN7PID0Qs5ERJPC8rhS0YTsfmxJ8oqWV1a8sYpcMKfE3KIxsC7fk69q3gueG4VZm4SzkvT1xdt/SyrAJbf4P+JtuW1Uj+5em7DTK48fapJ6w4ppd41FPJErtPmx+hShXGL2ZFqWB5SIshH9+/crr9JKE0COzfF+aPEu5IZzNwa806tZbudg3K+u0+k3vRj+b7wAFnLYR/l/9DaCh6P1CJqkbbpQnRO8pG8bkktUoB5V76doNPgvW/mBriQr4ta84NYeeHJAi3XH2He9X6Vjqu1et81UC9476ZzH8eA0oTz0rrkwUFa9iItwyWw67qZS+tyHYB8WLqkEQ+hP0NI3wtzZ8tgkKvAr2UzQaj/2Ta79Vt38K6Rb85U1PLOKC89ZnDXdKdL8/fyltD4M2z+EO8KGiN20HPx59RnwqieZi3ALAXls1rgM8lMJnmqBVpLMg+Zd+VKghDGgXCJByfpAoNUxT7XN5is6JLozEvaJ5UrrI0Ny0MK/YMzucxM/Li4JLe44G3BQnoDfpIvufU0rAe8p0O/ifOU1aUlP4xeNXfpKq+loZB6zl7gTBfD60hdrp+g7bPZW4Xzwlrl7c9vnDqcOHc25EGZVKViLSgDI1sSNZ8rT8ZOJ9SVphaMj6EdRgZS99n+sI+A0YPnHeRVyxNBwt5VauemOH8UvOt04t9ar+WsDo8JC4+BdHkQ1ef04Ih952+CaB/d2ceUQFr4CGTc/uP5HVCGV6a0TeqWSX71WfCfJENqCWoRe0JcevsPOXuuwUr4/OLprHxTYSTXh7Dw8HnTFxDvyvWfuyKvyEP5t4nLrfx4iXBeWhceDbTloPP18JQpSWSOlHtGcB3g2pkXmcU3wq5UbN8gHdyeEzO168xrUq0yYG0MLrfrhgdsYtM3TFO2AL7k4hdBivZBrgWZbiQP9ojk4pqsrZCHmUY1Bnay8wIPRPNQ+BhRKfGux4ijhjw4UqJMkmfKJO1L5iHV8HLSPl1yRlDvSmtSNZuVf3PVJkPvmpKaaaw87yF9V39+ozB7s2iZPv+twzfOfp4wlZQP3pDPji4MI8W20rrLwEjrkpKFh3eQkmCfuZC0L/hJ4VybTp69cvodX5p/fbkHu1rmvYM/PaHiXckT8U+py2ibkbuLJd7VyLVYkYfBgfw7m3T0O2EuVS3P5BortXNTnOTJ0cbzAdOX7IPd46P6vMhgGJ/wpN0roOSOU/kf74Bl5LyTTaISSDvcYtLDKyb7B5jxVKVCm3AWQcN0AQxr0QDf1UklreQXF6l3nT7fA0o+aq2/T1bCxBJ5icisLCPAa8GsoA90MUugDPOupP/k7LuPfJ0TAffCl/hP6IN4XlqXOxdxnq2FkVCe967cVbM8aSTcd74L34hixKzIr1ziAd4VHKlCruBvYe/6929c/vOc02Lf2QZ1iRpPZW5mM3Dq2k9364JdlmbmXr38XjCdjatgXp17dz6Ul3hXIlfeuxK58t6V6hbyojKZdyXKXBD2Rc2Jj0JJm8GZf7rCKZ/zrnBYwFmSl2JoBxfx7rZdcVSuX525HLR8vfSoojzvIRc4z14GXlFqAZG8V1B54Url2Ss3zhZkB8ykVXxTsy7WEOPFyx9+XiIveTm3oPJsMewHwJJ44ErNiX30KMV5+sKAjy5fgKYu3rhw8dy7yxf4kY+Vx8+Un/j886xiolJBrrT8ApdVIOOaCxehzXMH8stBrpxdDevsbHBgxATm+i4TykgR5Coa+SkO+Vi6/qz/erWL70fnGskymPCoKT9mwercOzC/mQUc72KxhdXsSx1wTfW3QYS0KUmbvHeF/BIdbR9o/7GpiaqFb4TixRbhsINlliWrD4snbbqWtRB6yMuVFAg/VUnX54T71elQS3Le5WLdRzdPklWuIFdSl5erSiOscR7r7kjr4Zdhg/rg2iwQLdD1gwe4VrDAMhgOBfjOUZTn8NWlnblRys2ukvfWvg5Gz+iTp5jlSvmH0cto+VSQK2iMVIHJVvw59a4LqHdNBSOVK8kAzLvSPJErVyv65Nmr0GZlYX72nnzOyMvVzZv1iaQW5oNWrAfFWldXmp+6MNRj/kK5faHb68vdZxmUn798ru/rYt3Zy7wWLlaWmb1s7uuBYpn5yz0XLFKWkeVf91i6bKqK3d7ySxaHRQXM4+yzgqJWr3hTUubNoPCYYPBmYnku77Wctw92FtLmcjrnjJeR55cFh0ct9jJaZt6K2LDl5NmksJtTV8hD/+VXOkj5QfNzZnud3+UMXhT0yQCVggWEqlpelocZKJ1dkM4KJPPHWHnr8nPeJPPcwC71rt6SvBS0MxzPPvJnZKCdYbGdeFeSk6dM02h3PPvInxHtNrSDd/WmOUwxxdTe03GuJMe06415x85bVwvz9pOni2EEQbQAeFcf+B9JiY75vDRFu6PaR/6MaB+anXhX9tlFcgzzjpq3rhbm7SQ/ztVd/Ix5B89bVwvzdpMH7+pDcz5ghQyXl6ZodxS7oQUyMgva7dvOvCvkfNlngxTtjmof+TOifah28lUTfMYUU0ztPwXvytQsS+kxtDugfeTPiHYb2olcKfQY5h08b10tzNtLXpAr91kNtDM4u/BPfBE7RHqnDNC8fdxUjwAEQTTBuFedZyAIoglQrgiiGVCuCKIZUK4IohlQrgiiGVCuCKIZUK4IohlQrgiiGVCuCKIZUK4IohlQrgiiGVCuCKIZUK4IohlQrgiiGVCuCKIZUK4IohlQrgiiGVCuCKIZUK4IohlQrgiiGVCuCKIZUK4IohlQrgiiGVCuCKIZUK4IohlU5DrRxcNluvfkGX6u7osULxHXEC7uC+ESnKd7T3SZpbhABNEoSrlOmuo11WPxzPlvePosm73wzTmL3tJu6uEdPHNBEFwOXJTiMhFEi8jk6uruPX32ktm+INRljpTCRcGlSa8UQbSIKFdYA4Mj8vQJ9vQNhhQmuiPl4dJwVYxoHVGusF9191rq4RPk4R3kSVNHyrvPWwr7WOFiEUSLiHKdPMNv5vzAWQsCPbzfcLx0xrylcIHCxSKIFhHl6uq+cNb8pTC5Z1HRylNHsMMFCheLIFpElOtUj4CZ85fOnLfEUVO4QOFiEUSLyOQ6wytgxjyWLnG8/NDk6r909bpFsxRGzeMZvPHNxbil1wxKubrPXWw63RST/FXRlbxT5+cuXGa6pPXp3pq/93d/v9dkGctTS+QavetYxlqZ5cQPjx+WbJVabIrXqg2pGXEr3nJT2EXgebHKU2LxDI7b9WHGhmB/3gJ9LjhK+TQ1zswny6c1v7deTlAYLcR7Ufyhgqt1P1z9Nm2r0BnAf21qQcn1umuXCxJWc0+ERfFHj36ya5FY5q24Twqy4t8i+YiMo6nRvB1RRyZX9zmvu8993XR6OLdwU0wSpGs2xoFl7sLgvC/Pg4A3bUkyUcuyNCTnzL9c/jDEZBnLU0vkCuKs+1RpGS65vvnJraZ79+7U1N25+1t7zYk3DQoknP656fHvnWKXAtOuPmy/9/MPNT83Pfy96cIuKnLo4b0yItdvy+oedj6s+3QxV90E5sp16/et976PVhgJ6z6t+a3z3s8lpwqOnvr+zr2H1z4MJPZZu87e/b397q0C6M9puLq7BVGkfPTlh52P7xas5qtHXKl/zHfgs7rOmhOcHTGCTK5uc/ynz/aD1G22v7H8xi2JZy9WfnHq/ByfILBsiE78NPfrjTRVLb80Jm37lrVLYz4/U3712J51YF914PKVS2f2viOWWbXj+OnyfzmTnbCU1X3rvR17dq2CfEjC9sT3loYkHCy+fqU4Z6Va++bnbShXz+BdaccK0uJ5ZzjrrTfBMc56K+7Dgk9TNhKjV0LasRO7IkRvo6wiwS3+0K4VND8ro+zhvbN0ZkuBJevKD2+0Srrktnojp+r471sf3thF8tI+v3Xi599/+Izl5XhF7/oM3G80c9Qg16YLcZ4RGZ8eOyH4QEDWW7g6OPu9G2lwUvnKedFnde33vo8TPPksb3Z1IMv2ugLhueM2i6sF9vqf7zZd4Lzo2gt3m+4+RLmaj0yu0z39YFoPmoIy3wqNZPnZPm98caoYBAy6VS1/4L/o9b91df72oPXXXv3Trh//tbe340HrbwNP+n46QMv8kRSAow/+3ves9798Rmr94afepw8uwFHI9Pd2/tbLjur/9Zhh++antpJr3OV7rXfBs9XV34O5TqcaOJ+Hd2Ei3qHurv7qjXooUHev/fG9s8aqqLOr5J4RHw6nUHaJ8Oaxn9t//nYlyUv67JVRdu9hWYpYjOEWf6Xp8W9NdXU/1D1svZoGFuJdQT8/gwV6+/NR6pCVvV19ouRncNfE/5d8tlFoDZbBR+t+rz9Fl7Iy4Cp+K0tVGAlErpe/r797hW40Ekru/lx2FeVqAQq5LprmsXDQlMp1E8v7LF6e92Wxbud+1ZKQglyfdP95C8nrvu941ln14RLILytvfdp7Yxcts2w1sUD5/GZ930/pkM+icgUL0W3v93Hi0QOsJJ/uO/Cn0qs1DMhLz2uY2sy78j7k1dQb7XevbIAM0dLdE9RDLjp1txP0QxyON4ih/jSd34ZV1HBLBSemuuZUkevKz74ni+e7N9KYZyY9/L394W9A5+Pfm64ekmwRGdFn7/5259g6qRF62H41g/ZN4pANewtaUulYQsk9vsriXVl020w3opKxgh0pte+KIB9Brq2XM47W3SNHU240XT8aRywoV3ORyXXqLF+Y1tNoaiIPcj34ybFTZ66ASMCv7krL8lyw1Fj5j0Guv5az/Plfn7VeZPY/3erT386i+WUfflHV/PdufW8fcbkfg4WX61SWoXVJHuTK8jRlbe47cIRq9YjCbpi3nVw3Jpy6cefuw6Z7sG2jk1iqJcm0o7OTzkWDKgmn68B9AQWCD1xx4oeHD0vi6boRHBo9+sPlE9RzqsjVbfG6lVEZJ2oe8ttdsc9ui+OO1vzWfv2Qm6wdKPDzUflXUNK9K+R5uRpcoCjXtALWYM23Cc5xsEHlqtATweODtiYZq5RvoTB4aVaMDciiYz+Db0+4eu/aJ97iEKFczUAp1ykzfSCdSlNj+ZyjBXmnit9cucFEGSFP5NpWzvJUrsx+BOR66xDkD9zoHmivOrI+yAcE2dPXAHKdcqih5+mD89ACWCDD2uSPGp4r4t14IW+qP7aRa3TBz7/Vn95FvnoFCZklV5UqZDu6mrDUi9aatasEXN+pOM6twXaRHl0pfFdsIFce2O6yjsn7zE4kawcK3D0h//5JTa5qFyjK1X8pa5B+TZ12/TdwzkJr/PVuLPhZtkgWHgRcgVlHr929W3/3Vpr0iYZyNQOZXKfM8J4y09t0GhEVf7akMje/eNa8xaZLsvSjWpDrdyx/vg3kyuyHqVwh/13r067SN8jRzWcf6PsaPoKjnFz5DGsN8uyo2lnMSW0kV8hwPurNT8gXLWbIVa2KFPo9atNl9gWvEaSnmLXr6KlDdL0NPrmonvuuVdpn/4SrD9uvH5U3SFTEn8V/5WoiJzW5qvUWVsX3vo/j2pGwFTbD90pSuG/UhJUt+QrqYd1R7rsrssWVydXZO+v6b2zzjHK1CIVcF0x2X2A6BdcaEaWDNPjt9aZLspSXK8kTuZYwO5VrJuTjitsGnvTre7r17bW/dBJBLpicCSr9ezG0QOT69/OsNU6uYsuWphbK9XfYAXKQaQQbv987rx+FPR756eLhvaa79+5cvlFvllzVqkgAHwVnaX8oPZ20wNFrZEdKDpGtKenDuqzr99g2tf0x8YRUgaTPnPHhb011RQnMb0uB9fa93ztpgdarh+ary1Wtt7MOXYMOQLOXicakvJn6fT09RHr48O7ZVOZUvTccq2vl+vN75726T8W9Kz2d1yq2rJDJVRjwx5y8EQUyubq6zZ/sPt90Gh6546uiy3lfnps51990SfPTkJi0d0MHKTP01BK5qsF+raF5WMpyi1izsaLKIJCFrhV/aOW9aMXgPVHrrfLvNKSQtb34BxsC5Fz4V1M2RCHXeS7TvRw1HapcEWS0kcnVZdpcmNaOmqJcEa0jytXFfSFMa+epcxw2xX9Ah2gcUa7kvYFTYVrPnjRltgOm0+biP09HtI7Eu073dp4277XJnpOmeBqks7Vuh0vDl78gWkeU60SXWZNn+r822eM1Vw/HS6fM9MdXqyFaR5QrMGnKXFc3n9dcZ010mTmRpiwvTbVod3XzhUuTXimCaBGZXAGY1sTHTpk70dUDJvqrzjNpCr6XpK+KeQ3YYRkMFwKXg1pFHAOlXAFYNMI2b7LGg264YtANxOFQkSuCIPYJyhVBNAPKFUE0A8oVQTQDyhVBNAPKFUE0A8oVQTQDyhVBNAPKFUE0A8oVQTQDyhVBNAPKFUE0g4pcnafOdZvjP2tB4JxFbyEIYj/I5PraZA8QqqfvsulzXp8yc6HL9Pk8C8S8mySPdg6t2A0tlraA9tG0y+Tq7hUww2uJixtfDkEQe0KUq6v7fA+fIGJlapamTOto17rdMB2tnqDdKrso1xleAdM8/LhymGKKqf2lolzBtU6e4UusVMeuEk1j3jHy1tXCvP3kRbnO9n2Tilg4hnnHy1tXC/P2khflOmfRW0zHmGKKqX2mMrkSERMrpphiao+pwrt6g5XqGFLMO17eulqYtyA/1cNv1oI3PHyC2B822Ba5d4VTcmelGcw7WN66Wpg3Oz/Nw2/m/DemzvJzdfcdDlS9K6aYYmpNCn51+LQKKL0rOSt5WpDUWH7SlLkvOk957o2JTvEvPhcx8cXZU16bMtdEeVP5wLjY5MS3rKvrtsAvcnd8zEajZQK3xyYnvGVoH8N562ph3sy8h3eQQmC2xQzvynrDWyZOmjFh9ktOXzmN/7fxTv/NyemJk1Ov04S0519xdlctL6QZRZWlZZWXysQ0d+8C18yGnqd/L1YrL28nDqpzdYtyo0I5e8Yd/ZO279TK0/QgNP6g2Eh/jPVzDNlH/oyObiciMtCYDVF4Vzgl3xu1/MtLpj038WWnLCene07j/6vT+CcTCP/dafy/P+cU/sLLr7mbqFvc8KCl7UFL98CTp/p2yLQ9qPjE24XIFRSlUl6eP1zTN9ByExR781Zbr/6pviaT2DNqqVxVytOM8CxQ2IeWf+dYTelVgdPhamWEvF9mRWnx5/5Su+70qcz10jKU1JyrNTk6lh/kvLSYehkuv+LzU6x7xZ/78fZwWXUgYk9xDdcTUr5izwpih7PwBQCTZ7E0z124ehkLrm448twISOxgOZYqK2NGXk2ukVEp6fGUrZFhroG6rfxHzuLu6x+5V8irE5rMChh4V9YD6VOETye9Ofv5oy9MCHJyKpkwvsNp/D+cxv+vTk7/QRX735zGl42fMP2FSd5zZLUM2yn5+5O+hgzBzhQVuJ343qI/vCXYQxMPFVUWH/84PJBaph++1QcS5Y7mNgz01B6GPOddufZXRmRevFR2MSNyFXde7llA8mTZrNvuR+ybIj+5WFpSmBK5krVGUsN+DmKHuS7cXandwGJol85ai89LVJejU7FLLKB8rm9+meWlZM4t8IenxrFUWlI4uh7kSvUMR0+fKi6ncjVo3/IeGrWTC49QsUss4tlteF7r7FSuKnZFKreryDXzp57+XuKo2h7UlaS7ppTX0TzQ2T/QUqLbWdbV0912/WYbfGwsUFNsbHlLX29dZW1Ln17pXelZ1dMXdzlPWPzC+LcmjL8w/vmWF8Z3O41/PGH8Pya89rvzzIUznTKdnpv2wgsHnE20QNKSB0SuggUU1d/b2U2up73vmb4hl9hjvmvp17fXVl9v7tV3V6eQkuBd9beIR/V2DdxX0fGsvWw75Jl3pS0nFrcN6LuFUUgkJTm5ervQBmsyV7nSYj0dDRU3f+ns761IVeuhWSnMdc4dUUtqTvHn74AkqENjvhRmJ3MXnE7AIjg9DtYClQ3/UTgLccu0GHOAgl3wfoJFmYrzjFpWrIf+vHNM2tv1/tx5T+/JPL1nxfo9xz7fQwvA0cHbpy1A3Rzwh6B2eplQhbcrr4U8KZgFHgrCtQiDQ/2/0LJZZxeGkXo/ZpeuHagF7sjpHOjMsVQ6krQ/8Lw4dhrq5uhoP7k7RdY1tO7pd7jWBAsH3+bgqbpc28qVRiDzp86OqijIBIb5U4t/WZe+IY8cCj2QW9nc0tZ2vSAdDqXV6tsrdcSeWqvwrj7iud0leWp//vWJTs+9Mj5g/PgZ4z7KyljYtOiVf3vV95dF2XmfjBs/zun18RNeePm5N18Ryqu3w8uVsxNF9VbE0Pzxv+rhkHtocduz9kqqN7fQSx0DdcehLnjXZ0/6B/TA02f6jqqUQNImlWs5lPSHZrtrd9Lz+kM7T3/Jh/aZXAOhLhEwlPeDYh3VkaRlbz8yOoX0LOrXa9IO91uqrvfhBrO5SP2DpDzMSzarhHaIk9nA5UU7a5AvQyT3Ps3D1IH1ttgHZfu8Xdp+KVdXsMt7K57xdPiKz3OOnc7JXE/1TOx8+2otk1SoW5MT7wO1TmWmClfE1SXlSbfJ7CfSYvsFUgWGiLQjXh0djUPiaMiuTuW8kIeWzzBdgQ7ZeeGJQFqWXSncEcjAk6gi59AGKAnNkpGBpwNcMjyn4FAx1w5tGcpz+xruvNxTT9IH9f7I7CpyLWjr6fjpekPz9aLsENGuK+3ovZ4qfAyL0OVVdOjB2cLHjMq2upK8+P2XGvtIGZjk7WXU6wYWKxfDfG84pHmnDS86vfqqU9xz4yaPA33Cf+PpfyQ3bpyT34QJE16dsPJlobx6O4JcmZ13gFyeHCLK7OngFgzgcltK4ChZDHPeNTQxF1TaXQteV/CukGHLY9KO27nGp6QwfRZ0NTYPPOn7a1YgqUvK93Wxlls6BM/MdcaSPDcteDvxrsxLsKkjlqdy5fKsLpncdNbyRpoX5tkgdjah5WXkef6M3G6Q+BC+t7xfoi2AESYodyhc7l3F1qQti3lWl6/FXRFnZGWYP2ed4erSYlwPecdFkKwFBr86TkUyO3cummfK5O8I1yW4KcTI+sm1wB3i2+GfLzxs78rlpXaTeRW5ppZfL8uLT8kubNbrm4uZI3VNqe3sro0XyuRUt3TrQdX5sbwlNDmrqKquAyY/+OFaWAk21v7U0t2r5l3Z00KaUvuLf3zVacULE156xWnB8+OncCp9Ydy41VOnTpg3wcn1hQkfPP/86olCefV2VLwryJXPE+96pKZvoK5oX3wyR1QoHGV7V6Gd8hao5e4jeFfIgKvkzwtH9bf2M+/6TN9WWdpM97q0vL7hImk2hTYes4mWZ7UkqYn+c3lulvP292Fy+NO8/yHhSU/Lk/lKPQmzQEomDfUngoWkG2iDgkW6HpPaef/Daqn2U3pGmHPcrBV7y7cAZ4T56uOnS4Weq3tXRcuQcnZaV6jFXRHv97iz0DPqzhA1srrxZ9jICF5R1jJXS3J10lToD1GRZDyJXXZ1MP60/+yOcP0kGo7n+8m1ALXE3nLeleVZyslVYpGmRuwqchUAifb9lEHzEZVdPbVHZEdhMVzQBsvGCPewjNreno7m0uPZ+Q3gb+lR8lXT3giFd2X9oE8Lrk/S/EuTpj733MTZc3wmPP+Kk8uLTgFO44OdXvOfcN/V2ee5l59/7tXnJr784rzJqnXFPCdX3s7Jlc9TJec3D+ibz4WwMoGhsMFwdQMNg8Nk7YTGl3XRpa/gXX1cC37R97cVhpJ2ouBo/18PQUnSeFdpoI9LTGV7P6wrvGFlou//JT+Ua8ePrKiN9HOQPKcu3k6e5bSf3FwUy7P5Kq2ro7NW2SY/56idW90Z2CGl3k9RV5Fn60Cah7OTjvmQDeSx9+lR6DlrgZ+vtBbvXX2oYIy1LO0tqStokvVWUpetLam6rp6hLlcyMsRIjkJecqWkjHlXp6xLM2xFzd0X/o5w/YQCRMOsn5wOqZLF9smzhvWTOxenapqX2k3mDeUalXqArYGJGnmPCvOWW99Cgf3ZZAcLHG+mBYob+2HSgiUMhEDlGuZPPvrCxJZ7V4OnhTSdNNVrwvMTn3vuNWACAfKvApFOLzx+663Pprk99/xrUMZECyRV9a5CnnhXH5fAw9c7YJuq7+zW6/t+KQyHo5K9a/9AT/dfC5NXQQt+Rb88edp7/UNoLTTjZhc7+qS/63omOSqutN19Iiu76K51FSvW093b069vLIpR6eFgKUxKcSHH+RM17ypf8nE7NLCTycqMdK4TDyAWI78MkXYE73rmHWiHrrTl52V1lX3j0vgzXLHiCupdpWcBC0iCWUR/IvWu/Cn474FUzsLVFb0rd3XiWVhrUB5Uylo7BZ1hIwN2sYfcuFlwdcIACn6b9ISrS7wosRjxrtBPTof80wpKciel8P2R2vk2B08N5BoG862H7L96Ib2eyUm0uI2ucsUCvS1tXT39vbeOw95Vx31p2t3b3k3kGna8mX63DC20KRfD3NPCnWQM8+tC5u9YNTkq0Dlr8/TLf1z08guvfvW+9+tzXPMi3DI3zQzynWWirqV5/8g9sFg1XQbyITFstUztoUnxuh1shhkrT/KBO2KTE0NMl3HEvHW1MG9+3tC7EshvrcmS75kMMCgQEisvDwViIyFjgXdladxat+8OTPOeOfHvJS9NeGHinZNu+9ZNuvuVi+4dNxO1MMV0LKSzFrwhamwYMPCu7GlhMvVbOPelV6d//f7EV16esiv8tbAlUxf5epkojymmYySdtSBo5P7EHz670LPSFPOOl7euFubNzU/z9J85nP8oR7kYJtBzY94B89bVwrwleVAs+NiR+OfpIF/J0wJTTDG1r9TQu4LPxRRTTO0xVfeu1LPzeVoO7Y5hH/kzot2GdqVcmZXPYN7B8tbVwry95A3lSktIVC5L0a51+8ifEe22s6t6VwRB7BGZXBEEsWfQuyKIZkC5IohmQLkiiGZAuSKIZkC5IohmQLkiiGZAuSKIZkC5IohmQLkiiGZAuSKIZrBYrh4L3li9fqvAwqWhigL2Q0isyaheCKI1LJbrwT/mfll4AVKgvPLPv9z/t/iUdEUZy0ipbu/vqkgxsFtLfoO+p6EYMsVtzwzflT58zPVdtnZD3JYdu0PCIt1nL1YcRZChY41c12zYJuTf3ZoCoh2SYgOzCysvpdH3lEtel6xGyYMnQjAvaV5OVEFVaU4yZCyQq/HWzGGm15It2/fEJ+/bsCUhLDxmc1xK0p6MiKj4YX0pHjIGGapc96b/AZbEN2/Xg26FMqYJSf2muKw8d780QC15CTKsXUkkyEqws3ciR0blXCotu5TBFrShyfH0TfxQKypJkg+lL03W6fwD03NLyrNixTUwkys9Hd8IPQupQvJhEbq9EfCYkLbMDtGAQsXHD5CjtJZ/5JHCsqrCHPWXO29LSH1nU9zbazdLiY7buWpttKIkggyFIckV9rHwEWDLY6GMCXZW9j7pe3C9rLal75m+mwaozfyJxt3wzarsIhGoSIzW6iz3vFt9A50kVBwY9bcySZSudhJ3g0SCrbgpyefQEJp9XS3d+va25uIU0alCRt+np8HsSMD1WzTkgcSBH6FR7eQtQ2ux5dLosjuhJI2oW1dZfatN33nzALsQKdsS0lati1bIdf3m+DUR3EAhiE0YklwFwGKeXLM5hUD+eLO+o4rEz+PlSvUjLob9Azl/mEtipWdDhsWbY0ZpnrbAqRGQylVYDO+82Qvl4XQqcpW1FgYF2ivJWhrypSS6LImHK4b6U0OQ6+a4nZB+e6VSl7wP5YrYnBGWa3pF90BjEdEVCaDMFGJMrinfVDR0dYJ77OdUZ1KurAWCqlxp5DsSrm8wuZI+yKPLgr+91AjuFxYFRVw0MQWxur2h4THhUbpfH3RVVNXcvN1AvOu7sI+NVZREkKEwwnL19T/eDJ6ws4OE0ypkwWdV5Zpa29nfVZGpA58GWrKNXGm4PjPkOlBXRPbVDHGjm1le1z0AbpZ+lLEhOjE8agdINCvni780/A10C/nouF1vhIQrSiLIUBghuUbllOfvZ1tHfR3sD6VHRbHpyFdNLO4lqIstld2TC9tgMUxUF0G+EGJGWd6UXBvyWCOlHc/YthOMLOO/v7qd7YrlrdHossWcFw0MA6N/yoF49p0TDdFJMnKClq8HcYJEpcTvTPdcEKQoiSBDYWTkClvWZ8x9ZcAe8ikfprXjJ+JgJWIjX0SRow9KM8tb+skXRZ19XbeaezknmVrd+fQZVGwp08nyJuTa0dVDzvVM31GbQSVHouLSij0dtbc6OLnKWgs8Ioku21YYHpNFQsKSFQGkjUVsW6tEl/yh9NsmcLYR7+oUZRBkiFgj1/zC8weyj0oBi2nv6hqqoz+K5N3qflAaGxlF15m5DZIFLY9/5F7+T5GgmMEPJ+w3G8O8CUKT42lsTFMWQN4adENWhvzaRH/1ESxyZnsHJ+zcv5IqNuLd+LiE9/FHV8TmWCxXjwVvgC81BOyKkipEVrX3d5WyLWtgemnbgOrvIhplptfSxN0fbY7b+e7WFNQqMhxYLNehERZf1Ey+7O3u7enrbSzLVv2iVbtMmblo4dJVCiOC2IoRliuCINaDckUQzYByRRDNgHJFEM2AckUQzYByRRDNgHJFEM2AckUQzSCTK4Ig9gx6VwTRDChXBNEMKFcE0QwoVwTRDChXBNEMKFcE0QwoVwTRDChXBNEMKFcE0QwoVwTRDBbLVUPxXRHEwbBYrgetiu8akXmpMFP+3t3Ub0qPi69B9E/5pvhmc0vzT6XH0+nbQ3UZRVWlZQLfkNBS7gdyy0iMOb4WfGR2IDKtqLaure0WWLj37qu2oIL7nIBlb2/asmPvxujExUFrFEcRxH6wRq5r+NeCQ97M+K5C4AwRSUjVqJIH+qf6ltrqUhKZbqClJEYRq4bGpIOS5S1Pn0miS8FH7vX/xW0D+u7mirIqEjaOxsIx0oKM6Z6vb9iSlLT3o+i4XWERW8OjdmxLeD9x10dvr47GN48idshQ5WpmfFdTco2sosEvuPhxLM6FNICNhPKWvq520S7IFTJdpUIsVi50nWoLMiJjkjdtSRRevc9YtS5623tpb6yIUBRGkFFnSHI1P76rCbn6l5FQyBHSQwQjcn36oLSgTd9RHcV/pHLNq+t/1llbzEWy4Rhcrlt27Fm7URlGGXh3286312AkZcTuGJJcBcBitVwl0d+kkKXsExpKB2gsYkamz+SKDlgww05YkKuv//6qFij/dKC9oTyN27uqtiBDkCuLy6pL3tfUch/litgtoy/XNKNyVfeuRJ+ZP/X0N+cGinJlhOwvr+sYeNL/oDgSPg7uXWExvHYjCfRYUVUD/PqgKyvnC/gYvW1n8IoNisIIMuqMvlxdC9qegPakhwgm5eoeltug77xZq5ArhThVGsF1cLmGhcdGxiSBPsOjdH9p+BvTKrA96QPfAIydgdgddiBXKrCe2jwWLyckp/ZWSfJgcvV1jSxv6RvQ04/+mT+1NPAb19Dixv4BGkJ2cLkuXPz29sQ0JlGBleui39u5X1ESQeyBkZPrk6ewt2RQFYly9XWNLa7rJjtP2GQ+6e+6nhPJNKysIpWru+9OEiqWfgzNLm2D9uk2FbavN4/w3y0btqBkU0xSeCRZDwtEb98VFIIrYcQesUau1sR3NQMSUtWceK3GGCwEqypTZ/ltS0hbvzmBaTU2PnXN+jhFGQSxEyyW65Diu9oloNiN0Ylb499P2vvxshXrFUcRxH6wWK6OygK/kBlzFiuMCGJXoFwRRDOgXBFEM6BcEUQzoFwRRDOgXBFEM6BcEUQzoFwRRDOgXBFEM8jkiiCIPYPeFUE0A8oVQTQDyhVBNAPKFUE0A8oVQTQDyhVBNAPKFUE0A8oVQTQDyhVBNAPKFUE0g8VyxfiuCDJaWCzXg9bGdy0tyBZeSqoS7tUI8ZVd+o7qeAO7+WD4VsRhsEaua/jXgkPe/PiuT572Xk8VPypf6i9F8sZw//2XKopEnVsEhm9FHIyhytX8+K6dHV36tnIWGFIm10Aa5rzoCPdG79Bk8KhPwKOmpEeF0neFx0ZyJd3DiJcuu5QRyQWD9Y88kFtSVZiTzAJ2KMDwrYiDMSS5Whjf9dL1bn3dcaI0Ua6x5S39A50kunmvvv9Bcayva051Own02NvS9qAih5TkPW1ycdvAk74usHf2dZXqSGicnv7expvVtzr0kpDqIhi+FXEwhiRXAbCYIdcj/seb9X0/ZYhyDStue9Z58wAtE5bbMPCkuZgVFhbDQt6/qO0J1BVjahy51TfAxE/zXRUkSKQMDN+KOBgjKldXdx14yPbKZP6jPEhcARWkEbnyVfjCJLwVc8uEzn6ViFUYvhVxMEZYrrD6rW7vb7suyhU8JF+s5AHINc24XPUN33AlCSDXrorUdNjiUlSCWWH4VsTBGHG5MuH1P2Mf85sH9M2X6BdFyaUdz/QNeWCMIF81VbG9qCjdgjZucwsL45T0KPf0iu5n7ZXprJh/IPflkxQM34o4GKMgV9fA4kZerq6BR6530Lis/c/0HbXc1jS1uvPpMzC2lOlEubqHZdT2ckFc+7sq9oOjvtTYN6Dv6+2EtLt6J2tcDoZvRRwJa+Rq8/iuIbHpW/nfZjgCdVtVY72GJitiwKrUlYDhWxFHwmK5ai6+K4ZvRRwGi+WqUTB8K+IAjBW5IogDgHJFEM2AckUQzYByRRDNgHJFEM2AckUQzYByRRDNgHJFEM0gkyuCIPYMelcE0QwoVwTRDChXBNEMKFcE0QwoVwTRDChXBNEMKFcE0QwoVwTRDChXBNEMKFcE0QwWy3XsxXeNjEpRD5mFICOMxXI9aHV817IqSnluKv/m0dRveCNBEvGVxKSTBoAVqxflRYVyRsNirpFHCkl8OpKXnJGQy8eqtJzylqcPipVGkbm+y9ZuiNuyY3dIWKT7bHx7GzKMWCPXNfxrwSFvfnxXfVstUc7N5va+Z1zkyJIHLNIco66EbySltrN/QN9RxaJLSqrX1rX16p8OtJQkU/uRW33PnnTXCsGad97sffKUC5ZD3idOw9UxKnK4MpZjVK4zvZZs2b4nPnnfhi0JYeExm+NSkvZkRETFY/BYZJgYqlzNj+8qvsU/srqdCUASdllK/M3entrquv6uUj6onLQ6iRPJRXYGufZ2dgtRng9c79b38DGyZGc0ICSWBI8NSf2muOxSGnHXYfHHy8UYswQulmxaqFG5bktIfWeTMiBldNzOVWsxvB0yLAxJrhbGd+XFE1vdaUqu6aA6kFx+87P2Sm6hK9deWGnHs86b4IqJXG/VdnEhJ0HGHc115sm1uO1ZT3dXZ8eDlg79k/62ujZ9Z9sD0e2TAB9gJ56/vW/giVG5pq1aF62Q6/rN8WsilEFJEMQmDEmuAmAZVK7SxTAIiWxfQa5czBtY+tamscJkJdycBZmCticd1SrR1sWPNN7k8ap2GmYyq0HfWEQtwmJYaLz5klCXAXLlIzh/U9c/0FhAA7QTv92Wz2W6Smn4LBOLYUGuLH7st1cqdcn7UK7I8DHCcq1t6Qefmcx91aTmXcn+s6OZfj/UBpphQZYVcgXHK8o1U1faoa/LyavraysMlMlVWkUB8a7cUbGKqEzoGB8Cz4RcY3V7Q8NjwqN0vz7oqqiquXm7gXjXd2EfG6soiSA2YeTkyuQRAUqgzpDYVeQK+89n7bXc17l1JCQkWQ/LtZdX1/+ssShMUJp/UVtPd28PibxuQ7lyjt2EXDdEJ4ZHkfB2WTlf/KXhb6BbyEfH7XojJFxREkFswkjLlQmS220ayjW1tpMtRyn+vGzE6oHp+Q16XvC80gIvNT4dqCPf/ZqWa3JWSXEa/TJpELmSbnRVsFiyZGGsLteg5etBnCBRKfE70z0XBClKIohNGHm5SnaGoManz0TayslKmDhJvmJkVTv9Ehiqs40opD1ttfncrlJUWkgs+0sGmVyljZOz74fzPmspIUcHkasklmxPR1t7n7pcAV3yh9Jvm8DZRrwr+R0YQWyKNXK1eXzXESMkUi1mrDFCk+Nj6VdQxpntHZywc/9KqtiId+PjEt7HH12R4cNiuWouvutwM9NraeLujzbH7Xx3awpqFRlWLJYrYsiUmYsWLl2lMCKIzUG5IohmQLkiiGZAuSKIZkC5IohmQLkiiGZAuSKIZkC5IohmQLkiiGaQyRVBEHsGvSuCaAaUK4JoBpQrgmgGlCuCaAaUK4JoBpQrgmgGlCuCaAaUK4JoBpQrgmgGlCuCaAaL5Toa8V0jo1LS43WWvMQQQRwRi+V60Kr4rkMgLL95QN/9oKWh6svKLn1HtRAe0hhZtb09zSwylY3BUK7I6GKNXNfwrwWHvJnxXYdAcSMfstV//6WKouxBfWxUQVVpDgsAazMwlCtiDwxVrmbGdyWEHsgtqSo+foC+bh/gAqhmRIZxBQJ1W2HFG0hiopcWZdNiYRG66van+rqC9K2RYf6Re4X3dPuTQOlVhTnp8KRQvLxbrRh7xz+BxnQtz00VqsBKG45G7jxeXlryTbwY31UGhnJF7IEhydX8+K7+mT91Ph3obCPxVHsavnF1Ty5uo0vctged/Xw09MyfevpJeGUaVfWZviHP1T29uKFX//RZTweJrU7iaNCYOv4Fbfr+3rrK6rruARIivRJKiucSirEGoditNj2NykOjttKQ6tB+JwtaSQJt6Ds7etuhb9CaJGS7FAzlitgDQ5KrAFhMypXGlSsTF6gkVlV37U6WP96sF6Oq9rJAUq5g5OJWETmJYW+oDovboDXqkyOrWXBXkucRiqXV6vkgrhTSflsh85+x4LR7r6dAHtofaCzgWzMSvQpDuSL2wMjIFSTRVSpZZ4KiJOHh+N0pkROvFsgbl2t+84AYMb3vJy6OM49QzDX2UmPfsyd9D64X0RU4iWcnRG1NhycIDW8ltk/z6nLFUK6IPTBycmWBlRmgKD1ZEgtH9bf2WyBX15SqdhIbrqunr+t6Jr/15RGLEWCHXA5rZnCz8qit2bf6LJArhnJF7IGRkWt6RTcJ0Eg9W2R8is6VbD7bCkPJ0aiyrif9zVlwyGy5wiq3vUwRG06M3SoU8085wH11BKfrro1Pqe6EBfB+Im8atbWrlDxBzJIrhnJF7IGRkSusS8tb+p896acRUxuKw9zDMm52QZ6EbO3nPaTZco0oatPT4Kukte4HFVBdEruVLxaTRU5BvkaCtLGI7Jyjitp62EmfchYz5QpgKFdk1LFGrtbGdw2L0KVHUY/KEZps1d8q6Uo7em9lRkJr8SnpaeCc6fZViN0qWwwH6ram7I2Q/jxDLOLvOuaDoVyRUcdiudpBfNe8un7wh2zLGpkl+/o3LCRUl988IPkey5ZgKFdkdLFYrvZASE51Sx8sg3t7+vQtN4slf9uQd6sb7M2F7NegYQBDuSKjiCbliiBjE5QrgmgGlCuCaAaUK4JoBpQrgmgGlCuCaAaUK4JoBpQrgmgGmVwRBLFn0LsiiGZAuSKIZkC5IohmQLkiiGZAuSKIZkC5IohmQLkiiGZAuSKIZkC5IohmQLkiiGawWK7WxXcNiSUxqSAjjTdlnwhdNR8rqtgcexxYa186iRjDYrketCq+a3EbeS04ZGRvFbU9YRG6vNzj2bLXo8qBaS2TFkypnEuFOeLLTYWumo8VVWzOIAMbmpx2/FLufvPFM/hIurrrMoouZfDBGfz3F5eW5G2VFpC+OFogpbq9v6uCRCeyAIysy7BGrmv414JD3sz4riMh18Aj1zv0PR0PWtq6esibvg3cXQqJvvHkqURa+2HqcHHxnvQ/KKbvTxxGuYrvOrc9JgY2rKC5p1/Phdjr/ilD+tZlVQYdSY4jt/r4N6qT174blFSVa2B2YeUlFm/BHDCyrpShytXM+K7qcg3UsaiqO/lHOKwq4XFOQ7BeSiPGsHgoUHREfK+3QRUJB/ILuGDNUZW9KnOXrM3SKzqk0kqOT2EzLKy041l7JXkrP+sq64MYe1YBF4SW6xitks1FjuUaJITE5pHospn0feVwdpBNX3N+iuHKmYsum5aSHs+/xNwggK0kDi0LfhuYnlvCNy4OLBQTVwrcCjkQWmAWENhA3XHuqAB3rkxdSORe6k4HG0kOXq6BpFku6qcUJlc6VmJcX3GFjJF1LWZIcjU/vquKXMPJ85g88tt69eDZ6JqKFOvu6oTnOnF3bXVtenB9JNZrWzkJRaVWRRX/si5jk0zoiQw64RoLuAL6PuZbevWqvoUGEGEFejqqYPkndptUYaF3fMn7ymkAW+g/OSPn20n/60qki5EDJIBQW23pzQc9JARuc3GKagBbEhyExaElA9Lc3NLXS70f6IQ8ZfiBJZH1Gou4Z1BhM/cM4oitau/vVSxEabggPvRuPxe4RDxqfCR5uZJQvXCB/JvZJdCWe0g/uZsYxRmZyxWvCCPrmsmQ5CoAFovl6h7mzz1QdeDx2CwhUuFeyf9NXT/oh353Qm4wDQCrVkWN5IoObhIbYiDXvApQ3dOB9jLOn0gL7LxJfIt8IhINKGanpNuSjgWGcWVorFoS1bLkwRPDxfB+cYWc2wDdIEqDBg0C2MLk5uPQQoP9bfl0fUHGs7mYy9CBjSjr0jdfIqcOvNTYzz07yMOiQ6/v19cVKHwgSE4MvQvnlY+qqZFkcm1v08P+4klHNZGiAnLj+ECh0BkWi0gmV4ysaxmjKFffkP3lt0AqfXo9H41KIhX25GbVxUhTyiowC4mzfdDSUB5PC4Cc4BT6jio6e0CK9GhbNYlwR5GcggFLsvSskuZOfuslK8AJTNqOtGMc0iqQ5+WanlvZ3N6t7+kDD0klpyrXlOrOfhYkOkyoqxbAlvgi7rzUyLUDbdLxFAc2ElwoadC/qA3GgXNZZAmanVv5oIffovNAs2LoXbHzBOlIqkKkTvf8JKOialGZACujkKvKLVaAkXWljJpcyXqvr60wlfhPYZYIxVTlqlKFbkfJ7o4PjRVV8kAP+0NuRhIpkqOSnxMkp5ARUcmt+mQFQAzdtfGydqBjyu2ftArXMRIwfqC9km5uBXWpytU9LLdB/6Svlwi7+RKnDZUAtmbLlZN9GGgeUmoRAQcuv3wiVyH0Lt95gnwkVSFDwdwjXb035/Ky5zCQK9luWChXjKwrZdTkChlu3gRm84HMpfNeRa6qVaSQGab0Hkokp/CNKqou3c9+qwTNDLBVJSnQkEfFn1za8YzzchJABtw2zD0MBAwWaZuQpx0jMmBeK6oI5j1VF10V83IVAtKWt/TR8LYS1ALYSib3IHKlfrW5uRGGgrQfllFWnc99AQaLW7gisnOOyinPJ6Fu4bEC+1tyFXQDz42qOSMpv0dkrNgg+O8vLs6hDRJldpXSRTvdIdNFhIVyxci6UkZIrmQTCHeLzqQnJCYy3LbazqfPYFnb0912q01vjlxVq4iQaMvPnjwdgHUy2U2JU4EjjUWUZWX6u6AP/pm14MRonNhnT8Bv09lJ+gBujRr1HbUqP3vAtO7mYtXqO6phpaomV11x28CTfuiqvr22rZOpK7C4kQa57azNkwSkPQKC4Vrrh8LEwaoEsB1MruLAkhPBRvHZE7aDJc8LLqqt5IpI8HiuYuZPMG7kaF9XJ5PrYCPJI71Hvq6pcHe6KmLJCMAigizgoZ/9ve195NRiQF0L5QpgZF0Ba+RqVXzXsAgd9wMD+7WGGGEpa2l8VyuqDAb0R+XvgUKTTf+RkDl/ySReqQj79YLkuYC0OT91tlVFwenIYvubOroJVA1gK29HBbXTCZDgurIOh+qE33uEXvHPmqERqIuQdUO89daBkXUFLJarHcR3dSjIF7n81zn+KVUt/b3XU00EsB1ejMtV+LJNgPzgZFBsuMDIugyL5YrYmMD0wgbyXXdnN1nks7208QC2w0tW5YOKHKXRTsDIugDKFUE0A8oVQTQDyhVBNAPKFUE0A8oVQTQDyhVBNAPKFUE0A8oVQTSDTK4Igtgz6F0RRDOgXBFEM6BcEUQzoFwRRDOgXBFEM6BcEUQzoFwRRDOgXBFEM6BcEUQzoFwRRDNYLFeM72qIFVVsjv0PrDHsYfS0gsVyPYjxXQ2woorNGWRgrYrvWliUZ1JIg8V3FZGVVKA6evGVXfqOaj6Qilm4zwlY9vamLTv2boxOXBy0RnHUMbBGrmv414JDHuO7AuZWkb7O29aYGFiL47u6Z1d0DNDweSzCHX2dtwqS14KrxncVkb9AXI7q6Pnvv1RRxAUZG5Tpnq9v2JKUtPej6LhdYRFbw6N2bEt4P3HXR2+vjnawt5wOVa4Y3xU+0iqOFN/VNyolnTVLAkZ2VKuGchRFSMNtGlc1MLhcFQMurO3ZaojrIRchRUlkTPKmLYnCa/4Zq9ZFxyWmvbF8vaKwphmSXDG+q6PHd6VB6GjoIDWYCI3HdxUZRK6GAy7ME9KB7t5O3tXXqb0GecuOPWs3KkM2A5u3paxYvVlRWNMMSa4CYLFYrhjflWGv8V2zKh90wnOho4rG3VKFiNBUfFeRQeRqOOBSuT7p5ho3CJ/HIciVxYDVJe9rarmPciXYTq4Y35Vir/FdYWMSn0NWBHBp0meTBCJ1U/FdRcyVqzBEMrnyEwby8nvHAYvhtRtJUMmKqhrg1wddWTlfwMfouJ1BIRsUhTXNqMkV47vy2Gl8Vw7jcc3pPTIZ31XEErmSAbdMrmHhsZExSaDP8CjdXxr+xrQK7Eja5xuwUlFY04yaXMWhx/iu9hbfNfZSRRn9Nguagj5zOxFDpPdIjO+qhopc+Q6oD7h0ngwq14WL396emMYkKrByXfR7O/crSmqdEZIrxncl6tJKfFfykxh3djEuqwpyEfLxXSUFBMiDgN4UCum82AHVAbdIrsCmmKTwSLIeFojevsvBVsKANXLF+K6AOX+LI16pCPs9huTtPb4r6ZK4ibA9sg4MPuCmmTrLb1tC2vrNCUyrsfGpa9bHKco4ABbLFeO72hZHi++aUy0vJv0WcHgBxW6MTtwa/37S3o+XrXCon1sFLJYrYmMwvqtNWeAXMmPOYoXRYUC5IohmQLkiiGZAuSKIZkC5IohmQLkiiGZAuSKIZkC5IohmQLkiiGaQyRVBEHsGvSuCaAaUK4JoBpQrgmgGlCuCaAaUK4JoBpQrgmgGlCuCaAaUK4JoBpQrgmgGlCuCaAaL5YrxXQ2xoorNsf+BHRQHuIThxmK5HsT4rgZYUcXmDDKwox/flUS4Ey0kopwy4qvqJWCgVynWyHUN/1pwyGN8V8DcKtLXedsaEwNrH/Fdn7FoGsxCXxMveaU4RfUSMNCrlKHKFeO7wkdaBeO7GlM1AAV6O7t7r6eyjweuk0hf6nKlnSnPpZGQAHGFzN4Iz408FxNEgcMHeh2SXDG+K8Z3NTu+a++t2i4xrF5Hc52qXEmYEhg00auLEwZq9RPNsyHVN8CYyKoDDh/odUhyFQCLxXLF+K6MsRPf9XgVu5asBn1jEZO6rBi5hI4qNmgk5A8tLJPr014uBo8sOJiIwwd6HUW5YnxXyhiK76or7dDX5eTV9UH3VIZROjeoOLmwgxK5ctNANgISHD7Q66jJFeO78oyh+K7wBCGRRMhywBy5kiWVRXJ1+ECvoyZXyHDzBuO7jp34riSS5UAdCcMj1hUCvZJL6Kii5yVLJBbwUrwuM+Tq8IFeR0iuGN+VzK0xHd+VKxkSy1Y6gkXsCShT382NvHA7LJIr4NiBXq2RK8Z3Bcz5SybxSkXY7zEkP9bjuwooAr0O7RY7dqBXi+WK8V1tC8Z3tTkOHOjVYrkiNgbjuw4PDhnoFeWKIJoB5YogmgHliiCaAeWKIJoB5YogmgHliiCaAeWKIJoB5YogmkEmVwRB7Bn0rgiiGVCuCKIZUK4IohlQrgiiGVCuCKIZUK4IohlQrgiiGVCuCKIZUK4IohlQrgiiGSyWK8Z3NcSKKjZHy8FRI6VhuBATWCzXgxjf1QArqticQQZ2lOO7ApFpRbV1bW23ygyjB0peoSxSXNenr6NBxsxnru+ytRvituzYHRIW6T7b0d6rBlgj1zX8a8Ehj/FdAXOrGH+Z9dAxMbB2EN+VvCRd391cUVZ1vbnXIBqVqlyT88uqskwGZJAy02vJlu174pP3bdiSEBYeszkuJWlPRkRUvGOEdRUYqlwxvit8pFUwvqsxVQMgSDFwln+g4vKZXNmlCfdavBDJrJANr5RtCanvbFKGioyO27lqbbSipKYZklwxvivGdzUvvmteXf+zzlpjL0yGSxvoIbEnqf8H30uKiS5XdXgVbEtIW7UuWiHX9Zvj10QoA8RomiHJVQAsFssV47syxkR8V9jZVrX0kehE7Q3ldN0kRVQmC7FFA+fJ5KoyvHIEubLIrt9eqdQl70O52lCuGN+VMibiu3LA7avrGCBVZB5SKldhMGVyVRleObG6vaHhMeFRul8fdFVU1dy83UC867uwj41VlNQ0oyZXjO/KMxbiu0ph2pZalHKlIS0tk+uG6MTwKBJ4Livni780/A10C/nouF1vhIQrSmqaUZMrZLh7gPFdHT2+K2yPWxr4jWtocWM/DfEamJ1fkkeHkVxaXQ7tIeyun/ZeT+WM5ss1aPl6ECdIVEr8znTPBUGKkppmhOSK8V2JusZsfNfQ7FKyxaVtwvb1Jl1dQ8/hikiHyaW1k6215KiFcgV0yR9Kv20CZxvx7uDrc21hjVwxvisAtQb9SybxSkVgXc2tzMdcfFeycxF/ZCJdipSNsMn+D85s7+CEnftXUsVGvBsfl/C+g/3oClgsV4zvalswvqsNmem1NHH3R5vjdr67NcXxtApYLFfExmB8V5syZeaihUtXKYwOA8oVQTQDyhVBNAPKFUE0A8oVQTQDyhVBNAPKFUE0A8oVQTQDyhVBNINMrgiC2DPoXRFEM6BcEUQzoFwRRDOgXBFEM6BcEUQzoFwRRDOgXBFEM6BcEUQzoFwRRDOgXBFEM1gsV4zvaogVVWyO3Q0see+hTV+kiFgh14MY39UAK6rYnEEG1uL4rozIwQIlswE/oFIm86cew9f/p1S393cpYmoNisNHbTUfa+S6hn8tOOQxvitgbhXp67xtjYmBtTy+K0dUZZfh29Ul0HBEJMoeDdmqaFZVroHZhZWXjMfIUjJGoraaz1DlivFd4SOt4lDxXQmx8CwDhRuXa0pt51MuGBeNwSV/ZDO50oEqPn6A8+riCllyOSXfGHsz6xiJ2mo+Q5Irxnd13PiuLOjmN7KwGgpKHjzpqOLeVy7NM2izPaST3B0krz4XXa54OcTnC5Hy5IyRqK3mMyS5CoDFYrlifFeGXcZ3hWUwfUTKo+DISRNvJb1Hiksjd40PRUli9tB2ZHLlL8d4kLsxErXVfEZRrhjflWKH8V3JMrgtnxhNyTWePsu4j2I/eURlAuShQFqWyVXl/ioYI1FbzWfU5IrxXXnsLr4rF6qPi0BHouM1FgnlJcCpO6rZ40Bl92EgV7LXsFCuYyRqq/mMmlwhw80bjO9qb/FdWcsE6V0wANw4FyESBgqW8WT34b+/uDiHtkaU2VVKV+x0e0xXEBbKdYxEbTWfEZIrxncl6tJKfFfWMsGkXLnnI1ntwziw/sDlwwqCrIqhk/297XBIGiTWQrkCYyFqq/lYI1eM7wqY85dM4pWKsB8wSN7e47uaiWKgAnURsj6I9906xkLUVvOxWK4Y39W2OFp812HA4aO2mo/FckVsDMZ3NQPHjtpqPihXBNEMKFcE0QwoVwTRDChXBNEMKFcE0QwoVwTRDChXBNEMKFcE0QwyuSIIYs+gd0UQzYByRRDNgHJFEM2AckUQzYByRRDNgHJFEM2AckUQzYByRRDNgHJFEM2AckUQzWCxXDG+qyFWVLE5GN91LGCxXA9ifFcDrKhicwYZ2OGK7wqIr2KVIb5SWALGdx0a1sh1Df9acMhjfFfA3CrS13nbGhMDO2zxXX2zKmGonz1Rfam3qlwxvuvQGKpcMb4rfKRVxl58V3q/4uGiTMgV47valCHJFeO7jun4rgxVLwrQZjG+q20ZklwFwGKxXDG+K0Oz8V05TMkV47vamFGUK8Z3pWg2viuHKbkKdvJQIC3L5KpyfxVgfFcFoyZXjO/Ko9n4rgyz5YrxXYfOqMkVMty8wfiuGo3vypDLFeO7DisjJFeM70rU5WjxXWFvTNqE0SbVmy+BES4fVhAY33WYsEauGN8VMOcvmcQrFWE/YJC8g8R3VRCI8V2HEYvlivFdbQvGdx0UjO8qYLFcERuD8V3NAOO7MlCuCKIZUK4IohlQrgiiGVCuCKIZUK4IohlQrgiiGVCuCKIZUK4IohlkckUQxJ5B74ogmgHliiCaAeWKIJoB5YogmgHliiCaAeWKIJoB5YogmgHliiCaAeWKIJoB5YogmsFiuWJ8V0OsqGJz7H9gB8UBLmG4sViuBzG+qwFWVLE5gwzssMR31WUUXcrgIwD47y8uLcnbKisgQOLQFWZK3g9MYt6JdRmqlxBf2aXvqOYjqpiF+5yAZW9v2rJj78boxMVBaxRHNY01cl3DvxYc8hjfFTC3ivR13rbGxMAOW3xXyUvDSVQ+tTHnIPEBaPgSzkJfE69sWfUS/Pdfqijioo0NynTP1zdsSUra+1F03K6wiK3hUTu2JbyfuOujt1dHO8YbT4cqV4zvCh9plTEY35WXK4nWMdBSQt/Trw4U6O3s7r2eyj4euE4ifanLlXamPJdGQgLEFTJ7Izw38nQEDIiMSd60JVF45T9j1brouMS0N5avVxTWIkOSK8Z3HdvxXZlck4vbSJgskw6QyPVWbZcYVq+juU5VriRMCQwauy6if3HC0CgeoHk2pPoGGBNZdWDLjj1rNyrDNwObt6WsWL1ZUViLDEmuAmCxWK4Y35Wh4fiu5Gh7m55EJOqo5mLnqUPbOV7FriWrQd9YpNIyuYSOKjZoJOQPLSyT69PeCrZMkAUHExHkyuLB6pL3NbXcR7naRK4Y35Wi4fiucPQZ3fOTjLHnI4W1oyvt0Nfl5NX1QfdUWpbODSpOctPlcuUjX0lHQAIshtduJAEmK6pqgF8fdGXlfAEfo+N2BoVsUBTWIqMmV4zvyqPd+K5kKJjDp6v35lxe9gZwdxOeICSSCFkOmCNXsqSySK5h4bGRMUmgz/Ao3V8a/sa0CuxI2ucbsFJRWIuMmlwhw80bjO+q1fiu0qNkrIRBMIAvSSJZDtSRMDxiXb4n9BI6quiDlSyRWMBL8brMkOvCxW9vT0xjEhVYuS76vZ37FSU1ygjJFeO7krnlaPFd5UdT4e50cXtLJWLJkFi20hEsYk9AmfpubuSF22GRXIFNMUnhkWQ9LBC9fZdjrIQBa+SK8V0Bc/6SSbxSEfZ7DMk7ZnxXK5D1ZKi3eOosv20Jaes3JzCtxsanrlkfpyijXSyWK8Z3tS2OFt81p1peTPot4AgBit0Ynbg1/v2kvR8vW+EIP7cKWCxXxMZgfNfhYYFfyIw5ixVGrYNyRRDNgHJFEM2AckUQzYByRRDNgHJFEM2AckUQzYByRRDNgHJFEM0gkyuCIPYMelcE0QwoVwTRDChXBNEMKFcE0QwoVwTRDChXBNEMKFcE0QwoVwTRDChXBNEMKFcE0QwWyxXjuxpiRRWbo+XgqJEmw1IiIhbL9SDGdzXAiio2Z5CBHeX4rkBkWlFtXVvbrTLD6IGSVyiLFNf16etokDHzmeu7bO2GuC07doeERbrPdrT3qgHWyHUN/1pwyGN8V8DcKsZfZj10TAysHcR3JS9J13c3V5RVXW/uNYhGpSrX5PyyqiyTARmkzPRasmX7nvjkfRu2JISFx2yOS0nakxERFe8YYV0FhipXjO8KH2kVjO9KI3eoA4IUA2f5Byoun8mVXZpwr8ULkcwK2fBK2ZaQ+s4mZajI6Lidq9ZGK0pqmiHJFeO7YnxX8+K75tX1P+usNfbCZLi0gR4Se5L6f/C9pJjoclWHV8G2hLRV66IVcl2/OX5NhDJAjKYZklwFwGKxXDG+K2NMxHeFnW1VSx+JTtTeUE7XTVJEZbIQWzRwnkyuKsMrR5Ari+z67ZVKXfI+lKsN5YrxXSljIr4rB9y+uo4BUkXmIaVyFQZTJleV4ZUTq9sbGh4THqX79UFXRVXNzdsNxLu+C/vYWEVJTTNqcsX4rjxjIb6rFKZtqUUpVxrS0jK5bohODI8igeeycr74S8PfQLeQj47b9UZIuKKkphk1uUKGuwcY39XR47vC9rilgd+4hhY39tMQr4HZ+SV5dBjJpdXl0B7C7vpp7/VUzmi+XIOWrwdxgkSlxO9M91wQpCipaUZIrhjflahrzMZ3Dc0uJVtc2gfYvt6kO3/oOVwR6TC5tHaytZYctVCugC75Q+m3TeBsI94dfH2uLayRK8Z3Bcz5SybxSkVgXc2tzMdcfFeyc5H+0UVYRKRshE32f3Bmewcn7Ny/kio24t34uIT3HexHV8BiuWJ8V9uC8V1tyEyvpYm7P9oct/PdrSmOp1XAYrkiNgbju9qUKTMXLVy6SmF0GFCuCKIZUK4IohlQrgiiGVCuCKIZUK4IohlQrgiiGVCuCKIZUK4IohlkckUQxJ5B74ogmgHliiCaAeWKIJoB5YogmgHliiCaAeWKIJoB5YogmgHliiCaAeWKIJoB5YogmsFiuWJ8V0OsqGJz7G5gyXsPxbexIzbBYrkexPiuBlhRxeYMMrAWxncl4ucCF6i91VWEDfgByetIeTJ/6uFDpYikVLf3dyliag2Kw0dtNR9r5LqGfy045DG+K2BuFenrvG2NiYG1Ir4raY1/L7m++ZLiKA8NR0Si7NGQrYpmVeUamF1YeYnGLjCLMRK11XyGKleM7wofaRWHiu8KrcEVScuokFLb+ZQLxkVjcMkf2UyudKCKjx/gvLq4QpZcTsk3xt7MOkaitprPkOSK8V0dNb4rXFFLiUz5KpQ8eNJRxb2vXJpn0GZ7SCe5O0hefS66XPFyiM8XIuXJGSNRW81nSHIVAIvFcsX4rgy7jO8KGfKYIE8fGElFFY408VbSe6S4NHLX+FCUJGYPDXgjkyt/OXCNhstmyhiJ2mo+oyhXjO9KscP4rqQ896zxh0cD96xUEk+fZdxHsZ88ojIB8lAgLcvkqnJ/FYyRqK3mM2pyxfiuPHYX31VyFCAFSsMVRgqcuqOaPQ5Udh8GciV7DQvlOkaitprPqMkVMty8wfiu9hbfNbK4gt8a0CFVdo8D3DgXIRIGCpbxZPfhv7+4OIe2RpTZVUpX7HR7TFcQFsp1jERtNZ8RkivGdyXq0kp819C86x3c2Z/AQBl+2cZDxUxW+zAOrD9w+bCCIKti6GR/bzscIiHY9Y1FgoYtkCswFqK2mo81csX4roA5f8kkXqkI+wGD5O09vivpkriJMIpioAJ1EbI+iPfdOsZC1FbzsViuGN/VtjhafNdhwOGjtpqPxXJFbAzGdzUDx47aaj4oVwTRDChXBNEMKFcE0QwoVwTRDChXBNEMKFcE0QwoVwTRDChXBNEMMrkiCGLPoHdFEM2AckUQzYByRRDNgHJFEM0wanL9Hx4LEK2juKfIcDO8clXcXWTsoJgJiE2wvVwVtw1BAMUkQazDlnJV3CEEkaKYLYgV2EyuinuDIIYo5gxiKaMjV0VdRKMobuugKKojlmKxXI3Fd1XcGIZQS3hvIMZ3HSbsbWAVM4GhKINYisVyPWgkvqvpGyO8iVf69uphAOO7Ku0cwxXfFRBfxSqDvlJYOSswvuvQsEaua/jXgkNeiO+qvDHyWiMhV4zvamRghy2+q29WJQz1syeqL/VWlSvGdx0aQ5WrEN9VeWPktdTlivFdObQZ35Xer3i4KAvkivFdh8SQ5CqN76q8MfJaKnLF+K4cmo3vyqCyNCLXAeWsEAuLl4PxXc1nSHIVAIvyxsgLCCKReFeM70rRbHxXDlNy7VLOCplcMb6rxYyiXDG+K0Wz8V05TMnVYDEsk6vK/VWA8V0VjJpcMb4rj2bjuzKGU64Y31XBqMkVMty8wfiuGo3vypDLVRHfVTkrLJQrxndVMEJyxfiuRF2OFt8V9sakTRhtUpj+3gOXDysIIb6rclZYKFcA47tKsUauqvFdlTdGWRHjuzLYDxgk7yDxXRUEyuK7DjYrBgfju0qxWK7G4rsqbgxDURcxxFHjuypmAkNRxkwwvquAxXI1huLGmI+inTGHNuO7Km6imSgaMR+M78oYfbkiYwfFnEEsxWZyBRT3BkGkKGYLYgW2lCtDcZMQRDFDEKuxvVylKG4bMnZQzATEJgyvXM1BcZsR+0dxB5ERY/TliiCImaBcEUQzoFwRRDPI5IogiD2D3hVBNAPKFUE0A8oVQTQDyhVBNAPKFUE0A8oVQTQDyhVBNAPKFUE0A8oVQTQDyhVBNIPFcjUW39U0wnsDMb7rMGH/AzsoDnAJw43Fcj1oJL6raYQ38UrfXj0MYHxXpZ1jWOK76jKKLmXwEQD89xeXluRtlRUQIAH7CjMl7wcmMe/EugzVS4iv7NJ3VPMRVczCfU7Asrc3bdmxd2N04uKgNYqjmsYaua7hXwsOeSG+q1BAlZGQK8Z3NTKwwxbfVfLqdhKVT23MOUh8ABq+hLPQ18QLrwXnUL0E//2XKoq4kAKDMt3z9Q1bkpL2fhQdtyssYmt41I5tCe8n7vro7dXRjvHG06HKVYjvCroVyhiiLleM78qh0fiuvFxpuE2TgeqgQG9nd+/1VPbxwHUS6UtdrrQz5bk0EhIgrpDZG+G5kacjYEBkTPKmLYnCK/8Zq9ZFxyWmvbF8vaKwFhmSXKXxXSEVyhiiIleM78qh3fiuTK7JxW0kTJZJB0jkequ2Swyr19FcpypXEqYEBo1dF9G/OGFoFA/QPBtSfQOMiaw6sGXHnrUbleGbgc3bUlas3qworEWGJFcBsFgsV4zvytBwfFci1/Y2WDY/e9JRzcXOU4cK+3gVu5asBn1jEZO6rBi5hI4qNmgk5A8tLJPr094KtkyQBQcTEeTK4sHqkvc1tdx3HLm6+/7/6syvpc1kNl0AAAAASUVORK5CYII=)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The prefix 'q5_1' signifies the quantization method we used. I won't delve into too many details, but to determine the best method in each case, I follow the rule that 'q8' yields superior responses at the cost of higher memory usage [slow]. On the other hand, 'q2' may generate subpar responses but requires less RAM [fast].\n",
        "\n",
        "There are other quantization methods available, and you can read about them in the [model card](https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML)"
      ],
      "metadata": {
        "id": "4L7AZFe_7Px0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "oI-kXwg5bHF-"
      },
      "outputs": [],
      "source": [
        "model_name_or_path = \"TheBloke/Llama-2-13B-chat-GGML\"\n",
        "model_basename = \"llama-2-13b-chat.ggmlv3.q5_1.bin\" # the model is in bin format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Td05XSuiWdI"
      },
      "source": [
        "First, we download the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "cBEJr-G-2ht4"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_path"
      ],
      "metadata": {
        "id": "sx4VKmyF59m9",
        "outputId": "ed8f7521-f702-48d5-c4d6-f82cac56c715",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/root/.cache/huggingface/hub/models--TheBloke--Llama-2-13B-chat-GGML/snapshots/3140827b4dfcb6b562cd87ee3d7f07109b014dd0/llama-2-13b-chat.ggmlv3.q5_1.bin'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "max1jwxvCSbm"
      },
      "source": [
        "# Inference with llama-cpp-python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TOfnZpj394g"
      },
      "source": [
        "Loading the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oY1bItJu4Zfv",
        "outputId": "00b8ad07-aa1d-4732-995a-d2432e7e86a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n"
          ]
        }
      ],
      "source": [
        "# GPU\n",
        "from llama_cpp import Llama\n",
        "lcpp_llm = None\n",
        "lcpp_llm = Llama(\n",
        "    model_path=model_path,\n",
        "    n_threads=2, # CPU cores\n",
        "    n_batch=512, # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
        "    n_gpu_layers=43, # Change this value based on your model and your GPU VRAM pool.\n",
        "    n_ctx=4096, # Context window\n",
        "    logits_all=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "For run in CPU\n",
        "```\n",
        "# CPU\n",
        "from llama_cpp import Llama\n",
        "\n",
        "lcpp_llm = Llama(\n",
        "    model_path=model_path,\n",
        "    n_threads=2, # CPU cores\n",
        "    )\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "UdZnPtB8-Bhx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BeH6eWiKuaxW",
        "outputId": "9f70a0c2-57b5-4faa-e2b2-e92df49a0343"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "43"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# See the number of layers in GPU\n",
        "lcpp_llm.params.n_gpu_layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLEEOufGVlID"
      },
      "source": [
        "We will use this prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "-NzVIlMCVoVD"
      },
      "outputs": [],
      "source": [
        "prompt = \"Write a linear regression in python\"\n",
        "prompt_template=f'''SYSTEM: You are a helpful, respectful and honest assistant. Always answer as helpfully.\n",
        "\n",
        "USER: {prompt}\n",
        "\n",
        "ASSISTANT:\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qaFQjPhcFgfN"
      },
      "source": [
        "Generating response\n",
        "\n",
        "If you only use CPU, the response can take a long time. You can reduce the max_tokens to get a faster response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R76uxL293jTc",
        "outputId": "19efe3bc-028b-4453-c273-7ac2576f0ba3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SYSTEM: You are a helpful, respectful and honest assistant. Always answer as helpfully.\n",
            "\n",
            "USER: Write a linear regression in python\n",
            "\n",
            "ASSISTANT:\n",
            "\n",
            "To write a linear regression in Python, you can use scikit-learn library. Here's an example of how to do it:\n",
            "```\n",
            "from sklearn.linear_model import LinearRegression\n",
            "import pandas as pd\n",
            "\n",
            "# Load your dataset\n",
            "df = pd.read_csv('your_data.csv')\n",
            "\n",
            "# Create a linear regression object and fit the data\n",
            "reg = LinearRegression()\n",
            "reg.fit(df[['x1', 'x2']], df['y'])\n",
            "\n",
            "# Print the coefficients\n",
            "print(reg.coef_)\n",
            "```\n",
            "This will print the coefficients of the linear regression model, which you can use to make predictions on new data.\n",
            "\n",
            "You can also use the `predict()` method to make predictions on new data:\n",
            "```\n",
            "# Make a prediction on some new data\n",
            "new_data = pd.DataFrame({'x1': [2, 3], 'x2': [4, 5]})\n",
            "prediction = reg.predict(new_data)\n",
            "print(prediction)\n",
            "```\n",
            "This will print the predicted values for the new data.\n",
            "\n",
            "I hope this helps! Let me know if you have any questions or need further\n"
          ]
        }
      ],
      "source": [
        "response = lcpp_llm(\n",
        "    prompt=prompt_template,\n",
        "    max_tokens=256,\n",
        "    temperature=0.5,\n",
        "    top_p=0.95,\n",
        "    repeat_penalty=1.2,\n",
        "    top_k=50,\n",
        "    logprobs=10,\n",
        "    stop = ['USER:'], # Dynamic stopping when such token is detected.\n",
        "    echo=True # return the prompt\n",
        ")\n",
        "\n",
        "print(response[\"choices\"][0][\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"[S1] Hi my name is Tom. [S2] Hi my name is Jerry. [S1] Where are you from? \"\n",
        "prompt_template=f'''SYSTEM: The following is a conversation between 2 speakers denoted by [S1] and [S2]. Complete the following conversation in the same format.\n",
        "\n",
        "USER: {prompt}\n",
        "\n",
        "ASSISTANT:\n",
        "'''\n",
        "\n",
        "response = lcpp_llm(\n",
        "    prompt=prompt_template,\n",
        "    max_tokens=5,\n",
        "    temperature=0.0,\n",
        "    top_p=0.95,\n",
        "    repeat_penalty=1.2,\n",
        "    top_k=10,\n",
        "    logprobs=2,  # Requesting the top 5 log probabilities\n",
        "    # logits_all=True,\n",
        "    stop = ['USER:'], # Dynamic stopping when such token is detected.\n",
        "    echo=True # return the prompt\n",
        ")\n",
        "\n",
        "print(response[\"choices\"][0][\"text\"])"
      ],
      "metadata": {
        "id": "c6DkhADY7VcQ",
        "outputId": "4dd511c4-5a56-409d-d1c4-fe0abcf87b64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SYSTEM: The following is a conversation between 2 speakers denoted by [S1] and [S2]. Complete the following conversation in the same format.\n",
            "\n",
            "USER: [S1] Hi my name is Tom. [S2] Hi my name is Jerry. [S1] Where are you from? \n",
            "\n",
            "ASSISTANT:\n",
            "\n",
            "[S2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response.keys()"
      ],
      "metadata": {
        "id": "-GBBQ85f8VoD",
        "outputId": "13d512f8-b6d1-42d0-9f13-91d12d0975d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['id', 'object', 'created', 'model', 'choices', 'usage'])"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response[\"usage\"]"
      ],
      "metadata": {
        "id": "_Fq4--8TCRVE",
        "outputId": "44c52d0b-10e9-4832-fdf5-b5d269d32f19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'prompt_tokens': 76, 'completion_tokens': 5, 'total_tokens': 81}"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response[\"choices\"][0].keys()"
      ],
      "metadata": {
        "id": "4-Pawo6i_KmO",
        "outputId": "2b07d9a4-e2e2-4d23-cd1d-fbcaa2b208d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['text', 'index', 'logprobs', 'finish_reason'])"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response[\"choices\"][0][\"logprobs\"]#.keys()"
      ],
      "metadata": {
        "id": "909z_AwU_YkP",
        "outputId": "f8fe0e1b-227b-447e-a320-78b6749881a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'tokens': [' SY',\n",
              "  'ST',\n",
              "  'EM',\n",
              "  ':',\n",
              "  ' The',\n",
              "  ' following',\n",
              "  ' is',\n",
              "  ' a',\n",
              "  ' conversation',\n",
              "  ' between',\n",
              "  ' ',\n",
              "  '2',\n",
              "  ' speak',\n",
              "  'ers',\n",
              "  ' denoted',\n",
              "  ' by',\n",
              "  ' [',\n",
              "  'S',\n",
              "  '1',\n",
              "  ']',\n",
              "  ' and',\n",
              "  ' [',\n",
              "  'S',\n",
              "  '2',\n",
              "  '].',\n",
              "  ' Complete',\n",
              "  ' the',\n",
              "  ' following',\n",
              "  ' conversation',\n",
              "  ' in',\n",
              "  ' the',\n",
              "  ' same',\n",
              "  ' format',\n",
              "  '.',\n",
              "  '\\n',\n",
              "  '\\n',\n",
              "  'USER',\n",
              "  ':',\n",
              "  ' [',\n",
              "  'S',\n",
              "  '1',\n",
              "  ']',\n",
              "  ' Hi',\n",
              "  ' my',\n",
              "  ' name',\n",
              "  ' is',\n",
              "  ' Tom',\n",
              "  '.',\n",
              "  ' [',\n",
              "  'S',\n",
              "  '2',\n",
              "  ']',\n",
              "  ' Hi',\n",
              "  ' my',\n",
              "  ' name',\n",
              "  ' is',\n",
              "  ' Jerry',\n",
              "  '.',\n",
              "  ' [',\n",
              "  'S',\n",
              "  '1',\n",
              "  ']',\n",
              "  ' Where',\n",
              "  ' are',\n",
              "  ' you',\n",
              "  ' from',\n",
              "  '?',\n",
              "  ' ',\n",
              "  '\\n',\n",
              "  '\\n',\n",
              "  'ASS',\n",
              "  'IST',\n",
              "  'ANT',\n",
              "  ':',\n",
              "  '\\n',\n",
              "  '\\n',\n",
              "  '[',\n",
              "  'S',\n",
              "  '2',\n",
              "  ']'],\n",
              " 'text_offset': [0,\n",
              "  3,\n",
              "  5,\n",
              "  7,\n",
              "  8,\n",
              "  12,\n",
              "  22,\n",
              "  25,\n",
              "  27,\n",
              "  40,\n",
              "  48,\n",
              "  49,\n",
              "  50,\n",
              "  56,\n",
              "  59,\n",
              "  67,\n",
              "  70,\n",
              "  72,\n",
              "  73,\n",
              "  74,\n",
              "  75,\n",
              "  79,\n",
              "  81,\n",
              "  82,\n",
              "  83,\n",
              "  85,\n",
              "  94,\n",
              "  98,\n",
              "  108,\n",
              "  121,\n",
              "  124,\n",
              "  128,\n",
              "  133,\n",
              "  140,\n",
              "  141,\n",
              "  142,\n",
              "  143,\n",
              "  147,\n",
              "  148,\n",
              "  150,\n",
              "  151,\n",
              "  152,\n",
              "  153,\n",
              "  156,\n",
              "  159,\n",
              "  164,\n",
              "  167,\n",
              "  171,\n",
              "  172,\n",
              "  174,\n",
              "  175,\n",
              "  176,\n",
              "  177,\n",
              "  180,\n",
              "  183,\n",
              "  188,\n",
              "  191,\n",
              "  197,\n",
              "  198,\n",
              "  200,\n",
              "  201,\n",
              "  202,\n",
              "  203,\n",
              "  209,\n",
              "  213,\n",
              "  217,\n",
              "  222,\n",
              "  223,\n",
              "  224,\n",
              "  225,\n",
              "  226,\n",
              "  229,\n",
              "  232,\n",
              "  235,\n",
              "  236,\n",
              "  237,\n",
              "  238,\n",
              "  239,\n",
              "  240,\n",
              "  241],\n",
              " 'token_logprobs': [None,\n",
              "  -2.327445777033891,\n",
              "  -0.39795338598781227,\n",
              "  -4.204136717877657,\n",
              "  -2.989035109667723,\n",
              "  -5.6917857353421555,\n",
              "  -1.2322710768071183,\n",
              "  -0.23003753348039035,\n",
              "  -4.850004529468673,\n",
              "  -0.21626420886459044,\n",
              "  -6.304879237936868,\n",
              "  -0.6304540198139182,\n",
              "  -6.823429744180358,\n",
              "  -0.00038801064013188304,\n",
              "  -12.36871885329407,\n",
              "  -0.3662999293305259,\n",
              "  -5.59676960408101,\n",
              "  -1.9660233812581462,\n",
              "  -0.04194958941018943,\n",
              "  -0.03950021710532585,\n",
              "  -0.008849445206871615,\n",
              "  -0.0009225567932708566,\n",
              "  -0.00010509809204025641,\n",
              "  -4.609595062116904e-05,\n",
              "  -0.23927236636818716,\n",
              "  -13.421902011377504,\n",
              "  -0.029970727877673676,\n",
              "  -2.630089346304658,\n",
              "  -0.8099617418312095,\n",
              "  -3.930089416304154,\n",
              "  -2.7734125463134207,\n",
              "  -6.656799728856001,\n",
              "  -3.914077935532759,\n",
              "  -0.5395306284971195,\n",
              "  -0.08208120285120318,\n",
              "  -0.009573684482977969,\n",
              "  -9.18499304672942,\n",
              "  -0.03972789545257173,\n",
              "  -4.939330158747618,\n",
              "  -0.007736085662123747,\n",
              "  -0.025836076515474832,\n",
              "  -0.009893634379083147,\n",
              "  -0.9079010876438046,\n",
              "  -11.56406564911283,\n",
              "  -0.8264971352392166,\n",
              "  -0.009258694361375355,\n",
              "  -4.857439926765747,\n",
              "  -1.9156707269902102,\n",
              "  -6.135128523924952,\n",
              "  -0.38613855297632665,\n",
              "  -0.0024680140909901374,\n",
              "  -0.0014605348286769388,\n",
              "  -0.1653362208061032,\n",
              "  -6.32294643155708,\n",
              "  -0.0015170292141569767,\n",
              "  -0.0006232462751890337,\n",
              "  -3.5025449563366506,\n",
              "  -0.10146106226184849,\n",
              "  -4.797656696879561,\n",
              "  -0.13408368792531553,\n",
              "  -0.0005196477453436202,\n",
              "  -0.001277447058927382,\n",
              "  -5.160222005279786,\n",
              "  -0.03833785526430129,\n",
              "  -0.0010768084068781538,\n",
              "  -0.00987485404399747,\n",
              "  -0.07837825757135733,\n",
              "  -4.741292049277823,\n",
              "  -0.5856172428805937,\n",
              "  -0.03405142342432563,\n",
              "  -5.147361618164885,\n",
              "  -0.565905504111811,\n",
              "  -0.0318312725558759,\n",
              "  -0.0025860342844396616,\n",
              "  -6.4520091434672,\n",
              "  -0.16961814681353182,\n",
              "  -1.4003441797454472,\n",
              "  -0.005088484010389667,\n",
              "  -0.03739705913400957,\n",
              "  -0.0004007965015478506],\n",
              " 'top_logprobs': [None,\n",
              "  {'N': -1.058104308222856, 'ST': -2.327445777033891},\n",
              "  {'EM': -0.39795338598781227, 'E': -1.3564170548110543},\n",
              "  {' OF': -2.905795920453341,\n",
              "   'AT': -3.35920511539963,\n",
              "   ':': -4.204136717877657},\n",
              "  {' The': -2.989035109667723, ' A': -2.990740279345457},\n",
              "  {' system': -2.904881400320183,\n",
              "   ' System': -3.7181386177274094,\n",
              "   ' following': -5.6917857353421555},\n",
              "  {' is': -1.2322710768071183, ' are': -2.40822971022997},\n",
              "  {' a': -0.23003753348039035, ' an': -2.4129179350917185},\n",
              "  {' list': -0.6471646780892785,\n",
              "   ' summary': -3.3073523993417195,\n",
              "   ' conversation': -4.850004529468673},\n",
              "  {' between': -0.21626420886459044, ' with': -2.3034929838890044},\n",
              "  {' two': -1.1248102682103052,\n",
              "   ' a': -1.2987528341282741,\n",
              "   ' ': -6.304879237936868},\n",
              "  {'2': -0.6304540198139182, '3': -1.601276354103469},\n",
              "  {' friends': -1.3767587351166375,\n",
              "   ' people': -1.577345054086364,\n",
              "   ' speak': -6.823429744180358},\n",
              "  {'ers': -0.00038801064013188304, 's': -9.649469240803706},\n",
              "  {',': -0.29218601256531007,\n",
              "   '.': -2.1733410266033957,\n",
              "   ' denoted': -12.36871885329407},\n",
              "  {' by': -0.3662999293305259, ' as': -1.3554356715180262},\n",
              "  {' A': -1.1614811748207563,\n",
              "   ' S': -1.663651737564897,\n",
              "   ' [': -5.59676960408101},\n",
              "  {'Spe': -0.9252662019978923,\n",
              "   'A': -1.8235797243367595,\n",
              "   'S': -1.9660233812581462},\n",
              "  {'1': -0.04194958941018943, ']': -3.514008839288119},\n",
              "  {']': -0.03950021710532585, ' and': -4.5270900532259315},\n",
              "  {' and': -0.008849445206871615, ' (': -5.12183124147152},\n",
              "  {' [': -0.0009225567932708566, ' S': -8.103354807769833},\n",
              "  {'S': -0.00010509809204025641, ' S': -10.80613823255249},\n",
              "  {'2': -4.609595062116904e-05, '1': -10.969668707950133},\n",
              "  {'].': -0.23927236636818716, ']': -1.8518116577500232},\n",
              "  {'\\n': -0.9412587389776026,\n",
              "   ' The': -1.074145625574282,\n",
              "   ' Complete': -13.421902011377504},\n",
              "  {' the': -0.029970727877673676, ' each': -4.45842798800951},\n",
              "  {' conversation': -0.3453422225863964,\n",
              "   ' dialog': -2.265621725455049,\n",
              "   ' following': -2.630089346304658},\n",
              "  {' conversation': -0.8099617418312095, ' dialog': -1.013864940073397},\n",
              "  {' by': -0.9509939573197791,\n",
              "   ' with': -1.3030428312211464,\n",
              "   ' in': -3.930089416304154},\n",
              "  {' a': -0.30033477226556915,\n",
              "   ' such': -2.6756475774413504,\n",
              "   ' the': -2.7734125463134207},\n",
              "  {' appropriate': -0.9828123402451614,\n",
              "   ' space': -2.0496954274033645,\n",
              "   ' same': -6.656799728856001},\n",
              "  {' style': -0.3945733884136186,\n",
              "   ' tone': -1.7214013913433062,\n",
              "   ' format': -3.914077935532759},\n",
              "  {'.': -0.5395306284971195, ':': -2.127269619280811},\n",
              "  {'\\n': -0.08208120285120318, ' ': -4.277498607026008},\n",
              "  {'\\n': -0.009573684482977969, 'S': -5.684451804905342},\n",
              "  {'S': -0.10030915161833734,\n",
              "   '[': -2.6315686883615013,\n",
              "   'USER': -9.18499304672942},\n",
              "  {':': -0.03972789545257173, ' IN': -3.7915093590756186},\n",
              "  {' Hello': -1.0347986798657818,\n",
              "   ' Hi': -1.0762806516065044,\n",
              "   ' [': -4.939330158747618},\n",
              "  {'S': -0.007736085662123747, 'SY': -8.055990098723647},\n",
              "  {'1': -0.025836076515474832, '2': -3.6751032731707483},\n",
              "  {']': -0.009893634379083147, ']:': -5.119793155253107},\n",
              "  {' Hi': -0.9079010876438046, ' Hello': -1.531983652951422},\n",
              "  {',': -0.7536427517739626,\n",
              "   '!': -1.2150418301431032,\n",
              "   ' my': -11.56406564911283},\n",
              "  {' name': -0.8264971352392166, ' friend': -1.611161289291951},\n",
              "  {' is': -0.009258694361375355, \"'\": -4.759945339869188},\n",
              "  {' John': -0.9640960012896729,\n",
              "   ' Alex': -3.044029167793579,\n",
              "   ' Tom': -4.857439926765747},\n",
              "  {' and': -0.39638171026657765,\n",
              "   ',': -1.8004020197148198,\n",
              "   '.': -1.9156707269902102},\n",
              "  {' I': -0.7022424002677261,\n",
              "   '\\n': -1.696159865477687,\n",
              "   ' [': -6.135128523924952},\n",
              "  {'S': -0.38613855297632665, '/': -1.1789623444802328},\n",
              "  {'2': -0.0024680140909901374, '1': -6.020057583182787},\n",
              "  {']': -0.0014605348286769388, ']:': -7.390112382668033},\n",
              "  {' Hi': -0.1653362208061032, ' Hello': -2.5084339075736812},\n",
              "  {' Tom': -0.02084720365180615,\n",
              "   ',': -5.0413531088031736,\n",
              "   ' my': -6.32294643155708},\n",
              "  {' name': -0.0015170292141569767, ' names': -8.13552543680693},\n",
              "  {' is': -0.0006232462751890337, \"'\": -8.92122413738847},\n",
              "  {' Sam': -1.8067727853161428,\n",
              "   ' Alex': -2.289260463843975,\n",
              "   ' Jerry': -3.5025449563366506},\n",
              "  {'.': -0.10146106226184849, ',': -2.7179657314513017},\n",
              "  {' It': -1.3442780195113966,\n",
              "   ' What': -1.5909210859176466,\n",
              "   ' [': -4.797656696879561},\n",
              "  {'S': -0.13408368792531553, '/': -2.1229066249370345},\n",
              "  {'1': -0.0005196477453436202, '2': -7.7617243672888},\n",
              "  {']': -0.001277447058927382, '].': -7.53351926834799},\n",
              "  {' Nice': -0.4259938711245121,\n",
              "   ' What': -2.262018155487305,\n",
              "   ' Where': -5.160222005279786},\n",
              "  {' are': -0.03833785526430129, ' do': -3.805849223062153},\n",
              "  {' you': -0.0010768084068781538, ' from': -8.00123321099477},\n",
              "  {' from': -0.00987485404399747, ' originally': -5.5519757604160676},\n",
              "  {'?': -0.07837825757135733, ' Jerry': -2.803132591067451},\n",
              "  {' [': -0.054657031882803075,\n",
              "   '\\n': -3.446455051291983,\n",
              "   ' ': -4.741292049277823},\n",
              "  {'\\n': -0.5856172428805937, '': -1.5958368168552028},\n",
              "  {'\\n': -0.03405142342432563, '[': -3.8812918714223725},\n",
              "  {'USER': -0.9469469605721111,\n",
              "   'Please': -1.5437849580086347,\n",
              "   'ASS': -5.147361618164885},\n",
              "  {'IST': -0.565905504111811, 'IGN': -1.1192521380229439},\n",
              "  {'ANT': -0.0318312725558759, ':': -4.123769291354704},\n",
              "  {':': -0.0025860342844396616, ' (': -7.060344365583267},\n",
              "  {' [': -0.545225085850013,\n",
              "   ' Here': -2.388185443516029,\n",
              "   '\\n': -6.4520091434672},\n",
              "  {'\\n': -0.16961814681353182, '[': -2.85616065779986},\n",
              "  {'USER': -0.7896130548675176, '[': -1.4003441797454472},\n",
              "  {'S': -0.005088484010389667, ' ]': -6.825182592591933},\n",
              "  {'2': -0.03739705913400957, '1': -3.3054634653840096},\n",
              "  {']': -0.0004007965015478506, ':': -9.179125085441978}]}"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "response[\"choices\"][0][\"logprobs\"]"
      ],
      "metadata": {
        "id": "xSbxwWVvCv08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(response[\"choices\"][0][\"logprobs\"][\"top_logprobs\"])\n"
      ],
      "metadata": {
        "id": "JSLnmB-5_mAN",
        "outputId": "dfddcb33-f213-4f61-c5da-d6f055d57a0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        }
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                    0\n",
              "0                                                None\n",
              "1   {'N': -1.058104308222856, 'ST': -2.32744577703...\n",
              "2   {'EM': -0.39795338598781227, 'E': -1.356417054...\n",
              "3   {' OF': -2.905795920453341, 'AT': -3.359205115...\n",
              "4   {' The': -2.989035109667723, ' A': -2.99074027...\n",
              "..                                                ...\n",
              "75  {'\n",
              "': -0.16961814681353182, '[': -2.8561606577...\n",
              "76  {'USER': -0.7896130548675176, '[': -1.40034417...\n",
              "77  {'S': -0.005088484010389667, ' ]': -6.82518259...\n",
              "78  {'2': -0.03739705913400957, '1': -3.3054634653...\n",
              "79  {']': -0.0004007965015478506, ':': -9.17912508...\n",
              "\n",
              "[80 rows x 1 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8a53c1e5-f8fc-42ce-be53-486c5116797f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>{'N': -1.058104308222856, 'ST': -2.32744577703...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>{'EM': -0.39795338598781227, 'E': -1.356417054...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>{' OF': -2.905795920453341, 'AT': -3.359205115...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>{' The': -2.989035109667723, ' A': -2.99074027...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75</th>\n",
              "      <td>{'\n",
              "': -0.16961814681353182, '[': -2.8561606577...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76</th>\n",
              "      <td>{'USER': -0.7896130548675176, '[': -1.40034417...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77</th>\n",
              "      <td>{'S': -0.005088484010389667, ' ]': -6.82518259...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78</th>\n",
              "      <td>{'2': -0.03739705913400957, '1': -3.3054634653...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>{']': -0.0004007965015478506, ':': -9.17912508...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>80 rows × 1 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8a53c1e5-f8fc-42ce-be53-486c5116797f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8a53c1e5-f8fc-42ce-be53-486c5116797f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8a53c1e5-f8fc-42ce-be53-486c5116797f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-485f6e0f-3a75-44ad-9f33-7eb5a968d6ac\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-485f6e0f-3a75-44ad-9f33-7eb5a968d6ac')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-485f6e0f-3a75-44ad-9f33-7eb5a968d6ac button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.iloc[-2]"
      ],
      "metadata": {
        "id": "IL0XGq9bC-HW",
        "outputId": "6828fbea-6577-4639-dfe6-ec24554fee0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    {'2': -0.03739705913400957, '1': -3.3054634653...\n",
              "Name: 78, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "no531n827gI9"
      },
      "source": [
        "# Inference with langchain\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zG2uFQFl7jrq",
        "outputId": "6771bfa4-abce-4234-b47a-52e808efb596"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.8/46.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip -q install langchain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Kl8UYvXHYIt"
      },
      "source": [
        "We can use the model that we loaded earlier. However, for illustrative purposes, we will load one from the 'langchain' library. Due to vRAM limitations, before running these cells, you need to delete the previous model from memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "NWaQuYH0AT1z"
      },
      "outputs": [],
      "source": [
        "lcpp_llm.reset()\n",
        "lcpp_llm.set_cache(None)\n",
        "lcpp_llm = None\n",
        "del lcpp_llm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Prompt for the model."
      ],
      "metadata": {
        "id": "scri08pVDp8U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "N5ijTbW6_dI_"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import LlamaCpp\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "from langchain.callbacks.manager import CallbackManager\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "template = \"\"\"''SYSTEM: You are a helpful, respectful and honest assistant. Always answer as helpfully.\n",
        "USER: {question}\n",
        "ASSISTANT: \"\"\"\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hx06Ea1DH8ky"
      },
      "source": [
        "Stream tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "iVIXxNoIHOe_"
      },
      "outputs": [],
      "source": [
        "# Callbacks support token-wise streaming\n",
        "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
        "# Verbose is required to pass to the callback manager"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2DHWIr-IEDE"
      },
      "source": [
        "Load the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iipsXBGhHNjf",
        "outputId": "3d4ef58b-cdc6-4483-92b2-a8cfca24c14d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n"
          ]
        }
      ],
      "source": [
        "n_gpu_layers = 40  # Change this value based on your model and your GPU VRAM pool.\n",
        "n_batch = 512  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
        "\n",
        "# Loading model,\n",
        "llm = LlamaCpp(\n",
        "    model_path=model_path,\n",
        "    max_tokens=1024,\n",
        "    n_gpu_layers=n_gpu_layers,\n",
        "    n_batch=n_batch,\n",
        "    callback_manager=callback_manager,\n",
        "    verbose=True,\n",
        "    n_ctx=4096, # Context window\n",
        "    stop = ['USER:'], # Dynamic stopping when such token is detected.\n",
        "    temperature = 0.4,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZ8kbRz2F9vi"
      },
      "source": [
        "Generating response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 609
        },
        "id": "Lw4MaS4WBlR-",
        "outputId": "8f9ad2ff-3f0c-40de-f508-dcaffe36a284"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Sure thing! Here is an example of how you can write a simple linear regression in Python using scikit-learn library:\n",
            "```\n",
            "from sklearn.linear_model import LinearRegression\n",
            "import pandas as pd\n",
            "\n",
            "# create a sample dataset\n",
            "data = pd.DataFrame({'x': [1, 2, 3, 4], 'y': [2, 4, 6, 8]})\n",
            "\n",
            "# create a linear regression object and fit the data\n",
            "reg = LinearRegression()\n",
            "reg.fit(data[['x']], data['y'])\n",
            "\n",
            "# print the coefficients\n",
            "print(reg.coef_)\n",
            "\n",
            "# print the R-squared value\n",
            "print(reg.score(data[['x']], data['y']))\n",
            "```\n",
            "This code will create a simple linear regression object and fit the data using the `fit()` method. It will then print the coefficients of the linear regression and the R-squared value, which measures the goodness of fit of the model.\n",
            "\n",
            "I hope this helps! Let me know if you have any questions or need further assistance.\n",
            "```\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" Sure thing! Here is an example of how you can write a simple linear regression in Python using scikit-learn library:\\n```\\nfrom sklearn.linear_model import LinearRegression\\nimport pandas as pd\\n\\n# create a sample dataset\\ndata = pd.DataFrame({'x': [1, 2, 3, 4], 'y': [2, 4, 6, 8]})\\n\\n# create a linear regression object and fit the data\\nreg = LinearRegression()\\nreg.fit(data[['x']], data['y'])\\n\\n# print the coefficients\\nprint(reg.coef_)\\n\\n# print the R-squared value\\nprint(reg.score(data[['x']], data['y']))\\n```\\nThis code will create a simple linear regression object and fit the data using the `fit()` method. It will then print the coefficients of the linear regression and the R-squared value, which measures the goodness of fit of the model.\\n\\nI hope this helps! Let me know if you have any questions or need further assistance.\\n```\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
        "\n",
        "question = \"Write a simple linear regression in python\"\n",
        "\n",
        "llm_chain.run(question)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference with llama.cpp"
      ],
      "metadata": {
        "id": "IdTSFrMqF-yu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To use llama.cpp directly, we must clone the repository. In this example, we will use only the CPU."
      ],
      "metadata": {
        "id": "AjwqmnwYZlyR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ggerganov/llama.cpp\n",
        "%cd llama.cpp\n",
        "!git checkout dadbed99e65252d79f81101a392d0d6497b86caa # For compatibility with GGML"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xwt3kdw4Rnmt",
        "outputId": "2645f8ee-a494-448e-a26c-376d7598a6ba"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'llama.cpp'...\n",
            "remote: Enumerating objects: 12284, done.\u001b[K\n",
            "remote: Counting objects: 100% (4414/4414), done.\u001b[K\n",
            "remote: Compressing objects: 100% (338/338), done.\u001b[K\n",
            "remote: Total 12284 (delta 4246), reused 4106 (delta 4076), pack-reused 7870\u001b[K\n",
            "Receiving objects: 100% (12284/12284), 14.53 MiB | 8.43 MiB/s, done.\n",
            "Resolving deltas: 100% (8558/8558), done.\n",
            "/content/llama.cpp\n",
            "Note: switching to 'dadbed99e65252d79f81101a392d0d6497b86caa'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by switching back to a branch.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -c with the switch command. Example:\n",
            "\n",
            "  git switch -c <new-branch-name>\n",
            "\n",
            "Or undo this operation with:\n",
            "\n",
            "  git switch -\n",
            "\n",
            "Turn off this advice by setting config variable advice.detachedHead to false\n",
            "\n",
            "HEAD is now at dadbed9 metal : fix synchronization in new matrix multiplication kernel (#2686)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build llama.cpp\n",
        "\n",
        "We will use the model only with the CPU."
      ],
      "metadata": {
        "id": "yjhMVxTVZ9o0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CPU\n",
        "!make\n",
        "\n",
        "# GPU\n",
        "#!make LLAMA_CUBLAS=1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8E38icaHSLB_",
        "outputId": "d075b84f-9d31-469e-e3ad-4d025da875ac"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I llama.cpp build info: \n",
            "I UNAME_S:  Linux\n",
            "I UNAME_P:  x86_64\n",
            "I UNAME_M:  x86_64\n",
            "I CFLAGS:   -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS\n",
            "I CXXFLAGS: -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS\n",
            "I LDFLAGS:  \n",
            "I CC:       cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "I CXX:      g++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "\n",
            "cc  -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS   -c ggml.c -o ggml.o\n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -c llama.cpp -o llama.o\n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -c examples/common.cpp -o common.o\n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -c examples/console.cpp -o console.o\n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -c examples/grammar-parser.cpp -o grammar-parser.o\n",
            "cc -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS   -c -o k_quants.o k_quants.c\n",
            "cc  -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS   -c ggml-alloc.c -o ggml-alloc.o\n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/main/main.cpp ggml.o llama.o common.o console.o grammar-parser.o k_quants.o ggml-alloc.o -o main \n",
            "\n",
            "====  Run ./main -h for help.  ====\n",
            "\n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/quantize/quantize.cpp ggml.o llama.o k_quants.o ggml-alloc.o -o quantize \n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/quantize-stats/quantize-stats.cpp ggml.o llama.o k_quants.o ggml-alloc.o -o quantize-stats \n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/perplexity/perplexity.cpp ggml.o llama.o common.o k_quants.o ggml-alloc.o -o perplexity \n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/embedding/embedding.cpp ggml.o llama.o common.o k_quants.o ggml-alloc.o -o embedding \n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS pocs/vdot/vdot.cpp ggml.o k_quants.o ggml-alloc.o -o vdot \n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/train-text-from-scratch/train-text-from-scratch.cpp ggml.o llama.o k_quants.o ggml-alloc.o -o train-text-from-scratch \n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.cpp ggml.o llama.o k_quants.o ggml-alloc.o -o convert-llama2c-to-ggml \n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/simple/simple.cpp ggml.o llama.o common.o k_quants.o ggml-alloc.o -o simple \n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -Iexamples/server examples/server/server.cpp ggml.o llama.o common.o grammar-parser.o k_quants.o ggml-alloc.o -o server  \n",
            "g++ --shared -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/embd-input/embd-input-lib.cpp ggml.o llama.o common.o k_quants.o ggml-alloc.o -o libembdinput.so \n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/embd-input/embd-input-test.cpp ggml.o llama.o common.o k_quants.o ggml-alloc.o -o embd-input-test  -L. -lembdinput\n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/llama-bench/llama-bench.cpp ggml.o llama.o common.o k_quants.o ggml-alloc.o -o llama-bench \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "xm4ofY8F3gWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./main -t 2 -m {model_path} --color -c 128 --temp 0.7 -n 56 -p \"USER: Write a linear regression in python\\nASSISTANT:\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMNdFcnte8-K",
        "outputId": "59e712b6-495f-466f-ed75-5298d98812c7"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "main: build = 1011 (dadbed9)\n",
            "main: seed  = 1700687284\n",
            "llama.cpp: loading model from /root/.cache/huggingface/hub/models--TheBloke--Llama-2-13B-chat-GGML/snapshots/3140827b4dfcb6b562cd87ee3d7f07109b014dd0/llama-2-13b-chat.ggmlv3.q5_1.bin\n",
            "llama_model_load_internal: format     = ggjt v3 (latest)\n",
            "llama_model_load_internal: n_vocab    = 32000\n",
            "llama_model_load_internal: n_ctx      = 128\n",
            "llama_model_load_internal: n_embd     = 5120\n",
            "llama_model_load_internal: n_mult     = 256\n",
            "llama_model_load_internal: n_head     = 40\n",
            "llama_model_load_internal: n_head_kv  = 40\n",
            "llama_model_load_internal: n_layer    = 40\n",
            "llama_model_load_internal: n_rot      = 128\n",
            "llama_model_load_internal: n_gqa      = 1\n",
            "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
            "llama_model_load_internal: n_ff       = 13824\n",
            "llama_model_load_internal: freq_base  = 10000.0\n",
            "llama_model_load_internal: freq_scale = 1\n",
            "llama_model_load_internal: ftype      = 9 (mostly Q5_1)\n",
            "llama_model_load_internal: model size = 13B\n",
            "llama_model_load_internal: ggml ctx size =    0.11 MB\n",
            "llama_model_load_internal: mem required  = 9311.07 MB (+  100.00 MB per state)\n",
            "llama_new_context_with_model: kv self size  =  100.00 MB\n",
            "llama_new_context_with_model: compute buffer total size =   22.34 MB\n",
            "\n",
            "system_info: n_threads = 2 / 2 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n",
            "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
            "generate: n_ctx = 128, n_batch = 512, n_predict = 56, n_keep = 0\n",
            "\n",
            "\n",
            "\u001b[33m USER: Write a linear regression in python\\nASSISTANT:\u001b[0m Sure! Here is an example of^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Details about the different parameters."
      ],
      "metadata": {
        "id": "wwxfqy_daf5F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<tbody><tr><td class=\"l\"></td><td class=\"l\"> </td><td class=\"l\"> <b>param value</b> </td><td class=\"l\"></td></tr>\n",
        "  <tr><td class=\"l\">  <code>-h</code> </td><td class=\"l\"> <code>--help</code> </td><td class=\"l\"> </td><td class=\"l\"> Show this help message and exit</td></tr>\n",
        "  <tr><td class=\"l\">  <code>-i</code> </td><td class=\"l\"> <code>--interactive</code> </td><td class=\"l\"> </td><td class=\"l\"> Run in interactive mode</td></tr>\n",
        "  <tr><td class=\"l\">  </td><td class=\"l\"> <code>--interactive-first</code> </td><td class=\"l\"> </td><td class=\"l\"> Run in interactive mode and wait for input right away</td></tr>\n",
        "  <tr><td class=\"l\">  </td><td class=\"l\"> <code>-ins</code>, <code>--instruct</code> </td><td class=\"l\">  </td><td class=\"l\"> Run in instruction mode (use with Alpaca models)</td></tr>\n",
        "  <tr><td class=\"l\">  <code>-r</code> </td><td class=\"l\"> <code>--reverse-prompt</code> </td><td class=\"l\"> <code>PROMPT</code> </td><td class=\"l\"> Run in interactive mode and poll user input upon seeing <code>PROMPT</code> (can be specified more than once for multiple prompts).</td></tr>\n",
        "  <tr><td class=\"l\">  </td><td class=\"l\"> <code>--color</code> </td><td class=\"l\">  </td><td class=\"l\"> Colorise output to distinguish prompt and user input from generations</td></tr>\n",
        "  <tr><td class=\"l\">  <code>-s</code> </td><td class=\"l\"> <code>--seed</code> </td><td class=\"l\"> <code>SEED</code> </td><td class=\"l\"> Seed for random number generator (default: <code>-1</code>, use random seed for &lt;= 0)</td></tr>\n",
        "  <tr><td class=\"l\">  <code>-t</code> </td><td class=\"l\"> <code>--threads</code> </td><td class=\"l\"> <code>N</code> </td><td class=\"l\"> Number of threads to use during computation (default: 12)</td></tr>\n",
        "  <tr><td class=\"l\">  <code>-p</code> </td><td class=\"l\"> <code>--prompt</code> </td><td class=\"l\"> <code>PROMPT</code> </td><td class=\"l\"> Prompt to start generation with (default: empty)</td></tr>\n",
        "  <tr><td class=\"l\">  </td><td class=\"l\"> <code>--random-prompt</code> </td><td class=\"l\"> </td><td class=\"l\"> Start with a randomized prompt.</td></tr>\n",
        "  <tr><td class=\"l\">  </td><td class=\"l\"> <code>--in-prefix</code> </td><td class=\"l\"> <code>STRING</code> </td><td class=\"l\"> String to prefix user inputs with (default: empty)</td></tr>\n",
        "  <tr><td class=\"l\">  <code>-f</code> </td><td class=\"l\"> <code>--file</code> </td><td class=\"l\"> <code>FNAME</code> </td><td class=\"l\"> Prompt file to start generation.</td></tr>\n",
        "  <tr><td class=\"l\">  <code>-n</code> </td><td class=\"l\"> <code>--n_predict</code> </td><td class=\"l\"> <code>N</code> </td><td class=\"l\"> Number of tokens to predict (default: 128, -1 = infinity)</td></tr>\n",
        "  <tr><td class=\"l\">   </td><td class=\"l\"> <code>--top_k</code> </td><td class=\"l\"> <code>N</code> </td><td class=\"l\"> Top-k sampling (default: 40)</td></tr>\n",
        "  <tr><td class=\"l\"> </td><td class=\"l\"> <code>--top_p</code> </td><td class=\"l\"> <code>N</code> </td><td class=\"l\"> Top-p sampling (default: 0.9)</td></tr>\n",
        "  <tr><td class=\"l\"> </td><td class=\"l\"> <code>--repeat_last_n</code> </td><td class=\"l\"> <code>N</code> </td><td class=\"l\"> Last n tokens to consider for penalize (default: 64)</td></tr>\n",
        "  <tr><td class=\"l\"> </td><td class=\"l\"> <code>--repeat_penalty</code> </td><td class=\"l\"> <code>N</code> </td><td class=\"l\"> Penalize repeat sequence of tokens (default: 1.1)</td></tr>\n",
        "  <tr><td class=\"l\"> <code>-c</code> </td><td class=\"l\"> <code>--ctx_size</code> </td><td class=\"l\"> <code>N</code> </td><td class=\"l\"> Size of the prompt context (default: <code>512</code>)</td></tr>\n",
        "  <tr><td class=\"l\"> </td><td class=\"l\"> <code>--ignore-eos</code> </td><td class=\"l\"> </td><td class=\"l\"> Ignore end of stream token and continue generating</td></tr>\n",
        "  <tr><td class=\"l\"> </td><td class=\"l\"> <code>--memory_f32</code> </td><td class=\"l\"> </td><td class=\"l\"> Use <code>f32</code> instead of <code>f16</code> for memory key+value</td></tr>\n",
        "  <tr><td class=\"l\"> </td><td class=\"l\"> <code>--temp</code> </td><td class=\"l\"> <code>N</code> </td><td class=\"l\"> Temperature (default: <code>0.8</code>)</td></tr>\n",
        "  <tr><td class=\"l\">  </td><td class=\"l\"> <code>--n_parts</code> </td><td class=\"l\"> <code>N</code> </td><td class=\"l\"> Number of model parts (default: -1 = determine from dimensions)</td></tr>\n",
        "  <tr><td class=\"l\">  <code>-b</code> </td><td class=\"l\"> <code>--batch_size</code> </td><td class=\"l\"> <code>N</code> </td><td class=\"l\"> Batch size for prompt processing (default: 8)</td></tr>\n",
        "  <tr><td class=\"l\">  </td><td class=\"l\"> <code>--perplexity</code> </td><td class=\"l\"> </td><td class=\"l\"> Compute perplexity over the prompt</td></tr>\n",
        "  <tr><td class=\"l\">  </td><td class=\"l\"> <code>--keep</code> </td><td class=\"l\">   </td><td class=\"l\"> Number of tokens to keep from the initial prompt (default: 0, -1 = all)</td></tr>\n",
        "  <tr><td class=\"l\">  </td><td class=\"l\"> <code>--mlock</code> </td><td class=\"l\">  </td><td class=\"l\"> Force system to keep model in RAM rather than swapping or compressing</td></tr>\n",
        "  <tr><td class=\"l\">  </td><td class=\"l\"> <code>--mtest</code> </td><td class=\"l\">  </td><td class=\"l\"> Determine the maximum memory usage needed to do inference for the given <code>n_batch</code> and <code>n_predict</code> parameters (uncomment the <code>\"used_mem\"</code> line in <code>llama.cpp</code> to see the results)</td></tr>\n",
        "  <tr><td class=\"l\">  </td><td class=\"l\"> <code>--verbose-prompt</code> </td><td class=\"l\">   </td><td class=\"l\"> Print prompt before generation</td></tr>\n",
        "  <tr><td class=\"l\">  <code>-m</code> </td><td class=\"l\"> <code>--model</code> </td><td class=\"l\"> <code>FNAME</code> </td><td class=\"l\"> Model path (default: <code>models/llama-7B/ggml-model.bin</code>)</td></tr>\n",
        "\n",
        "</tbody>"
      ],
      "metadata": {
        "id": "ZdNp0ixPevHC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQcO_xyCJyGZ"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "# What is better: GPTQ or GGML?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MU6o0MlQJ2K3"
      },
      "source": [
        "- GPTQ is a specific format for GPU only.\n",
        "\n",
        "- GGML is designed for CPU and Apple M series but can also offload some layers on the GPU\n",
        "\n",
        "- GGMLs like q4_2, q4_3, q5_0, q5_1 and q8_0 have superior inference quality and outperform GPTQ on benchmarks"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "\n",
        "- [renenyffenegger.ch - LLaMA C++ Library](https://renenyffenegger.ch/notes/development/Artificial-intelligence/language-model/LLM/LLaMA/libs/llama_cpp/)\n",
        "- [LLaMA C++ Library Documentation](https://llama-cpp-python.readthedocs.io/en/latest/)\n",
        "- [MacOS Install with Metal GPU - LLaMA C++ Library Documentation](https://llama-cpp-python.readthedocs.io/en/latest/install/macos/)\n"
      ],
      "metadata": {
        "id": "YX8Fx_n8fWwe"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}