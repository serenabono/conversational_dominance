{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U \"transformers==4.40.0\" --upgrade\n",
    "!pip install accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /u/sebono/miniconda3/envs/.py311env/lib/python3.11/site-packages (4.44.0)\n",
      "Requirement already satisfied: filelock in /u/sebono/miniconda3/envs/.py311env/lib/python3.11/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /u/sebono/miniconda3/envs/.py311env/lib/python3.11/site-packages (from transformers) (0.24.5)\n",
      "Requirement already satisfied: numpy>=1.17 in /u/sebono/miniconda3/envs/.py311env/lib/python3.11/site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /u/sebono/miniconda3/envs/.py311env/lib/python3.11/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /u/sebono/miniconda3/envs/.py311env/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /u/sebono/miniconda3/envs/.py311env/lib/python3.11/site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in /u/sebono/miniconda3/envs/.py311env/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /u/sebono/miniconda3/envs/.py311env/lib/python3.11/site-packages (from transformers) (0.4.2)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers)\n",
      "  Obtaining dependency information for tokenizers<0.20,>=0.19 from https://files.pythonhosted.org/packages/a7/03/fb50fc03f86016b227a967c8d474f90230c885c0d18f78acdfda7a96ce56/tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /u/sebono/miniconda3/envs/.py311env/lib/python3.11/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /u/sebono/miniconda3/envs/.py311env/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /u/sebono/miniconda3/envs/.py311env/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /u/sebono/miniconda3/envs/.py311env/lib/python3.11/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /u/sebono/miniconda3/envs/.py311env/lib/python3.11/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /u/sebono/miniconda3/envs/.py311env/lib/python3.11/site-packages (from requests->transformers) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /u/sebono/miniconda3/envs/.py311env/lib/python3.11/site-packages (from requests->transformers) (2024.7.4)\n",
      "Using cached tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[33mDEPRECATION: pytorch-lightning 1.6.0 has a non-standard dependency specifier torch>=1.8.*. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: tokenizers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.13.3\n",
      "    Uninstalling tokenizers-0.13.3:\n",
      "      Successfully uninstalled tokenizers-0.13.3\n",
      "Successfully installed tokenizers-0.19.1\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tokenizers==0.13.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5757788c6fd45b496dcd0cb7c1d1e44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.41k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "/u/sebono/miniconda3/envs/.py311env/lib/python3.11/site-packages/transformers/quantizers/auto.py:174: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45e32bd6baf44a348ceef2cae8a3b8e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/5.70G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "model_id = \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\n",
    "        \"torch_dtype\": torch.float16,\n",
    "        \"quantization_config\": {\"load_in_4bit\": True},\n",
    "        \"low_cpu_mem_usage\": True,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'PreTrainedTokenizerFast'. \n",
      "The class this function is called from is 'LlamaTokenizerFast'.\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n"
     ]
    }
   ],
   "source": [
    "from transformers import LlamaForCausalLM, LlamaTokenizerFast\n",
    "\n",
    "# Load the model and tokenizer with device map\n",
    "model_id = \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\"\n",
    "model = LlamaForCausalLM.from_pretrained(model_id, device_map=\"auto\")\n",
    "tokenizer = LlamaTokenizerFast.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.nn.functional import pad\n",
    "\n",
    "def p1(dialog):\n",
    "    \n",
    "    max_length = model.config.max_position_embeddings\n",
    "    stride = 1\n",
    "    \n",
    "    pad_token_id = 0\n",
    "    encodings = tokenizer(\" \".join(dialog), return_tensors=\"pt\")\n",
    "    seq_len = encodings.input_ids.size(1)\n",
    "    padding_len = max_length -1 \n",
    "    padded_input_ids = pad(torch.tensor([], dtype=torch.long), (0, padding_len), value=pad_token_id).unsqueeze(dim=0)\n",
    "    encodings.input_ids = torch.cat([padded_input_ids, encodings.input_ids], dim=1)\n",
    "    seq_len = encodings.input_ids.size(1)\n",
    "    \n",
    "    nlls = []\n",
    "    prev_end_loc = padding_len\n",
    "    for begin_loc in tqdm(range(0, seq_len, stride)):\n",
    "        end_loc = min(begin_loc + max_length, seq_len)\n",
    "        trg_len = end_loc - prev_end_loc  # may be different from max_length on the last loop \n",
    "        begin_loc = max(padding_len, begin_loc)\n",
    "        input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device)\n",
    "        target_ids = input_ids.clone()\n",
    "        target_ids[:, :-trg_len] = -100\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, labels=target_ids)\n",
    "            neg_log_likelihood = outputs.loss\n",
    "\n",
    "        nlls.append(neg_log_likelihood.item())\n",
    "\n",
    "        prev_end_loc = end_loc\n",
    "        if end_loc == seq_len:\n",
    "            break\n",
    "    return nlls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/sebono/miniconda3/envs/.py311env/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model: unsloth/Meta-Llama-3.1-8B-bnb-4bit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                        | 0/141021 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'device' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m dialog \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m<\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mn<\u001b[39m\u001b[38;5;124m'\u001b[39m, el)\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m     25\u001b[0m dialog_no_s \u001b[38;5;241m=\u001b[39m [re\u001b[38;5;241m.\u001b[39msub(pattern,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, d) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m dialog]\n\u001b[0;32m---> 26\u001b[0m perpl \u001b[38;5;241m=\u001b[39m p1(dialog_no_s)\n\u001b[1;32m     27\u001b[0m ppl[path] \u001b[38;5;241m=\u001b[39m perpl\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/dominance_p1_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n",
      "Cell \u001b[0;32mIn[3], line 24\u001b[0m, in \u001b[0;36mp1\u001b[0;34m(dialog)\u001b[0m\n\u001b[1;32m     22\u001b[0m trg_len \u001b[38;5;241m=\u001b[39m end_loc \u001b[38;5;241m-\u001b[39m prev_end_loc  \u001b[38;5;66;03m# may be different from max_length on the last loop \u001b[39;00m\n\u001b[1;32m     23\u001b[0m begin_loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(padding_len, begin_loc)\n\u001b[0;32m---> 24\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m encodings\u001b[38;5;241m.\u001b[39minput_ids[:, begin_loc:end_loc]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     25\u001b[0m target_ids \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mclone()\n\u001b[1;32m     26\u001b[0m target_ids[:, :\u001b[38;5;241m-\u001b[39mtrg_len] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'device' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "data_path = \"/u/sebono/conversational_dominance/data/processed/CANDOR/conversations.csv\"\n",
    "index_path = \"/u/sebono/conversational_dominance/data/processed/CANDOR/group_1.csv\"\n",
    "output_path =\"/u/sebono/conversational_dominance/notebooks/dominance_scores_topical.pkl\"\n",
    "\n",
    "print(f\"Loaded model: {model_id}\")\n",
    "# Load dataset from the specified data_path\n",
    "dataset = pd.read_csv(data_path)\n",
    "indices = pd.read_csv(index_path)[\"conversation_id\"]\n",
    "\n",
    "filtered_d = dataset[dataset[\"file_name\"].isin(indices)]\n",
    "\n",
    "pattern='<(SPK[1-9]|MOD)>'\n",
    "tot_n = len(filtered_d['file_content'])\n",
    "for idx, (el, path) in enumerate(zip(filtered_d[\"file_content\"], filtered_d[\"file_name\"])):\n",
    "    ppl = {}\n",
    "    if os.path.exists(f\"{output_path}/dominance_scores_{path}.pkl\"):\n",
    "        print(f\"skipping {path} ...\")\n",
    "        continue\n",
    "\n",
    "    dialog = re.sub(r'\\<', r'\\n<', el).split(\"\\n\")[1:]\n",
    "    dialog_no_s = [re.sub(pattern,\"\", d) for d in dialog]\n",
    "    perpl = p1(dialog_no_s)\n",
    "    ppl[path] = perpl\n",
    "    with open(f\"{output_path}/dominance_p1_{path}.pkl\", 'wb') as file:\n",
    "        pickle.dump(ppl, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.nn.functional import pad\n",
    "import numpy as np\n",
    "\n",
    "def p2(dialog):\n",
    "    \n",
    "    max_length = model.config.n_positions\n",
    "    stride = 1\n",
    "    #encodings = tokenizer(\" \".join(dialog), return_tensors=\"pt\")\n",
    "    tokens = [tokenizer(token, return_tensors=\"pt\", return_offsets_mapping=True).input_ids[0] for token in dialog]\n",
    "    encodings = tokenizer(\" \".join(dialog), return_tensors=\"pt\")\n",
    "    tokens_ids_per_sentence = np.cumsum([t.size(0) for t in tokens])\n",
    "    \n",
    "    assert tokens_ids_per_sentence[-1] == encodings.input_ids.size(1)\n",
    "    \n",
    "    pad_token_id = 0    \n",
    "    padding_len = max_length -1 \n",
    "    \n",
    "    tokens_ids_per_sentence+=padding_len\n",
    "    padded_input_ids = pad(torch.tensor([], dtype=torch.long), (0, padding_len), value=pad_token_id).unsqueeze(dim=0)\n",
    "    encodings.input_ids = torch.cat([padded_input_ids, encodings.input_ids], dim=1)\n",
    "    seq_len = encodings.input_ids.size(1)\n",
    "    #seq_len_tter = encodings.input_ids.size(1)\n",
    "    seq_len_tter = tokens_ids_per_sentence \n",
    "    nlls = []\n",
    "    \n",
    "    begin_loc_win=padding_len\n",
    "    i=0\n",
    "    for begin_loc in tqdm(range(0, seq_len, stride)):\n",
    "        end_loc = min(begin_loc + max_length, seq_len)\n",
    "        begin_loc = max(padding_len, begin_loc)\n",
    "        input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device)\n",
    "        target_ids = input_ids.clone()\n",
    "        target_ids[:, :-begin_loc_win] = -100\n",
    "        #print([tokenizer.decode(token, skip_special_tokens=True) for token in encodings.input_ids[:, begin_loc_win:end_loc]])\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, labels=target_ids)\n",
    "            neg_log_likelihood = outputs.loss\n",
    "\n",
    "        nlls.append(neg_log_likelihood.item())\n",
    "        if seq_len_tter[i] == end_loc:\n",
    "            begin_loc_win = seq_len_tter[i]\n",
    "            i+=1\n",
    "        prev_end_loc = end_loc\n",
    "        if end_loc == seq_len:\n",
    "            break\n",
    "    \n",
    "    return nlls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "data_path = \"/u/sebono/conversational_dominance/data/processed/multisimo/conversations.csv\"\n",
    "index_path = \"/u/sebono/conversational_dominance/data/processed/multisimo/group_2.csv\"\n",
    "output_path =\"/u/sebono/conversational_dominance/notebooks/information_exchange_labelling/dataset_perplexity_results/multisimo_p2/\"\n",
    "\n",
    "print(f\"Loaded model: {model_id}\")\n",
    "# Load dataset from the specified data_path\n",
    "dataset = pd.read_csv(data_path)\n",
    "indices = pd.read_csv(index_path)[\"conversation_id\"]\n",
    "\n",
    "filtered_d = dataset[dataset[\"file_name\"].isin(indices)]\n",
    "\n",
    "pattern='<(SPK[1-9]|MOD)>'\n",
    "tot_n = len(filtered_d['file_content'])\n",
    "for idx, (el, path) in enumerate(zip(filtered_d[\"file_content\"], filtered_d[\"file_name\"])):\n",
    "    ppl = {}\n",
    "    dialog = re.sub(r'\\<', r'\\n<', el).split(\"\\n\")[1:]\n",
    "    dialog_no_s = [re.sub(pattern, \">\", d) for d in dialog]\n",
    "    perpl = p2(dialog_no_s)\n",
    "    ppl[path] = perpl\n",
    "    with open(f\"{output_path}/dominance_scores_{path}.pkl\", 'wb') as file:\n",
    "        pickle.dump(ppl, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.nn.functional import pad\n",
    "\n",
    "def p3(dialog):\n",
    "    \n",
    "    max_length = model.config.n_positions\n",
    "    stride = 1\n",
    "    \n",
    "    pad_token_id = 0\n",
    "    encodings = tokenizer(\" \".join(dialog), return_tensors=\"pt\")\n",
    "    encoding_nxt_line = tokenizer(\"\\n\", return_tensors=\"pt\").input_ids.to(device)\n",
    "    seq_len = encodings.input_ids.size(1)\n",
    "    padding_len = max_length -1 \n",
    "    padded_input_ids = pad(torch.tensor([], dtype=torch.long), (0, padding_len), value=pad_token_id).unsqueeze(dim=0)\n",
    "    encodings.input_ids = torch.cat([padded_input_ids, encodings.input_ids], dim=1)\n",
    "    seq_len = encodings.input_ids.size(1)\n",
    "    \n",
    "    nlls = []\n",
    "    prev_end_loc = padding_len\n",
    "    for begin_loc in tqdm(range(0, seq_len, stride)):\n",
    "        end_loc = min(begin_loc + max_length, seq_len)\n",
    "        trg_len = end_loc - prev_end_loc # may be different from max_length on the last loop \n",
    "        begin_loc = max(padding_len, begin_loc)\n",
    "        input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device)\n",
    "        input_ids_xt = torch.cat([input_ids, encoding_nxt_line], dim=1)\n",
    "        target_ids = input_ids_xt.clone()\n",
    "        target_ids[:, :-trg_len] = -100\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids_xt, labels=target_ids)\n",
    "            neg_log_likelihood = outputs.loss\n",
    "\n",
    "        nlls.append(neg_log_likelihood.item())\n",
    "\n",
    "        prev_end_loc = end_loc\n",
    "        if end_loc == seq_len:\n",
    "            break\n",
    "    return nlls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "data_path = \"/u/sebono/conversational_dominance/data/processed/topical/conversations.csv\"\n",
    "index_path = \"/u/sebono/conversational_dominance/data/processed/topical/group_1.csv\"\n",
    "output_path =\"/u/sebono/conversational_dominance/notebooks/information_exchange_labelling/dataset_perplexity_results/topical_per_utterance/\"\n",
    "\n",
    "print(f\"Loaded model: {model_id}\")\n",
    "# Load dataset from the specified data_path\n",
    "dataset = pd.read_csv(data_path)\n",
    "indices = pd.read_csv(index_path)[\"conversation_id\"]\n",
    "\n",
    "filtered_d = dataset[dataset[\"file_name\"].isin(indices)]\n",
    "\n",
    "pattern='<(SPK[1-9]|MOD)>'\n",
    "tot_n = len(filtered_d['file_content'])\n",
    "for idx, (el, path) in enumerate(zip(filtered_d[\"file_content\"], filtered_d[\"file_name\"])):\n",
    "    ppl = {}\n",
    "    if os.path.exists(f\"{output_path}/dominance_scores_{path}.pkl\"):\n",
    "        print(f\"skipping {path} ...\")\n",
    "        continue\n",
    "    dialog = re.sub(r'\\<', r'\\n<', el).split(\"\\n\")[1:]\n",
    "    dialog_no_s = [re.sub(pattern, \"\", d) for d in dialog]\n",
    "    perpl = p3(dialog_no_s)\n",
    "    ppl[path] = perpl\n",
    "    with open(f\"{output_path}/dominance_scores_{path}.pkl\", 'wb') as file:\n",
    "        pickle.dump(ppl, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "data_path = \"/u/sebono/conversational_dominance/data/processed/topical/conversations.csv\"\n",
    "index_path = \"/u/sebono/conversational_dominance/data/processed/topical/group_1.csv\"\n",
    "output_path =\"/u/sebono/conversational_dominance/notebooks/dominance_scores_topical.pkl\"\n",
    "\n",
    "print(f\"Loaded model: {model_id}\")\n",
    "# Load dataset from the specified data_path\n",
    "dataset = pd.read_csv(data_path)\n",
    "indices = pd.read_csv(index_path)[\"conversation_id\"]\n",
    "\n",
    "filtered_d = dataset[dataset[\"file_name\"].isin(indices)]\n",
    "\n",
    "\n",
    "tot_n = len(filtered_d['file_content'])\n",
    "for idx, (el, path) in enumerate(zip(filtered_d[\"file_content\"], filtered_d[\"file_name\"])):\n",
    "    ppl = {}\n",
    "    if os.path.exists(f\"{output_path}/dominance_scores_{path}.pkl\"):\n",
    "        print(f\"skipping {path} ...\")\n",
    "        continue\n",
    "    dialog = re.sub(r'\\<', r'\\n<', el).split(\"\\n\")[1:]\n",
    "    dialog_no_s = [re.sub(pattern, \"\", d) for d in dialog]\n",
    "    perpl = p2(dialog_no_s)\n",
    "    ppl[path] = perpl\n",
    "    with open(f\"{output_path}/dominance_p1_{path}.pkl\", 'wb') as file:\n",
    "        pickle.dump(ppl, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "\n",
    "# Assuming 'matches', 'dialog', 'offset', and 'perpl' are defined earlier in your code\n",
    "def perplexity_to_info(dialog, tokens, perpl, answers, pattern = '<(SPK[1-9]|MOD)>'):\n",
    "\n",
    "    matches = re.findall(pattern, \"\".join(dialog))\n",
    "    unique_matches = np.unique(matches)\n",
    "    #encodings = tokenizer(\" \".join(dialog), return_tensors=\"pt\")\n",
    "    encodings = torch.cat(tokens)\n",
    "    tokens_ids_per_sentence = np.cumsum([t.size(0) for t in tokens])\n",
    "    assert tokens_ids_per_sentence[-1] == len(encodings)\n",
    "    assert len(encodings) == len(perpl)\n",
    "    \n",
    "    ppl_to_info = []\n",
    "    prev_idx_pp = 0\n",
    "    for idx, (match,answer) in enumerate(zip(matches,answers)):\n",
    "        idx_pp = tokens_ids_per_sentence[idx]\n",
    "        patt = matches[idx]\n",
    "        label = re.sub(r'\\[([^\\]]+)\\]: ', '', dialog[idx])\n",
    "        tokens = encodings[prev_idx_pp:idx_pp]\n",
    "        decoded = [tokenizer.decode([token], skip_special_tokens=True) for token in tokens]\n",
    "        perpl_per_sent = perpl[prev_idx_pp:idx_pp]\n",
    "        mean_value=np.nanmean(np.asarray(perpl_per_sent))\n",
    "        prev_idx_pp=idx_pp\n",
    "        ppl_to_info.append({\"label\":answer, \"perpl\": np.asarray(perpl_per_sent)})\n",
    "    \n",
    "    return ppl_to_info\n",
    "\n",
    "def filter_out_common_words(words, perpl):\n",
    "    # Remove stop words and corresponding perplexity values\n",
    "    filtered_words = [word for word in words if word.lower().strip() not in STOP_WORDS]\n",
    "    filtered_word_indices = [i for i, word in enumerate(words) if word.lower().strip() not in STOP_WORDS]\n",
    "    assert len(list(np.asarray(perpl)[filtered_word_indices])) == len(filtered_words)\n",
    "    return list(np.asarray(perpl)[filtered_word_indices]), filtered_word_indices\n",
    "\n",
    "def perplexity_to_info_filtering_out_common_words(dialog, tokens, perpl, answers, matches, pattern = '<(SPK[1-9]|MOD)>'):\n",
    "    \n",
    "    encodings = torch.cat(tokens)\n",
    "    unique_matches = np.unique(matches)\n",
    "    tokens_ids_per_sentence = np.cumsum([t.size(0) for t in tokens])\n",
    "    assert tokens_ids_per_sentence[-1] == len(encodings)\n",
    "    assert len(encodings) == len(perpl)\n",
    "    \n",
    "    \n",
    "    ppl_to_info = []\n",
    "    prev_idx_pp = 0\n",
    "    dialog_filtered = []\n",
    "    for idx, (match,answer) in enumerate(zip(matches,answers)):\n",
    "        idx_pp = tokens_ids_per_sentence[idx]\n",
    "        patt = matches[idx]\n",
    "        label = re.sub(pattern, '', dialog[idx])\n",
    "        tokens = encodings[prev_idx_pp:idx_pp]\n",
    "        decoded = [tokenizer.decode([token], skip_special_tokens=True) for token in tokens]\n",
    "        assert len(decoded) == len(perpl[prev_idx_pp:idx_pp])\n",
    "        perpl_per_sent, filtered_word_indices = filter_out_common_words(decoded, perpl[prev_idx_pp:idx_pp])\n",
    "        assert len(perpl_per_sent) == len(tokens[filtered_word_indices])\n",
    "        dialog_filtered.append(tokens[filtered_word_indices])\n",
    "        mean_value=np.nanmean(np.asarray(perpl_per_sent))\n",
    "        prev_idx_pp=idx_pp\n",
    "        ppl_to_info.append({\"label\":answer, \"perpl\": np.asarray(perpl_per_sent)})\n",
    "        \n",
    "    return dialog_filtered, ppl_to_info\n",
    "\n",
    "def compute_per_user_mean_perplexity_filtering_out_common_words(dialog, tokens, perpl, matches, pattern = '<(SPK[1-9]|MOD)>'):\n",
    "\n",
    "    encodings = torch.cat(tokens)\n",
    "    unique_matches = np.unique(matches)\n",
    "    tokens_ids_per_sentence = np.cumsum([t.size(0) for t in tokens])\n",
    "    assert tokens_ids_per_sentence[-1] == len(encodings)\n",
    "    assert len(encodings) == len(perpl)\n",
    "    \n",
    "    prev_idx_pp = 0\n",
    "    user_to_ppl = {}\n",
    "    decoded_utterances = []\n",
    "    for idx, match in enumerate(matches):\n",
    "        idx_pp = tokens_ids_per_sentence[idx]\n",
    "        patt = matches[idx]\n",
    "        label = re.sub(pattern, '', dialog[idx])\n",
    "        tokens = encodings[prev_idx_pp:idx_pp]\n",
    "        decoded = [tokenizer.decode([token], skip_special_tokens=True) for token in tokens]\n",
    "        assert len(decoded) == len(perpl[prev_idx_pp:idx_pp])\n",
    "        perpl_per_sent, filtered_word_indices = filter_out_common_words(decoded, perpl[prev_idx_pp:idx_pp])\n",
    "        decoded_utterances.append([tokenizer.decode([token], skip_special_tokens=True) for token in tokens[filtered_word_indices]])\n",
    "        assert len(perpl_per_sent) == len(tokens[filtered_word_indices])\n",
    "        mean_value=np.nanmean(np.asarray(perpl_per_sent))\n",
    "        prev_idx_pp=idx_pp\n",
    "        if patt not in user_to_ppl:\n",
    "            user_to_ppl[patt] = []\n",
    "        user_to_ppl[patt].append(mean_value)\n",
    "        \n",
    "    return user_to_ppl, decoded_utterances\n",
    "\n",
    "def compute_per_user_mean_perplexity(dialog, tokens, perpl, matches, pattern = '<(SPK[1-9]|MOD)>'):\n",
    "    \n",
    "    encodings = torch.cat(tokens)\n",
    "    #matches = re.findall(pattern, \"\".join(dialog))\n",
    "    unique_matches = np.unique(matches)\n",
    "    tokens_ids_per_sentence = np.cumsum([t.size(0) for t in tokens])\n",
    "    assert tokens_ids_per_sentence[-1] == len(encodings)\n",
    "    assert len(encodings) == len(perpl)\n",
    "\n",
    "\n",
    "    prev_idx_pp = 0\n",
    "    user_to_ppl = {}\n",
    "    for idx in range(len(matches)):\n",
    "        idx_pp = tokens_ids_per_sentence[idx]\n",
    "        patt = matches[idx]\n",
    "        tokens = encodings[prev_idx_pp:idx_pp]\n",
    "        decoded = [tokenizer.decode([token], skip_special_tokens=True) for token in tokens]\n",
    "        perpl_per_sent = perpl[prev_idx_pp:idx_pp]\n",
    "        mean_value=np.nanmean(np.asarray(perpl_per_sent))\n",
    "        if patt not in user_to_ppl:\n",
    "            user_to_ppl[patt] = []\n",
    "        user_to_ppl[patt].append(mean_value)\n",
    "        \n",
    "    return  user_to_ppl\n",
    "    \n",
    "def compute_graph_perplexity(tokens, p1,p2, matches, pattern = '<(SPK[1-9]|MOD)>', answers=None):\n",
    "    \n",
    "    dialog = [tokenizer.decode(token, skip_special_tokens=True) for token in tokens]\n",
    "    unique_matches = np.unique(matches)\n",
    "    \n",
    "    rows = int(np.ceil(np.sqrt(len(dialog))))\n",
    "    # Create an 8x8 grid of subplots\n",
    "    fig, axes = plt.subplots(rows, rows, figsize=(30, 30))\n",
    "    num_plots = len(dialog)\n",
    "    # Set smaller font size\n",
    "    plt.rcParams.update({'font.size': 8})\n",
    "    \n",
    "    assert num_plots == len(matches)\n",
    "    encodings = torch.cat(tokens)\n",
    "    tokens_ids_per_sentence = np.cumsum([t.size(0) for t in tokens])\n",
    "    assert tokens_ids_per_sentence[-1] == len(p1)\n",
    "    \n",
    "    prev_idx_pp = 0\n",
    "    for idx, ax in enumerate(axes.flatten()):\n",
    "        if idx < num_plots:\n",
    "            idx_pp = tokens_ids_per_sentence[idx]\n",
    "            patt = matches[idx]\n",
    "            tokens = encodings[prev_idx_pp:idx_pp]\n",
    "            decoded = [tokenizer.decode([token], skip_special_tokens=True) for token in tokens]\n",
    "            p1_per_sent = p1[prev_idx_pp:idx_pp]\n",
    "            p2_per_sent = p2[prev_idx_pp:idx_pp]\n",
    "            ax.plot(np.asarray(p1_per_sent), label=f'{patt} p1')\n",
    "            ax.plot(np.asarray(p2_per_sent), label=f'{patt} p2', color='r')\n",
    "            #mean_value=np.nanmean(np.asarray(perpl_per_sent))\n",
    "            ax.axhline(p1_per_sent[0], color='g', label=f'{patt} p3')  # Fixed the color argument\n",
    "            ax.set_xticks(np.arange(len(decoded)))\n",
    "            ax.set_xticklabels(decoded, rotation=90)\n",
    "            if answers is not None:\n",
    "                ax.set_title(f'{answers[idx]}')\n",
    "            ax.legend()\n",
    "            prev_idx_pp=idx_pp\n",
    "\n",
    "    # Hide any remaining empty subplots\n",
    "    for ax in axes.flatten()[num_plots:]:\n",
    "        ax.axis('off')\n",
    "        \n",
    "    plt.subplots_adjust(hspace=0.5, top=0.95)  \n",
    "    plt.suptitle(\"Per-Word Perplexity across the Dataset\", fontsize=30)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multisimo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multisimo_df = pd.read_csv(\"/u/sebono/conversational_dominance/data/processed/multisimo/conversations.csv\")\n",
    "multisimo_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load perplexity\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "p1_multisimo_path = \"/u/sebono/conversational_dominance/notebooks/information_exchange_labelling/dataset_perplexity_results/multisimo_p1\"\n",
    "perplexity_scores_list = []\n",
    "perplexity_scores_list_p3 = []\n",
    "for root, dirs, files in os.walk(p1_multisimo_path):\n",
    "    for file in files:\n",
    "        if file.endswith('.pkl'):\n",
    "            file_name = file.replace(\"dominance_scores_\",\"\").replace(\".pkl\",\"\")\n",
    "            file_path = os.path.join(root, file)\n",
    "            with open(file_path, 'rb') as f:\n",
    "                loaded = pickle.load(f)\n",
    "                perplexity_scores_list.append(loaded)\n",
    "\n",
    "perplexity_scores_p1 = {k:v for element in perplexity_scores_list for k,v in element.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load perplexity\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "p1_multisimo_path = \"/u/sebono/conversational_dominance/notebooks/information_exchange_labelling/dataset_perplexity_results/multisimo_p2\"\n",
    "perplexity_scores_list = []\n",
    "\n",
    "for root, dirs, files in os.walk(p1_multisimo_path):\n",
    "    for file in files:\n",
    "        if file.endswith('.pkl'):\n",
    "            file_name = file.replace(\"dominance_scores_\",\"\").replace(\".pkl\",\"\")\n",
    "            file_path = os.path.join(root, file)\n",
    "            with open(file_path, 'rb') as f:\n",
    "                perplexity_scores_list.append(pickle.load(f))\n",
    "\n",
    "perplexity_scores_p2 = {k:v for element in perplexity_scores_list for k,v in element.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity_scores_p2.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_extended_list(lst, arr):\n",
    "    result = []\n",
    "    prev_count = 0\n",
    "    for i, count in enumerate(arr):\n",
    "        result.extend([lst[i]] * (count - prev_count))\n",
    "        prev_count = count\n",
    "    return result\n",
    "\n",
    "def compute_p3(multisimo_df, perplexity_scores_p1):\n",
    "    perplexity_scores_p3 = {}\n",
    "    for content, name in zip(multisimo_df[\"file_content\"], multisimo_df[\"file_name\"]):\n",
    "        pattern = '<(SPK[1-9]|MOD)>'\n",
    "        matches = re.findall(pattern, \"\".join(content))\n",
    "        content = re.sub(r'\\<', r'\\n<', content).split(\"\\n\")[1:]\n",
    "        content = [re.sub(pattern,\">\", d) for d in content]\n",
    "        assert len(perplexity_scores_p1[name]) == tokenizer(\" \".join(content), return_tensors=\"pt\", return_offsets_mapping=True).input_ids.size(1)\n",
    "        tokens = [tokenizer(token, return_tensors=\"pt\", return_offsets_mapping=True).input_ids[0] for token in content]\n",
    "        assert len(torch.cat(tokens)) == len(perplexity_scores_p1[name])\n",
    "        tokens_ids_per_sentence = np.cumsum([t.size(0) for t in tokens])\n",
    "        value = np.asarray([0] + list(np.asarray(perplexity_scores_p1[name])[tokens_ids_per_sentence[:-1]]))\n",
    "        tokens_value = np.asarray([0] + list(np.asarray(torch.cat(tokens))[tokens_ids_per_sentence[:-1]]))\n",
    "        perplexity_scores_p3[name] = create_extended_list(value, tokens_ids_per_sentence)\n",
    "        assert len(perplexity_scores_p3[name]) == tokenizer(\" \".join(content), return_tensors=\"pt\", return_offsets_mapping=True).input_ids.size(1)\n",
    "\n",
    "    return perplexity_scores_p3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity_scores_p3 = compute_p3(multisimo_df, perplexity_scores_p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity_scores_p3.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for content, name in zip(multisimo_df[\"file_content\"], multisimo_df[\"file_name\"]):\n",
    "    pattern = '<(SPK[1-9]|MOD)>'\n",
    "    matches = re.findall(pattern, \"\".join(content))\n",
    "    content = re.sub(r'\\<', r'\\n<', content).split(\"\\n\")[1:]\n",
    "    content = [re.sub(pattern,\">\", d) for d in content]\n",
    "    assert len(perplexity_scores_p1[name]) == tokenizer(\" \".join(content), return_tensors=\"pt\", return_offsets_mapping=False).input_ids.size(1)\n",
    "    encodings = tokenizer([\" \".join(content)], return_tensors=\"pt\", return_offsets_mapping=True).input_ids.ravel()\n",
    "    tokens = [tokenizer(token, return_tensors=\"pt\", return_offsets_mapping=True).input_ids[0] for token in content]\n",
    "    dialog_filtered_tokens, info = perplexity_to_info_filtering_out_common_words(content, tokens, perplexity_scores_p1[name],[None for _ in range(len(content))],matches, pattern=r'<([^>]+)>')\n",
    "    perpl = []\n",
    "    for el in info:\n",
    "        perpl += list(el['perpl'])\n",
    "    assert len(torch.cat(dialog_filtered_tokens)) == len(perpl)\n",
    "    compute_graph_perplexity(dialog_filtered_tokens,perpl,matches,pattern=pattern) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dominance = {}\n",
    "for content, name in zip(multisimo_df[\"file_content\"], multisimo_df[\"file_name\"]):\n",
    "    dominance[name] = {}\n",
    "    pattern = '<(SPK[1-9]|MOD)>'\n",
    "    matches = re.findall(pattern, \"\".join(content))\n",
    "    content = re.sub(r'\\<', r'\\n<', content).split(\"\\n\")[1:]\n",
    "    content = [re.sub(pattern,\">\", d) for d in content]\n",
    "    assert len(perplexity_scores_p1[name]) == tokenizer(\" \".join(content), return_tensors=\"pt\", return_offsets_mapping=True).input_ids.size(1)\n",
    "    tokens = [tokenizer(token, return_tensors=\"pt\", return_offsets_mapping=True).input_ids[0] for token in content]\n",
    "    assert len(torch.cat(tokens)) == len(perplexity_scores_p1[name])\n",
    "    compute_graph_perplexity(tokens,perplexity_scores_p1[name],perplexity_scores_p2[name],matches, pattern=pattern) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_non_common_words_dominance = {}\n",
    "decoded_utterances = {}\n",
    "for content, name in zip(multisimo_df[\"file_content\"], multisimo_df[\"file_name\"]):\n",
    "    print(name)\n",
    "    only_non_common_words_dominance[name] = {}\n",
    "    pattern = '<(SPK[1-9]|MOD)>'\n",
    "    matches = re.findall(pattern, \"\".join(content))\n",
    "    content = re.sub(r'\\<', r'\\n<', content).split(\"\\n\")[1:]\n",
    "    content = [re.sub(pattern,\">\", d) for d in content]\n",
    "    assert len(perplexity_scores[name]) == tokenizer(\" \".join(content), return_tensors=\"pt\", return_offsets_mapping=True).input_ids.size(1)\n",
    "    encodings = tokenizer(\" \".join(content), return_tensors=\"pt\", return_offsets_mapping=True).input_ids.ravel()\n",
    "    tokens = [tokenizer(token, return_tensors=\"pt\", return_offsets_mapping=True).input_ids[0] for token in content]\n",
    "    result, decoded = compute_per_user_mean_perplexity_filtering_out_common_words(content,tokens,perplexity_scores[name],matches,pattern=pattern)\n",
    "    decoded_utterances[name] = \"\".join([\"\".join(d) for d in decoded])\n",
    "    for patt in result.keys():\n",
    "        if patt in np.unique(matches):\n",
    "            print(f\"{patt}: {np.mean(result[patt])}\")\n",
    "            only_non_common_words_dominance[name][patt] = np.mean(result[patt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\"file_name\":decoded_utterances.keys(),\"file_content\":decoded_utterances.values()}).to_csv(\"/u/sebono/conversational_dominance/data/processed/multisimo/conversations_filter_out_common_words.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dominance_p2 = {}\n",
    "for content, name in zip(multisimo_df[\"file_content\"], multisimo_df[\"file_name\"]):\n",
    "    dominance_p2[name] = {}\n",
    "    pattern = '<(SPK[1-9]|MOD)>'\n",
    "    matches = re.findall(pattern, \"\".join(content))\n",
    "    content = re.sub(r'\\<', r'\\n<', content).split(\"\\n\")[1:]\n",
    "    content = [re.sub(pattern,\">\", d) for d in content]\n",
    "    assert len(perplexity_scores_p2[name]) == tokenizer(\" \".join(content), return_tensors=\"pt\", return_offsets_mapping=True).input_ids.size(1)\n",
    "    encodings = tokenizer(\" \".join(content), return_tensors=\"pt\", return_offsets_mapping=True).input_ids.ravel()\n",
    "    tokens = [tokenizer(token, return_tensors=\"pt\", return_offsets_mapping=True).input_ids[0] for token in content]\n",
    "    result = compute_per_user_mean_perplexity(content,tokens,perplexity_scores_p2[name], matches, pattern=pattern)\n",
    "    for patt in result.keys():\n",
    "        if patt in np.unique(matches):\n",
    "            print(f\"{patt}: {np.mean(result[patt])}\")\n",
    "            dominance_p2[name][patt] = np.mean(result[patt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dominance_p1 = {}\n",
    "for content, name in zip(multisimo_df[\"file_content\"], multisimo_df[\"file_name\"]):\n",
    "    print(name)\n",
    "    dominance_p1[name] = {}\n",
    "    pattern = '<(SPK[1-9]|MOD)>'\n",
    "    matches = re.findall(pattern, \"\".join(content))\n",
    "    content = re.sub(r'\\<', r'\\n<', content).split(\"\\n\")[1:]\n",
    "    content = [re.sub(pattern,\">\", d) for d in content]\n",
    "    assert len(perplexity_scores_p1[name]) == tokenizer(\" \".join(content), return_tensors=\"pt\", return_offsets_mapping=True).input_ids.size(1)\n",
    "    encodings = tokenizer(\" \".join(content), return_tensors=\"pt\", return_offsets_mapping=True).input_ids.ravel()\n",
    "    tokens = [tokenizer(token, return_tensors=\"pt\", return_offsets_mapping=True).input_ids[0] for token in content]\n",
    "    result = compute_per_user_mean_perplexity(content,tokens,perplexity_scores_p1[name], matches, pattern=pattern)\n",
    "    for patt in result.keys():\n",
    "        if patt in np.unique(matches):\n",
    "            print(f\"{patt}: {np.mean(result[patt])}\")\n",
    "            dominance_p1[name][patt] = np.mean(result[patt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dominance_p3 = {}\n",
    "for content, name in zip(multisimo_df[\"file_content\"], multisimo_df[\"file_name\"]):\n",
    "    print(name)\n",
    "    dominance_p3[name] = {}\n",
    "    pattern = '<(SPK[1-9]|MOD)>'\n",
    "    matches = re.findall(pattern, \"\".join(content))\n",
    "    content = re.sub(r'\\<', r'\\n<', content).split(\"\\n\")[1:]\n",
    "    content = [re.sub(pattern,\">\", d) for d in content]\n",
    "    assert len(perplexity_scores_p3[name]) == tokenizer(\" \".join(content), return_tensors=\"pt\", return_offsets_mapping=True).input_ids.size(1)\n",
    "    encodings = tokenizer(\" \".join(content), return_tensors=\"pt\", return_offsets_mapping=True).input_ids.ravel()\n",
    "    tokens = [tokenizer(token, return_tensors=\"pt\", return_offsets_mapping=True).input_ids[0] for token in content]\n",
    "    result = compute_per_user_mean_perplexity(content,tokens,perplexity_scores_p3[name], matches, pattern=pattern)\n",
    "    for patt in result.keys():\n",
    "        if patt in np.unique(matches):\n",
    "            print(f\"{patt}: {np.mean(result[patt])}\")\n",
    "            dominance_p3[name][patt] = np.mean(result[patt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/u/sebono/conversational_dominance/data/processed/multisimo/\"\n",
    "with open(f'{path}/dominance_p1.pkl', 'wb') as f:\n",
    "    pickle.dump(dominance_p1, f)\n",
    "with open(f'{path}/dominance_p2.pkl', 'wb') as f:\n",
    "    pickle.dump(dominance_p2, f)\n",
    "with open(f'{path}/dominance_p3.pkl', 'wb') as f:\n",
    "    pickle.dump(dominance_p3, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/u/sebono/conversational_dominance/data/processed/multisimo/\"\n",
    "with open(f'{path}/dominance_p1.pkl', 'rb') as f:\n",
    "    dominance_p1 = pickle.load(f)\n",
    "with open(f'{path}/dominance_p2.pkl', 'rb') as f:\n",
    "    dominance_p2 = pickle.load(f)\n",
    "with open(f'{path}/dominance_p3.pkl', 'rb') as f:\n",
    "    dominance_p3 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(dominance_p1), pd.DataFrame(dominance_p2), pd.DataFrame(dominance_p3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multisimo_df_dominance = pd.read_csv(\"../data/processed/multisimo/transcript_dominance.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ppxl_p1 = pd.DataFrame(dominance_p1).T\n",
    "df_ppxl_p1.index.name = \"file_name\"\n",
    "final_results_ppxl_p1_multisimo = pd.merge(multisimo_df_dominance,df_ppxl_p1, on=\"file_name\")\n",
    "\n",
    "results = pd.DataFrame({\"one\":final_results_ppxl_p1_multisimo[\"speaker_1_dom_score\"]>final_results_ppxl_p1_multisimo[\"speaker_2_dom_score\"],\"two\": final_results_ppxl_p1_multisimo[\"SPK1\"]>final_results_ppxl_p1_multisimo[\"SPK2\"]})\n",
    "sum(results[\"one\"]==results[\"two\"])/len(results[\"one\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ppxl_p2 = pd.DataFrame(dominance_p2).T\n",
    "df_ppxl_p2.index.name = \"file_name\"\n",
    "final_results_ppxl_p2_multisimo = pd.merge(multisimo_df_dominance,df_ppxl_p2, on=\"file_name\")\n",
    "\n",
    "results = pd.DataFrame({\"one\":final_results_ppxl_p2_multisimo[\"speaker_1_dom_score\"]>final_results_ppxl_p2_multisimo[\"speaker_2_dom_score\"],\"two\": final_results_ppxl_p2_multisimo[\"SPK1\"]>final_results_ppxl_p2_multisimo[\"SPK2\"]})\n",
    "sum(results[\"one\"]==results[\"two\"])/len(results[\"one\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ppxl_p3 = pd.DataFrame(dominance_p3).T\n",
    "df_ppxl_p3.index.name = \"file_name\"\n",
    "final_results_ppxl_p3_multisimo = pd.merge(multisimo_df_dominance,df_ppxl_p3, on=\"file_name\")\n",
    "\n",
    "results = pd.DataFrame({\"one\":final_results_ppxl_p3_multisimo[\"speaker_1_dom_score\"]>final_results_ppxl_p3_multisimo[\"speaker_2_dom_score\"],\"two\": final_results_ppxl_p3_multisimo[\"SPK1\"]>final_results_ppxl_p3_multisimo[\"SPK2\"]})\n",
    "sum(results[\"one\"]==results[\"two\"])/len(results[\"one\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats\n",
    "import plotly\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = 'iframe'\n",
    "import plotly.express as px\n",
    "plotly.offline.init_notebook_mode(connected=True)\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmp = 'algae'\n",
    "def correlation_heatmap(y_cols, x_cols, full_data):\n",
    "    '''\n",
    "    Uses scipy.stats.spearmanr function\n",
    "    Params:\n",
    "    y_cols, x_cols: sets of column titles (strings)\n",
    "    full_data: pandas dataframe that includes all columns listed in y_cols, x_cols\n",
    "    Returns:\n",
    "    corr: Spearman correlation coefficient matrix (y_cols = rows, x_cols = cols of matrix)\n",
    "    fig_corr: annotated plotly heatmap of coefficients\n",
    "    p: Spearman p-value matrix\n",
    "    fig_p: annotated plotly heatmap of p-values\n",
    "    '''\n",
    "    cols = y_cols+x_cols\n",
    "    all_correlations = scipy.stats.spearmanr(full_data[cols], nan_policy='omit')\n",
    "    corr = all_correlations.statistic[:len(y_cols), -len(x_cols):]\n",
    "    corr = pd.DataFrame(corr)\n",
    "    corr.columns = x_cols\n",
    "    corr.index = y_cols\n",
    "\n",
    "    p = all_correlations.pvalue[:len(y_cols), -len(x_cols):]\n",
    "    p = pd.DataFrame(p)\n",
    "    p.columns = x_cols\n",
    "    p.index = y_cols\n",
    "    \n",
    "    fig_corr = px.imshow(corr, text_auto=True, aspect='auto', color_continuous_scale='agsunset')\n",
    "    fig_r2 = px.imshow(corr**2, text_auto=True, aspect='auto', color_continuous_scale='agsunset')\n",
    "    fig_p = px.imshow(p, text_auto=True, aspect='auto', color_continuous_scale='gray_r')\n",
    "\n",
    "    return corr, fig_corr, p, fig_p, fig_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation(df_final):\n",
    "    corr, fig_corr, p, fig_p, fig_r2 = correlation_heatmap(col_1, col_2, df_final)\n",
    "    fig_corr.show()\n",
    "    fig_p.show()\n",
    "    fig_r2.show()\n",
    "    return corr, fig_corr, p, fig_p, fig_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.concat((pd.DataFrame(dominance_p1).add_suffix(\"_P1\",axis=0),pd.DataFrame(dominance_p2).add_suffix(\"_P2\",axis=0),pd.DataFrame(dominance_p3).add_suffix(\"_P3\",axis=0))).T\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p1 - p2\n",
    "col_1 = [\"MOD_P1\",\"SPK1_P1\",\"SPK2_P1\"] \n",
    "col_2 = [\"MOD_P2\",\"SPK1_P2\",\"SPK2_P2\"]\n",
    "corr, fig_corr, p, fig_p, fig_r2 = correlation(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p1 - p3\n",
    "col_1 = [\"MOD_P1\",\"SPK1_P1\",\"SPK2_P1\"] \n",
    "col_2 = [\"MOD_P3\",\"SPK1_P3\",\"SPK2_P3\"]\n",
    "corr, fig_corr, p, fig_p, fig_r2 = correlation(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p2 - p3\n",
    "col_1 = [\"MOD_P1\",\"SPK1_P1\",\"SPK2_P1\"] \n",
    "col_2 = [\"MOD_P3\",\"SPK1_P3\",\"SPK2_P3\"]\n",
    "corr, fig_corr, p, fig_p, fig_r2 = correlation(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats\n",
    "import plotly\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = 'iframe'\n",
    "import plotly.express as px\n",
    "plotly.offline.init_notebook_mode(connected=True)\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_s = final_results_ppxl_multisimo[[\"speaker_1_dom_score\",\"speaker_2_dom_score\"]]\n",
    "X = final_results_ppxl_multisimo[[\"SPK1\",\"SPK2\"]]\n",
    "col_1 = list(X.keys())\n",
    "col_2 = list(y_s.keys())\n",
    "corr, fig_corr, p, fig_p, fig_r2 = correlation(final_results_ppxl_multisimo[[\"speaker_1_dom_score\",\"speaker_2_dom_score\",\"SPK1\",\"SPK2\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialog_filtered_tokens, info = perplexity_to_info_filtering_out_common_words(d,ppl_d,answers)\n",
    "perpl = []\n",
    "answers = []\n",
    "for el in info:\n",
    "    answers.append(el['label'])\n",
    "    perpl += list(el['perpl'])\n",
    "compute_graph_perplexity(dialog_filtered_tokens,perpl,answers=answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###TO BE FIXED: there are some spaces that mess up the tokenization and detokenization\n",
    "\n",
    "for el, dataset in zip(info, multisimo_df['file_content']):\n",
    "    print(f\"plotting {el}\")\n",
    "    dataset = re.sub(r'\\[', r'\\n[', content).split(\"\\n\")[1:]\n",
    "    perplexity = []\n",
    "    for sentence in info[el]:\n",
    "        perplexity += list(sentence[\"perpl\"]) \n",
    "    compute_graph_perplexity(dataset,perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (info_u, dataset) in enumerate(zip(info.keys(), considered_datasets_filtered)):\n",
    "    for u, d in zip(info[info_u][1:], dataset.split(\"\\n\")[1:]):\n",
    "        \"\"\"filtering out assertive sentences like \"'[Teacher]: \"Very good. Continue.\"',\n",
    "             '[Teacher]: \"Please, go ahead.\"',\n",
    "             '[Teacher]: \"That\\'s correct.\"',\n",
    "             '[Teacher]: \"Very good. Continue.\"',\"\"\"\n",
    "        per_label_ppl[u['label']] += list(u['perpl'])\n",
    "        per_label_dataset[u['label']].append(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tobedone when we have labels for MULTISIMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "considered_datasets =multisimo_df[\"file_content\"]\n",
    "\n",
    "per_label_dataset={\"Information seeking\":[], \"Information giving\":[],\"same\":[]}\n",
    "per_label_ppl={\"Information seeking\":[], \"Information giving\":[],\"same\":[]}\n",
    "\n",
    "for dataset in considered_datasets:\n",
    "    dataset = dataset.split(\"\\n\")\n",
    "    labels = [\"same\" for _ in range(len(dataset))]\n",
    "    encodings = tokenizer(dataset, return_tensors=\"pt\")\n",
    "    tokens_ids_per_sentence = np.cumsum([tokenizer(p, return_tensors=\"pt\").input_ids.size(1) for p in dialog])\n",
    "    prev_idx_pp = 0\n",
    "    for idx, (d, l) in enumerate(zip(dataset,labels)):\n",
    "        idx_pp = tokens_ids_per_sentence[idx]\n",
    "        offset_ppl = len(tokenizer(d, return_tensors=\"pt\"))\n",
    "        per_label_dataset[l].append(d)\n",
    "        per_label_ppl[l] += perplexity_scores[idx][prev_idx_pp:idx_pp]\n",
    "        prev_idx_pp=idx_pp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Dominance Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_user_utterances(content, pattern='<(SPK[1-9]|MOD)>'):\n",
    "    content = \" \".join(content)\n",
    "    count_utterance_p_user = {}\n",
    "    count_turns_p_user = {}\n",
    "    matches = re.findall(pattern, \"\".join(content))\n",
    "    content = re.sub(r'\\<', r'\\n<', content).split(\"\\n\")[1:]\n",
    "    for el in np.unique(np.array(matches)):\n",
    "        all_utt_p_user = [c for c in content if el in c]\n",
    "        all_utt_p_user_n = sum([len(u.split(\".\")) for u in all_utt_p_user])\n",
    "        count_utterance_p_user[el]=len(all_utt_p_user)\n",
    "        count_turns_p_user[el] = np.sum(np.asarray(matches) == el)\n",
    "    return count_utterance_p_user, count_turns_p_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multisimo_df = pd.read_csv(\"/u/sebono/conversational_dominance/data/processed/multisimo/conversations.csv\")\n",
    "\n",
    "count_utterance_p_user_l = {}\n",
    "count_turns_p_user_l = {}\n",
    "for content, name in zip(multisimo_df[\"file_content\"], multisimo_df[\"file_name\"]):\n",
    "    count_utterance_p_user_l[name] = {}\n",
    "    count_turns_p_user_l[name] = {}\n",
    "    print(name)\n",
    "    dominance[name] = {}\n",
    "    pattern = '<(SPK[1-9]|MOD)>'\n",
    "    matches = re.findall(pattern, \"\".join(content))\n",
    "    content = re.sub(r'\\<', r'\\n<', content).split(\"\\n\")[1:]\n",
    "    count_utterance_p_user, count_turns_p_user = count_user_utterances(content, pattern=pattern)\n",
    "    for cu, ct in zip(count_utterance_p_user, count_turns_p_user):\n",
    "        count_utterance_p_user_l[name][cu] = count_utterance_p_user[cu]\n",
    "        count_turns_p_user_l[name][ct] = count_turns_p_user[ct]\n",
    "        print(f\"{cu}:\", count_utterance_p_user[cu],count_turns_p_user[ct])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_utterance_p_user_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multisimo_df = pd.read_csv(\"../data/processed/multisimo/transcript_dominance.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ppxl = pd.DataFrame(count_utterance_p_user_l).T\n",
    "df_ppxl.index.name = \"file_name\"\n",
    "final_results_ppxl_multisimo = pd.merge(multisimo_df,df_ppxl, on=\"file_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results_ppxl_multisimo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame({\"one\":final_results_ppxl_multisimo[\"speaker_1_dom_score\"]>final_results_ppxl_multisimo[\"speaker_2_dom_score\"],\"two\": final_results_ppxl_multisimo[\"SPK1\"]>final_results_ppxl_multisimo[\"SPK2\"]})\n",
    "sum(results[\"one\"]==results[\"two\"])/len(results[\"one\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teacher Dominated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "file_path = \"/u/sebono/conversational_dominance/data/results/information_labels__teacher_dominated__withneutral.csv\"\n",
    "datasets = pd.read_csv(file_path).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppl = {}\n",
    "for index, dataset in zip(datasets[\"index\"],datasets[\"transcript\"]):\n",
    "    dataset = dataset.split(\"\\n\")\n",
    "    pattern = r'\\[([^\\]]+)\\]'\n",
    "    matches = re.findall(pattern, \"\".join(dataset))    \n",
    "    perpl = perplexity_of_fixedlength_models(dataset)\n",
    "    ppl[index] = perpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pickle\n",
    "#file_path = \"/u/sebono/conversational_dominance/data/results/information_labels__teacher_dominated__withneutral_perplexity_scores.pikle\"\n",
    "#with open(file_path, 'wb') as file:\n",
    "#    pickle.dump(ppl, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "file_path = \"/u/sebono/conversational_dominance/data/results/information_labels__teacher_dominated__withneutral_perplexity_scores.pikle\"\n",
    "with open(file_path, 'rb') as file:\n",
    "    perplexity_scores = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "considered_indices = datasets[datasets[\"n_responses_match\"] == True][\"index\"]\n",
    "considered_datasets =  datasets[datasets[\"n_responses_match\"] == True]\n",
    "\n",
    "considered_datasets[\"transcript\"].iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "considered_indices = datasets[datasets[\"n_responses_match\"] == True][\"index\"]\n",
    "considered_datasets =  datasets[datasets[\"n_responses_match\"] == True]\n",
    "\n",
    "per_label_dataset={\"Information seeking\":[],\"Neutral\": [],  \"Information giving\":[]}\n",
    "per_label_ppl={\"Information seeking\":[], \"Neutral\": [], \"Information giving\":[]}\n",
    "\n",
    "info = {}\n",
    "considered_datasets_filtered = []\n",
    "for dataset, response, ppl_idx in zip(considered_datasets[\"transcript\"],considered_datasets[\"response\"], considered_indices):\n",
    "    #encodings = [tokenizer(d, return_tensors=\"pt\").input_ids[0] for d in dataset.split(\"\\n\")]\n",
    "    dataset = dataset.split(\"\\n\")\n",
    "    encodings = tokenizer(\" \".join(dataset), return_tensors=\"pt\", return_offsets_mapping=True).input_ids.ravel()\n",
    "    dataset_filtered, info[ppl_idx] = perplexity_to_info_filtering_out_common_words(dataset,encodings,perplexity_scores[ppl_idx],response.split(\"\\n\"))\n",
    "    considered_datasets_filtered.append(dataset_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_label_ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_label_dataset={\"Information seeking\":[],\"Neutral\": [],  \"Information giving\":[]}\n",
    "per_label_ppl={\"Information seeking\":[], \"Neutral\": [], \"Information giving\":[]}\n",
    "for i, (info_u, dataset) in enumerate(zip(info.keys(), considered_datasets_filtered)):\n",
    "    for u, d in zip(info[info_u], dataset):\n",
    "        \"\"\"filtering out assertive sentences like \"'[Teacher]: \"Very good. Continue.\"',\n",
    "             '[Teacher]: \"Please, go ahead.\"',\n",
    "             '[Teacher]: \"That\\'s correct.\"',\n",
    "             '[Teacher]: \"Very good. Continue.\"',\"\"\"\n",
    "        assert len(u['perpl']) == len(d)\n",
    "        per_label_ppl[u['label']] += list(u['perpl'])\n",
    "        per_label_dataset[u['label']].append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#information giving\n",
    "pattern = r'\\[([^\\]]+)\\]'\n",
    "tokens = per_label_dataset[\"Neutral\"]\n",
    "perplexity = per_label_ppl[\"Neutral\"]\n",
    "compute_graph_perplexity(tokens, perplexity,pattern=pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#information giving\n",
    "tokens = per_label_dataset[\"Information giving\"]\n",
    "perplexity = per_label_ppl[\"Information giving\"]\n",
    "compute_graph_perplexity(tokens, perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(per_label_ppl[\"Information giving\"]), len(per_label_dataset[\"Information giving\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity = per_label_ppl[\"Neutral\"]\n",
    "np.nanmean(perplexity), np.nanstd(perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity = per_label_ppl[\"Information giving\"]\n",
    "np.nanmean(perplexity), np.nanstd(perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity = per_label_ppl[\"Information seeking\"]\n",
    "np.nanmean(perplexity), np.nanstd(perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming per_label_ppl is a dictionary containing data for \"Information seeking\" and \"Information giving\"\n",
    "\n",
    "# Create a figure and axis\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plot KDE using seaborn\n",
    "sns.kdeplot(per_label_ppl[\"Information seeking\"], color='blue', label='Information seeking', ax=ax, cut=0, clip=(0, 4))\n",
    "sns.kdeplot(per_label_ppl[\"Information giving\"], color='green', label='Information giving', ax=ax, cut=0, clip=(0, 4))\n",
    "sns.kdeplot(per_label_ppl[\"Neutral\"], color='yellow', label='Neutral', ax=ax, cut=0, clip=(0, 4))\n",
    "\n",
    "# Adding labels and legend\n",
    "ax.set_xlabel('Values')\n",
    "ax.set_ylabel('Density')\n",
    "ax.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming per_label_ppl is a dictionary containing data for \"Information seeking\" and \"Information giving\"\n",
    "\n",
    "# Create a figure and axis\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plot histograms using seaborn with density=True\n",
    "sns.histplot(per_label_ppl[\"Information seeking\"], kde=True, color='blue', label='Information seeking', ax=ax, stat='density')\n",
    "sns.histplot(per_label_ppl[\"Information giving\"], kde=True, color='green', label='Information giving', ax=ax, stat='density')\n",
    "sns.histplot(per_label_ppl[\"Neutral\"], kde=True, color='yellow', label='Neutral', ax=ax, stat='density')\n",
    "\n",
    "# Adding labels and legend\n",
    "ax.set_xlabel('Values')\n",
    "ax.set_ylabel('Density')\n",
    "ax.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#lower perplexity\n",
    "dataset = per_label_dataset[\"Information seeking\"]\n",
    "perplexity = per_label_ppl[\"Information seeking\"]\n",
    "pattern = r'\\[([^\\]]+)\\]'\n",
    "compute_graph_perplexity(dataset,perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
